{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "current_date = date.today()\n",
    "#%load_ext autotime\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib_venn import venn2, venn3\n",
    "import warnings\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from more_itertools import unique_everseen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import pandasql as ps\n",
    "import seaborn as sns\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import ast\n",
    "from fuzzywuzzy import process\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\total_summary_stats_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = total[\"patient_id\"].to_list()\n",
    "patient_id = patient_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN ONCE #\n",
    "\n",
    "# vertifying that all match records can be found\n",
    "for i in range(len(patient_id)):\n",
    "    specific_matches = matches[matches[\"PatientID\"] == patient_id[i]]\n",
    "    if (len(specific_matches) <= 0):\n",
    "        print(\"NOT FOUND: \", patient_id[i])\n",
    "    #specific_matches.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Specific_Matches\\patient_{}.csv\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Specific_Matches\\patient_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = matches[\"PatientID\"].unique().tolist()\n",
    "print(len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting match cases\n",
    "# patient_2 in specific study row 6, cream that has mci inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_loc_clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\3\\apoe_3_matches_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\Sentence-Analysis\\EDA\\Getting_Data\\keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = re.compile(r\"(?i:)\\bmemory\\b|(?i:)\\bno\\b|(?-i:)\\bOriented\\b|(?-i:)\\bMCI\\b\")\n",
    "\n",
    "# p2 = re.compile(r\"(?i:)\\bMOCA\\b\")\n",
    "# regexes = [p, p2]\n",
    "# p_combined = re.compile(r'|'.join(x.pattern for x in regexes))\n",
    "# print(p_combined)\n",
    "# print(p)\n",
    "\n",
    "k = regex[\"REGEX\"].to_list()\n",
    "c = regex[\"CASE\"].to_list()\n",
    "p_list = []\n",
    "\n",
    "for i in range(len(k)):\n",
    "    if (c[i] == 0):\n",
    "        p_list.append(re.compile(k[i][5:], re.IGNORECASE))\n",
    "    elif (c[i] == 1):\n",
    "        p_list.append(re.compile(k[i]))\n",
    "print(p_list)\n",
    "p_all = (r\"|\".join(x.pattern for x in p_list))\n",
    "p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = str(p1[\"merged_regex_sent\"][5]) + \" mCi.\"\n",
    "\n",
    "print(string)\n",
    "print(re.compile(r\"\\bmemory\\b\", re.IGNORECASE))\n",
    "\n",
    "for p in p_list:\n",
    "    for match in re.finditer(p, string):\n",
    "        s = match.start()\n",
    "        e = match.end()\n",
    "        print ('String match \"%s\" at %d:%d' % (string[s:e], s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "\n",
    "for r in tqdm(p1['merged_regex_sent']):\n",
    "    l = []\n",
    "    for p in (p_list):\n",
    "        for match in re.finditer(p, r):\n",
    "            l.append(match.span())\n",
    "    #print(l)\n",
    "    locations.append(l)\n",
    "        \n",
    "p1[\"sequence_level_regex_location\"] = locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"merged_regex_sent\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = p1[p1[\"sequence_level_regex_location\"].str.len() != 0]\n",
    "len(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merged = []\n",
    "\n",
    "for index, row in p1.iterrows():\n",
    "    curr = str(row[\"regex_location\"])\n",
    "    curr = curr[1:len(curr) - 1]\n",
    "    curr = list((eval(curr)))\n",
    "    #print(\"CURR: \", len(curr))\n",
    "    # print(curr)\n",
    "    merged = []\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    length = 0\n",
    "    \n",
    "    if (len(curr) == 2 and isinstance(curr[1], int)):\n",
    "        start = curr[0]\n",
    "        end = curr[1]\n",
    "        #print(start, end)\n",
    "        length = len(row[\"NoteTXT\"])\n",
    "        if (start < 100):\n",
    "            start = 100\n",
    "        elif (end > length - 100):\n",
    "            end = length - 100\n",
    "        curr = [(start - 100, end + 100)]\n",
    "        \n",
    "    else:\n",
    "        loc_list = []\n",
    "        for j in range(len(curr)):\n",
    "            start = curr[j][0]\n",
    "            end = curr[j][1]\n",
    "            loc_list.append((start, end))\n",
    "        length = len(row[\"NoteTXT\"])\n",
    "        for k in range(len(loc_list)):\n",
    "            start = loc_list[k][0]\n",
    "            end = loc_list[k][1]\n",
    "            if (start < 100):\n",
    "                start = 100\n",
    "            elif (end > length - 100):\n",
    "                end = length - 100\n",
    "            loc_list[k] = (start - 100, end + 100)\n",
    "        curr = loc_list\n",
    "        \n",
    "    for interval in curr:\n",
    "        if not merged:\n",
    "            merged.append(interval)\n",
    "            continue\n",
    "\n",
    "        prevInterval = merged.pop()\n",
    "        if prevInterval[0] <= interval[0] <= prevInterval[1]:\n",
    "            startTime = prevInterval[0]\n",
    "            endTime = prevInterval[1] if prevInterval[1] > interval[1] else interval[1]\n",
    "            merged.append((startTime, endTime))\n",
    "        else:\n",
    "            merged.append(prevInterval)\n",
    "            merged.append(interval)\n",
    "            \n",
    "    total_merged.append(merged)\n",
    "p1[\"merged_row_location\"] = total_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merged_note_txt = []\n",
    "for index, row in p1.iterrows():\n",
    "    curr = row[\"merged_row_location\"]\n",
    "    merged_note_txt = \"\"\n",
    "    \n",
    "    for i in range(len(curr)):\n",
    "        start = curr[i][0]\n",
    "        end = curr[i][1]\n",
    "        \n",
    "        merged_note_txt += \" ------- \" + row[\"NoteTXT\"][start:end]\n",
    "    total_merged_note_txt.append(merged_note_txt)\n",
    "    \n",
    "p1[\"merged_regex_sent\"] = total_merged_note_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1['merged_regex_sent'] = p1['merged_regex_sent'].str.split(' ',1).str[1].str.rpartition(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p1['NoteTXT'] = p1['NoteTXT'].astype(str)\n",
    "p1 = p1.drop_duplicates(subset='merged_regex_sent', keep=\"first\")\n",
    "p1 = p1.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "p1.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\4\\apoe_4_matches_updated_pruned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Regex matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\5\\apoe_5_matches_updated_pruned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    curr = []\n",
    "    for p in (p_list):\n",
    "        m = list(set(re.findall(p, note)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        if (m != []):\n",
    "            curr.append(\"\".join(m))\n",
    "    #print(curr)\n",
    "    #print(l)\n",
    "    l.append(str(curr))\n",
    "\n",
    "p2[\"sequence_level_regex_match\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1[p1[\"merged_regex_match\"] == \"[\\' mci\\']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p2[p2[\"sequence_level_regex_match\"] == \"[\\'mci\\']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_snippet = p2[:500]\n",
    "p2_snippet.to_excel(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\3\\apoe_3_matches_7_11_snippet.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\5\\apoe_5_matches_7_11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Each Sequence so that it becomes min 800 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2['char_count'] = p2['merged_regex_sent'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2['note_char_count'] = p2['NoteTXT'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p2[p2['note_char_count'] < 800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_padding = p2[p2[\"char_count\"] > 798] #798 b/c that is min cutoff to addd 1 char to each side\n",
    "len(no_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding = p2[p2[\"char_count\"] <= 798]\n",
    "len(need_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "48359 +8411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"merged_row_location\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting revised intervals with padding accounted for\n",
    "\n",
    "total_padded_intervals = []\n",
    "for index, row in tqdm(need_padding.iterrows()):\n",
    "    curr = str(row[\"merged_row_location\"])\n",
    "    curr = curr[1:len(curr) - 1]\n",
    "    curr = list((eval(curr)))\n",
    "    \n",
    "    curr_char_count = row[\"char_count\"]\n",
    "    padded_len = int((800 - curr_char_count) / 2)\n",
    "    \n",
    "    if len(curr) == 2 and isinstance(curr[1], int):\n",
    "        start = curr[0]\n",
    "        end = curr[1]\n",
    "        #print(\"b4\", start, end)\n",
    "\n",
    "        length = len(row[\"NoteTXT\"])\n",
    "        #print(\"length: \", length)\n",
    "        if (start - padded_len < 0):\n",
    "            start = 0\n",
    "\n",
    "        if (end + padded_len > length):\n",
    "            #print(\"reached\")\n",
    "            end = length - 1\n",
    "            diff = 800 - (end - start)\n",
    "            if (start - diff < 0):\n",
    "                start = 0\n",
    "            else:\n",
    "                start -= diff\n",
    "\n",
    "        if (start - padded_len > 0 and end + padded_len < length):\n",
    "            start -= padded_len\n",
    "            end += padded_len\n",
    "\n",
    "        if (end - start < 750):\n",
    "            curr_length = end - start\n",
    "            add_to_end = 750 - curr_length\n",
    "            if (end + add_to_end >= length):\n",
    "                end = length  - 1\n",
    "            else:\n",
    "                end += add_to_end\n",
    "\n",
    "        curr = [(start, end)]\n",
    "    else:\n",
    "        start = curr[0]\n",
    "        end = curr[len(curr) - 1]\n",
    "        length = len(row[\"NoteTXT\"])\n",
    "        if (start[0] - padded_len < 0):\n",
    "            curr[0] = (0, curr[0][1])\n",
    "        else:\n",
    "            curr[0] = (curr[0][0] - padded_len, curr[0][1])\n",
    "        if (end[1] + padded_len > length):\n",
    "            curr[len(curr) - 1] = (curr[len(curr) - 1][0], length - 1)\n",
    "        else:\n",
    "            curr[len(curr) - 1] = (curr[len(curr) - 1][0], curr[len(curr) - 1][1] + padded_len)\n",
    "\n",
    "        \n",
    "#         curr[0] = (curr[0][0] - padded_len, curr[0][1])\n",
    "#         curr[len(curr) - 1] = (curr[len(curr) - 1][0], curr[len(curr) - 1][1] + padded_len)\n",
    "    \n",
    "    #row[\"padded_merged_row_location\"] = curr\n",
    "    total_padded_intervals.append(curr)\n",
    "\n",
    "need_padding[\"padded_merged_row_location\"] = total_padded_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merged_note_txt = []\n",
    "for index, row in tqdm(need_padding.iterrows()):\n",
    "    curr = str(row[\"padded_merged_row_location\"])\n",
    "    curr = curr[1:len(curr) - 1]\n",
    "    curr = list((eval(curr)))\n",
    "    #print(len(curr))\n",
    "    \n",
    "    merged_note_txt = \"\"\n",
    "    \n",
    "    if (len(curr) == 2 and isinstance(curr[1], int)):\n",
    "        start = int(curr[0])\n",
    "        end = int(curr[1])\n",
    "        \n",
    "        merged_note_txt += row[\"NoteTXT\"][start:end]\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(curr)):\n",
    "            start = curr[i][0]\n",
    "            end = curr[i][1]\n",
    "            if (start == 0):\n",
    "                merged_note_txt += row[\"NoteTXT\"][start:end]\n",
    "            else:\n",
    "                merged_note_txt += \" ------- \" + row[\"NoteTXT\"][start:end]\n",
    "\n",
    "        \n",
    "    total_merged_note_txt.append(merged_note_txt)\n",
    "    \n",
    "need_padding[\"padded_merged_regex_sent\"] = total_merged_note_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding['padded_char_count'] = need_padding['padded_merged_regex_sent'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_padding.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = need_padding[need_padding['padded_char_count'] < 700] \n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(need_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_padding.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\3\\Padded_Matches\\apoe_3_padded_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_padding.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\3\\Padded_Matches\\apoe_3_not_padded_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = need_padding[:250]\n",
    "checking.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Padded_Matches\\apoe_1_checking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case Filtering (MCI, CDR, MMSE, MOCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_mci = re.compile(r'(?-i:)\\bMCI\\b|(?-i:)\\bmci\\b')\n",
    "mci_matches = []\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(pattern_mci, note)))\n",
    "        m = list(set(m))\n",
    "        mci_matches.append(str(m))\n",
    "    except:\n",
    "        mci_matches.append([])\n",
    "p2[\"contains_mci\"] = mci_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_2 = \"------- : TC-99m Sestamibi According to standard departmental protocol, the patient was injected with 10.97 mCi of TC-99m Sestamibi IV at rest and ungated SPECT myocardial images were obtained. Subsequently, a stress test was performed and 30.93 mci of TC-99m Sestamibi injected IV at peak stress. After 30-60 minutes, gated SPECT images were\"\n",
    "for match in re.finditer(pattern_mci, string_2):\n",
    "    s = match.start()\n",
    "    e = match.end()\n",
    "    print ('String match \"%s\" at %d:%d' % (string_2[s:e], s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_m = p2[p2[\"contains_mci\"] != \"[]\"]\n",
    "mci_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_m.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_mci.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[p2[\"merged_regex_match\"].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = str(p1[\"merged_regex_sent\"][5]) + \" mCi. \"\n",
    "print(string)\n",
    "p = re.compile(r\"(?i:)\\bmemory\\b|(?i:)\\bno\\b|(?-i:)\\bOriented\\b|(?-i:)\\bMCI\\b\")\n",
    "\n",
    "# p2 = re.compile(r\"(?i:)\\bMOCA\\b\")\n",
    "# regexes = [p, p2]\n",
    "# p_combined = re.compile(r'|'.join(x.pattern for x in regexes))\n",
    "#print(p_combined)\n",
    "#print(p)\n",
    "for match in re.finditer(p_all , string):\n",
    "    s = match.start()\n",
    "    e = match.end()\n",
    "    print ('String match \"%s\" at %d:%d' % (string[s:e], s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = p2[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "pattern = re.compile(r'\\bcerebral\\b|\\bcerebellar\\b', re.IGNORECASE)\n",
    "\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(pattern, note)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        matches.append(str(m))\n",
    "    except:\n",
    "        matches.append([])\n",
    "\n",
    "p2[\"contains_cerebral_cerebellar\"] = matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerebral_cerebellar = p2[p2[\"contains_cerebral_cerebellar\"] != \"[]\"]\n",
    "cerebral_cerebellar = cerebral_cerebellar[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2 = []\n",
    "matches3 = []\n",
    "matches4 = []\n",
    "\n",
    "pattern2 = re.compile(r'\\bCDR\\b', re.IGNORECASE)\n",
    "pattern3 = re.compile(r'\\bMMSE\\b', re.IGNORECASE)\n",
    "pattern4 = re.compile(r\"\\bMOCA\\b\", re.IGNORECASE)\n",
    "\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(pattern2, note)))\n",
    "        m = list(set(m))\n",
    "        matches2.append(str(m))\n",
    "    except:\n",
    "        matches2.append([])\n",
    "\n",
    "p2[\"contains_cdr\"] = matches2\n",
    "\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(pattern3, note)))\n",
    "        m = list(set(m))\n",
    "        matches3.append(str(m))\n",
    "    except:\n",
    "        matches3.append([])\n",
    "\n",
    "p2[\"contains_mmse\"] = matches3\n",
    "\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(pattern4, note)))\n",
    "        m = list(set(m))\n",
    "        matches4.append(str(m))\n",
    "    except:\n",
    "        matches4.append([])\n",
    "\n",
    "p2[\"contains_moca\"] = matches4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_pattern = re.compile(r'\\bmoca\\b|\\bMoca\\b')\n",
    "matches5 = []\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(moca_pattern, note)))\n",
    "        m = list(set((m)))\n",
    "        matches5.append(str(m))\n",
    "    except:\n",
    "        matches5.append([])\n",
    "p2[\"lower_case_moca\"] = matches5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_pattern = re.compile(r'\\bcdr\\b|\\bCdr\\b') \n",
    "matches6 = []\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(cdr_pattern, note)))\n",
    "        m = list(set((m)))\n",
    "        matches6.append(str(m))\n",
    "    except:\n",
    "        matches6.append([])\n",
    "p2[\"lower_case_cdr\"] = matches6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmse_pattern = re.compile(r'\\bmmse\\b|\\bMmse\\b') \n",
    "matches7 = []\n",
    "for note in tqdm(p2[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(mmse_pattern, note)))\n",
    "        m = list(set((m)))\n",
    "        matches7.append(str(m))\n",
    "    except:\n",
    "        matches7.append([])\n",
    "p2[\"lower_case_mmse\"] = matches7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"lower_case_moca\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"lower_case_cdr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"lower_case_mmse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr = p2[p2[\"contains_cdr\"] != \"[]\"]\n",
    "len(cdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmse = p2[p2[\"contains_mmse\"] != \"[]\"]\n",
    "mmse = mmse[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moca = p2[p2[\"contains_moca\"] != \"[]\"]\n",
    "moca = moca[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"contains_moca\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_lowercase = p2[(p2[\"lower_case_moca\"] == \"[\\'moca\\']\") | (p2[\"lower_case_moca\"] == \"[\\'Moca\\']\")]\n",
    "moca_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_lowercase[\"merged_regex_sent\"][32080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_lowercase = p2[(p2[\"lower_case_cdr\"] == \"[\\'cdr\\']\") | (p2[\"lower_case_cdr\"] == \"[\\'Cdr\\']\")]\n",
    "cdr_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_500_matches.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerebral_cerebellar.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_cerebral_cerebellar.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_cdr.csv\", index = False)\n",
    "mmse.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_mmse.csv\", index = False)\n",
    "moca.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_moca.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_lowercase.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_moca_lowercase.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_lowercase.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\Edge_Cases\\Keywords\\apoe_1_cdr_lowercase.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting #  of Unique Encounters per Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting number of unique encounters per patient\n",
    "df = p2 \n",
    "#pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_matches_updated_pruned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"EMPI\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = 'PatientEncounterID', keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats_1 = df.groupby(['EMPI'])[\"PatientEncounterID\"].count().reset_index()\n",
    "summary_stats_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats_1.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_summary_stats_7+8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_summary_stats_updated.csv\")\n",
    "# s2 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\2\\apoe_2_summary_stats_updated.csv\")\n",
    "# s3 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\3\\apoe_3_summary_stats_updated.csv\")\n",
    "# s4 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\4\\apoe_4_summary_stats_updated.csv\")\n",
    "# s5 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\5\\apoe_5_summary_stats_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2778+3335+3499+3431+3918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [s1, s2, s3, s4, s5]\n",
    "total = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\total_summary_stats_updated_7_6_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_matches_updated_pruned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.iloc[0:5000]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\Sentence-Analysis\\EDA\\Getting_Data\\keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for note in tqdm(subset[\"merged_regex_sent\"]):\n",
    "    try:\n",
    "        m = list(set(re.findall(re.compile(\"|\".join(regex[\"REGEX\"]), re.IGNORECASE), note)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        l.append(m)\n",
    "    except:\n",
    "        l.append([])\n",
    "\n",
    "subset[\"merged_regex_match\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"cognition\", \"Cognition\"]\n",
    "a = list(set(map(str.lower, a)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\1\\apoe_1_matches_updated_v2_snippet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up Similar Encounters on Patient Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\4\\apoe_4_matches_updated_pruned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_merged_sequences = []\n",
    "\n",
    "# for i in tqdm(unique_empi):\n",
    "#     merged_sequences = df[df[\"EMPI\"] == i][\"merged_regex_sent\"].values\n",
    "#     #print(\"MERGED: \", merged_sequences, \"\\n\")\n",
    "\n",
    "#     counter = 0\n",
    "#     for j in merged_sequences:\n",
    "# #         choices = []\n",
    "        \n",
    "# #         for k in merged_sequences:\n",
    "# #             if (k != j):\n",
    "# #                 choices.append(k)\n",
    "# #         choices = merged_sequences[0:counter]\n",
    "# #         choices += merged_sequences[counter+1:]\n",
    "#         #print(\"\\n------------------\")\n",
    "#         choices = list(merged_sequences[0:counter]) + list(merged_sequences[counter+1:])\n",
    "#         result = get_match(j, choices)\n",
    "#         score = result[0][1]\n",
    "#         txt = result[0][0]\n",
    "        \n",
    "#         if (score >= 90):\n",
    "#             # 90 is duplicate string threshold\n",
    "#             idx = df[df[\"merged_regex_sent\"] == str(txt)].index[0]\n",
    "#             df.drop([idx])\n",
    "        \n",
    "#         counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PatientEncounterID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PatientEncounterID\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='merged_regex_sent', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem1 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\BioBank\\sd587_03212118253982866_6375194824947826181_Dem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\4\\apoe_4_matches_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ((7509, 7518), (7735, 7741), (7745, 7754), (7858, 7862))\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    a = x[i][0]\n",
    "    b = x[i][1]\n",
    "    y.append((a,b))\n",
    "print(str(y))\n",
    "z = len(p1[(p1[\"regex_location\"] == str(y)) & (p1[\"PatientEncounterID\"] == int(encounters[21]))]['NoteTXT'].values[1])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(181, 390),\n",
    "  (479, 688),\n",
    "  (2048, 2269),\n",
    "  (6184, 6390),\n",
    "  (6584, 6821),\n",
    "  (7450, 7659),\n",
    "  (7716, 7932),\n",
    "  (9864, 10072),\n",
    "  (11192, 11648),\n",
    "  (12118, 12627)]\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    a = x[i][0]\n",
    "    b = x[i][1]\n",
    "    y.append((a,b))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p1[(p1[\"regex_location\"] == \"[\" +str(subset_loc[1]) + \"]\") & (p1[\"PatientEncounterID\"] == int(encounters[1]))]['NoteTXT'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'at sleeping with his legs elevated on a pillow at night does help his leg swelling. Recommended Triamcinolone 0.1% ointment every morning and Mupirocin ointment at night, risks/benefits reviewed. Path'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
