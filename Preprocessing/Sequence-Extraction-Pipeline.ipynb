{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "#  Author: Tanish Tyagi\n",
    "#  \n",
    "\n",
    
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "current_date = date.today()\n",
    "#%load_ext autotime\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib_venn import venn2, venn3\n",
    "import warnings\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from more_itertools import unique_everseen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import pandasql as ps\n",
    "import seaborn as sns\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in List of Regex Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = pd.read_csv(r\"keywords.csv\") #(r\"regex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = regex[\"REGEX\"].to_list()\n",
    "c = regex[\"CASE\"].to_list()\n",
    "p_list = []\n",
    "\n",
    "for i in range(len(k)):\n",
    "    if (c[i] == \"case\"):\n",
    "        p_list.append(re.compile(k[i]))\n",
    "    else:\n",
    "        p_list.append(re.compile(k[i], re.IGNORECASE))\n",
    "print(len(p_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = pd.read_csv(r\"C:\\users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Bert\\john_hsu_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = john_hsu[john_hsu[\"NoteTXT\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = john_hsu.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Regex Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for note in tqdm(john_hsu[\"NoteTXT\"]):\n",
    "    curr = []\n",
    "    for p in (p_list):\n",
    "        m = list(set(re.findall(p, note)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        if (m != []):\n",
    "            for i in range(len(m)):\n",
    "                l.append(m[i])\n",
    "    \n",
    "john_hsu[\"regex_matches\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows that don't have a regex match\n",
    "john_hsu = john_hsu[john_hsu[\"regex_matches\"] != \"[]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Character Locations for all Regex Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "\n",
    "for r in tqdm(john_hsu['NoteTXT']):\n",
    "    curr_loc = []\n",
    "    for p in p_list:\n",
    "         for match in re.finditer(p ,r):\n",
    "            curr_loc.append(match.span())\n",
    "            \n",
    "    if (curr_loc == []):\n",
    "        print(\"STOP\")\n",
    "        break\n",
    "        \n",
    "    locations.append(curr_loc)\n",
    "    \n",
    "john_hsu[\"regex_location\"] = locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = john_hsu.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Context Windows for all Matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_windows(i):\n",
    "    merged = []\n",
    "    context_length_one_direction = 100\n",
    "    \n",
    "    for j in range(len(john_hsu[\"regex_location\"][i])):\n",
    "        start = john_hsu[\"regex_location\"][i][j][0]\n",
    "        end = john_hsu[\"regex_location\"][i][j][1]\n",
    "    \n",
    "        prune_start = False\n",
    "        prune_end = False\n",
    "        if (start - context_length_one_direction <= 0):\n",
    "            start = 0\n",
    "            prune_start = True\n",
    "        if (end + context_length_one_direction >= len(john_hsu[\"NoteTXT\"][i]) - 1):\n",
    "            end = len(john_hsu[\"NoteTXT\"][i]) - 1\n",
    "            prune_end = True\n",
    "\n",
    "        if (prune_start and prune_end):\n",
    "            merged.append((start, end))\n",
    "        else:\n",
    "            if (prune_start):\n",
    "                merged.append((start, end + context_length_one_direction))\n",
    "            if (prune_end):\n",
    "                merged.append((start - context_length_one_direction, end))\n",
    "\n",
    "        if (not prune_start and not prune_end):\n",
    "            merged.append((start - context_length_one_direction, end + context_length_one_direction))\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_locs = []\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "     merged_locs.append(create_context_windows(i))\n",
    "\n",
    "john_hsu[\"merged_row_location\"] = merged_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all Context Windows on the Note Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(intervals):  \n",
    "    ans = []\n",
    "    i = 1\n",
    "    intervals.sort()\n",
    "    tempans = intervals[0]\n",
    "    \n",
    "    while i < len(intervals):\n",
    "        # seeing if context windows overlap with each other - if they do merge them together\n",
    "        if tempans[0] <= intervals[i][1] and tempans[1] >= intervals[i][0]:\n",
    "            newtempans = [min(tempans[0] , intervals[i][0]) , max(tempans[1] , intervals[i][1])]\n",
    "            tempans = newtempans\n",
    "            i += 1\n",
    "        else:\n",
    "            ans.append(tempans)\n",
    "            tempans = intervals[i]\n",
    "            i += 1\n",
    "            \n",
    "    ans.append(tempans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_locs = []\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "     merged_locs.append(merge(john_hsu[\"merged_row_location\"][i]))\n",
    "\n",
    "john_hsu[\"merged_row_location\"] = merged_locs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences from Merged Context Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_context_windows(i, col):\n",
    "    sequence = \"\"\n",
    "    \n",
    "    for j in range(len(john_hsu[col][i])):\n",
    "        if j > 0 and sequence != \"\":\n",
    "            sequence += \" ----- \"\n",
    "            \n",
    "        start = john_hsu[col][i][j][0]\n",
    "        end = john_hsu[col][i][j][1]\n",
    "        \n",
    "        sequence += john_hsu[\"NoteTXT\"][i][start : end]\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = []\n",
    "\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "    seqs.append(pull_context_windows(i, \"merged_row_location\"))\n",
    "\n",
    "john_hsu[\"regex_sent\"] = seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing all Sequences into ClincalBERT Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_bert_tokenize(i, col):\n",
    "    tokens = tokenizer.encode_plus(john_hsu[col][i], add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    return (len(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "    token_lens.append(clinical_bert_tokenize(i, \"regex_sent\"))\n",
    "\n",
    "john_hsu[\"token_length\"] = token_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding all Sequences such that they are as close as possible to 512 Clincial Bert Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu[\"padded_merged_regex_location\"] = john_hsu[\"merged_row_location\"].copy()\n",
    "\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "    john_hsu[\"padded_merged_regex_location\"][i] = list((john_hsu[\"padded_merged_regex_location\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_context_windows(i):\n",
    "    if (john_hsu[\"token_length\"][i] <= 400):\n",
    "        locs = list((john_hsu[\"merged_row_location\"][i]))\n",
    "        length = len(locs)\n",
    "        start = locs[0][0] \n",
    "        end = locs[length - 1][1]\n",
    "        \n",
    "        #print(start, end)\n",
    "                \n",
    "        seq_length = 1000\n",
    "        length_to_add_one_side = int((seq_length - john_hsu[\"sequence_length\"][i]) / 2)\n",
    "        \n",
    "        # print(length_to_add_one_side, john_hsu[\"sequence_length\"][i])\n",
    "        \n",
    "        prune_start = False\n",
    "        prune_end = False\n",
    "        if (start - length_to_add_one_side <= 0):\n",
    "            start = 0\n",
    "            prune_start = True\n",
    "        if (end + length_to_add_one_side >= len(john_hsu[\"NoteTXT\"][i]) - 1):\n",
    "            end = len(john_hsu[\"NoteTXT\"][i]) - 1\n",
    "            prune_end = True\n",
    "            \n",
    "        # print(prune_start, prune_end)\n",
    "\n",
    "        if (prune_start):\n",
    "            end += length_to_add_one_side\n",
    "            if (end + length_to_add_one_side <= len(john_hsu[\"NoteTXT\"][i])):\n",
    "                end += length_to_add_one_side\n",
    "        if (prune_end):\n",
    "            start -= length_to_add_one_side\n",
    "            if (start - length_to_add_one_side >= 0):\n",
    "                start -= length_to_add_one_side\n",
    "        if (not prune_start and not prune_end):\n",
    "            start -= length_to_add_one_side\n",
    "            end += length_to_add_one_side\n",
    "            \n",
    "        locs = [list(x) for x in locs]\n",
    "        locs[0][0] = start\n",
    "        locs[length - 1][1] = end\n",
    "        \n",
    "        john_hsu[\"padded_merged_regex_location\"][i] = locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(john_hsu))):\n",
    "    generate_padded_context_windows(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seqs = []\n",
    "\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "    padded_seqs.append(pull_context_windows(i, \"padded_merged_regex_location\"))\n",
    "\n",
    "john_hsu[\"padded_regex_sent\"] = padded_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Cleaning to Regex Matches and Sequence columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu['regex_matches'] = john_hsu['regex_matches'].astype(str)\n",
    "john_hsu['regex_matches'] = john_hsu['regex_matches'].str.replace(\"\\[\\]\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu['padded_regex_sent'] = john_hsu['padded_regex_sent'].astype(str)\n",
    "john_hsu['padded_regex_sent'] = john_hsu['padded_regex_sent'].str.replace('[', '')\n",
    "john_hsu['padded_regex_sent'] = john_hsu['padded_regex_sent'].str.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = john_hsu.dropna(subset=[\"padded_regex_sent\"])\n",
    "john_hsu['padded_regex_sent'] = john_hsu['padded_regex_sent'].str.replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Sequences more readable by following below regex substitutions:\n",
    "1. Replace combination of 3 spaces + bullet point + 1 space with semicolon\n",
    "2. Replace 4 more or spaces with a single space\n",
    "3. Replace 2 spaces with newline character\n",
    "4. Replace 4 spaces with 2 newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_preprocessing(note):\n",
    "    note = re.sub(r'   • ', ' ; ', note)\n",
    "    note = re.sub(r'[ ]{4,}', ' ', note)\n",
    "    note = re.sub(r'  ', '\\n', note)\n",
    "    note = re.sub(r'    ', '\\n\\n', note)\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(john_hsu))):\n",
    "    john_hsu.at[i, \"padded_regex_sent_preprocessed\"] = note_preprocessing(john_hsu.at[i, \"padded_regex_sent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu[\"sequence_length\"] = john_hsu['padded_regex_sent_preprocessed'].astype(str).map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking count of ClinicalBERT Tokens for Padded Preprocessed Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_token_lens = []\n",
    "\n",
    "for i in tqdm(range(len(john_hsu))):\n",
    "    padded_token_lens.append(clinical_bert_tokenize(i, \"padded_regex_sent_preprocessed\"))\n",
    "\n",
    "john_hsu[\"padded_token_length\"] = padded_token_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu[\"padded_token_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Extraneous Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu = john_hsu.drop_duplicates(subset = [\"padded_regex_sent_preprocessed\", \"NoteID\"], keep = \"last\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Dataframe to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu[:1000].to_csv(r\"Matches/john_hsu_dataset_1000_padding.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_hsu.to_csv(r\"Matches/john_hsu_dataset_matches_padding.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Count of Sequences Per Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sequence_count = pd.DataFrame(columns = [\"PatientID\", \"sequence_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(john_hsu[\"PatientID\"].unique())):\n",
    "    count = len(john_hsu[john_hsu[\"PatientID\"] == john_hsu[\"PatientID\"].unique()[i]])\n",
    "    patient_sequence_count.loc[i] = (john_hsu[\"PatientID\"].unique()[i], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sequence_count.to_csv(r\"Matches/patient_level_sequence_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sequence_count = pd.read_csv(r\"Matches/patient_level_sequence_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sequence_count[\"sequence_count\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
