{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "PMfUEuGw7BvY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "610kf_Y97NHe",
    "outputId": "38d89897-db34-49f4-ef5f-c9bf41d6ad16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Z8170308</td>\n",
       "      <td>medications allergies she is allergic to oxyco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Z9021665</td>\n",
       "      <td>follow commands follows step commands follow c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Z8485823</td>\n",
       "      <td>cancer brother testicular cancer u coronary ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Z10380050</td>\n",
       "      <td>are progressing faster he is very motivated to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Z6985118</td>\n",
       "      <td>relation age of onset u cirrhosis father cirrh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>8651</td>\n",
       "      <td>Z7869100</td>\n",
       "      <td>story bilateral knee arthroscopy u vasectomy f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8652</th>\n",
       "      <td>8652</td>\n",
       "      <td>Z8631790</td>\n",
       "      <td>female who was evaluated today due to language...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>8653</td>\n",
       "      <td>Z15536786</td>\n",
       "      <td>list items addressed this visit mild cognitive...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>8654</td>\n",
       "      <td>Z7156572</td>\n",
       "      <td>ors or hesitation language was fluent without ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>8655</td>\n",
       "      <td>Z11066758</td>\n",
       "      <td>antibiotics other see comments itching family ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8656 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 patient_id  \\\n",
       "0              0   Z8170308   \n",
       "1              1   Z9021665   \n",
       "2              2   Z8485823   \n",
       "3              3  Z10380050   \n",
       "4              4   Z6985118   \n",
       "...          ...        ...   \n",
       "8651        8651   Z7869100   \n",
       "8652        8652   Z8631790   \n",
       "8653        8653  Z15536786   \n",
       "8654        8654   Z7156572   \n",
       "8655        8655  Z11066758   \n",
       "\n",
       "                                               sequence  label  \n",
       "0     medications allergies she is allergic to oxyco...      1  \n",
       "1     follow commands follows step commands follow c...      0  \n",
       "2     cancer brother testicular cancer u coronary ar...      1  \n",
       "3     are progressing faster he is very motivated to...      2  \n",
       "4     relation age of onset u cirrhosis father cirrh...      1  \n",
       "...                                                 ...    ...  \n",
       "8651  story bilateral knee arthroscopy u vasectomy f...      1  \n",
       "8652  female who was evaluated today due to language...      2  \n",
       "8653  list items addressed this visit mild cognitive...      2  \n",
       "8654  ors or hesitation language was fluent without ...      0  \n",
       "8655  antibiotics other see comments itching family ...      1  \n",
       "\n",
       "[8656 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"input.csv\") \n",
    "#tanish_df = pd.read_csv(\"Data/input_tanish.csv\")\n",
    "\n",
    "#mappings = {1: 0, 2: 1, 3:2}\n",
    "\n",
    "#train_df.label = [mappings[item] for item in train_df.label]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kwfttvx-7RbZ"
   },
   "outputs": [],
   "source": [
    "X = train_df[\"sequence\"]\n",
    "y = train_df[\"label\"]\n",
    "\n",
    "y_label = y.to_numpy()\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X,y,random_state=0,test_size=0.1, stratify=y_label)\n",
    "\n",
    "y_train_valid_label = y_train_valid.to_numpy()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, random_state=0, test_size=0.1, stratify=y_train_valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7011\n",
      "866\n",
      "780\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5988    hasic errors followed simple and complex comma...\n",
      "4844    minutes away patient is the hcppoa for his fat...\n",
      "1703    has boston senior health care and laundry serv...\n",
      "7053    ve family history mother died at father metast...\n",
      "2057    ii dm cb retinopathy npdr od pdr os sp panreti...\n",
      "                              ...                        \n",
      "2353    drinking in social history social history narr...\n",
      "2091    started celexa in after death of father she wa...\n",
      "458     devoid of acute suicidal homicidal or psychoti...\n",
      "5039    standard drinks comment raresocial u drug use ...\n",
      "7793    will check b and thyroid function recent mri f...\n",
      "Name: sequence, Length: 7011, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3793\n",
       "0    1972\n",
       "2    1246\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "TyjWzdvA7Yt-",
    "outputId": "20828434-583a-498c-b555-fe3d38d58fc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaafpat</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaox</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacavir</th>\n",
       "      <th>abated</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zietmen</th>\n",
       "      <th>zing</th>\n",
       "      <th>zocor</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zolpidem</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zostavax</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zucker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaafpat  aand  aaox  ab  abacavir  abated  abbey  abby  abc  ...  \\\n",
       "0    0        0     0     0   0         0       0      0     0    0  ...   \n",
       "1    0        0     0     0   0         0       0      0     0    0  ...   \n",
       "2    0        0     0     0   0         0       0      0     0    0  ...   \n",
       "3    0        0     0     0   0         0       0      0     0    0  ...   \n",
       "4    0        0     0     0   0         0       0      0     0    0  ...   \n",
       "\n",
       "   zietmen  zing  zocor  zoloft  zolpidem  zoo  zoom  zostavax  zoster  zucker  \n",
       "0        0     0      0       0         0    0     0         0       0       0  \n",
       "1        0     0      0       0         0    0     0         0       0       0  \n",
       "2        0     0      0       0         0    0     0         0       0       0  \n",
       "3        0     0      0       0         0    0     0         0       0       0  \n",
       "4        0     0      0       0         0    0     0         0       0       0  \n",
       "\n",
       "[5 rows x 10645 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\",analyzer='word')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "count_valid = count_vectorizer.transform(X_valid)\n",
    "#count_tanish = count_vectorizer.transform(tanish_df[\"sequence\"].to_numpy())\n",
    "\n",
    "# Create the CountVectorizer DataFrame: count_train\n",
    "count_train = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the CountVectorizer DataFrame: test_df\n",
    "count_test = pd.DataFrame(count_test.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "count_valid = pd.DataFrame(count_valid.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "#count_tanish = pd.DataFrame(count_tanish.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "#count_tanish.head()\n",
    "count_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vectorizer.vocabulary_, open(\"feature.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10645"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "7xtHzY0F7cBD",
    "outputId": "ceef9692-5ffe-43a5-ec81-72ed65705c0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",analyzer='word')\n",
    "tfidf_train= tfidf_vectorizer.fit(X_train)\n",
    "tfidf_test = tfidf_vectorizer.fit(X_test)\n",
    "tfidf_valid = tfidf_vectorizer.fit(X_valid)\n",
    "#tfidf_tanish = tfidf_vectorizer.transform(tanish_df[\"sequence\"].to_numpy())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "# tfidf_train  = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "# tfidf_test  = pd.DataFrame(tfidf_test.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "# tfidf_valid  = pd.DataFrame(tfidf_valid.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "#tfidf_tanish = pd.DataFrame(tfidf_tanish.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "#tfidf_valid.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14',\n",
       " 'aaafpat',\n",
       " 'aaox',\n",
       " 'abbreviated',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'ablation',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'abraczsinskas',\n",
       " 'abramson',\n",
       " 'abs',\n",
       " 'absent',\n",
       " 'absentminded',\n",
       " 'abstra',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'abuse',\n",
       " 'ac',\n",
       " 'acarbose',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidents',\n",
       " 'accompanie',\n",
       " 'accompanied',\n",
       " 'according',\n",
       " 'ace',\n",
       " 'acei',\n",
       " 'acetaminophen',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acidosis',\n",
       " 'acne',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acute',\n",
       " 'ad',\n",
       " 'adaches',\n",
       " 'addendum',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'ade',\n",
       " 'adenocarcinoma',\n",
       " 'adenoidectomy',\n",
       " 'adenoma',\n",
       " 'adenopathy',\n",
       " 'adequate',\n",
       " 'adhd',\n",
       " 'adhesive',\n",
       " 'adjustment',\n",
       " 'adl',\n",
       " 'adliadl',\n",
       " 'adls',\n",
       " 'adlsindependent',\n",
       " 'admi',\n",
       " 'administered',\n",
       " 'admission',\n",
       " 'admitted',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advised',\n",
       " 'aerobic',\n",
       " 'afe',\n",
       " 'afebrile',\n",
       " 'affe',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'afib',\n",
       " 'aforementioned',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agitation',\n",
       " 'agnosis',\n",
       " 'ago',\n",
       " 'agoraphobia',\n",
       " 'agrees',\n",
       " 'ahs',\n",
       " 'ahvhdelusions',\n",
       " 'ahvhpi',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aids',\n",
       " 'ained',\n",
       " 'air',\n",
       " 'airway',\n",
       " 'ake',\n",
       " 'al',\n",
       " 'alc',\n",
       " 'alcohol',\n",
       " 'alcoholdrug',\n",
       " 'alcoholic',\n",
       " 'alcoholism',\n",
       " 'alculated',\n",
       " 'alemisis',\n",
       " 'alert',\n",
       " 'alertappropriate',\n",
       " 'alertness',\n",
       " 'alive',\n",
       " 'alivean',\n",
       " 'alk',\n",
       " 'alleles',\n",
       " 'allergen',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'alleviating',\n",
       " 'als',\n",
       " 'alt',\n",
       " 'altered',\n",
       " 'alternating',\n",
       " 'alzheimer',\n",
       " 'alzheimers',\n",
       " 'ama',\n",
       " 'amb',\n",
       " 'ambulates',\n",
       " 'ambulation',\n",
       " 'ambulatory',\n",
       " 'amherst',\n",
       " 'ampceftriaxonevanc',\n",
       " 'ampicillin',\n",
       " 'ams',\n",
       " 'amy',\n",
       " 'amyotrophic',\n",
       " 'anal',\n",
       " 'analogues',\n",
       " 'anaphylaxis',\n",
       " 'ancestry',\n",
       " 'andor',\n",
       " 'anemia',\n",
       " 'aneurysm',\n",
       " 'angiodema',\n",
       " 'angioedema',\n",
       " 'anhedonia',\n",
       " 'anisocoria',\n",
       " 'ankle',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anorexia',\n",
       " 'answer',\n",
       " 'anterior',\n",
       " 'antho',\n",
       " 'anthony',\n",
       " 'antibiotics',\n",
       " 'antibody',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'ao',\n",
       " 'aorta',\n",
       " 'aortic',\n",
       " 'aox',\n",
       " 'apartment',\n",
       " 'aphasia',\n",
       " 'apnea',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appendectomy',\n",
       " 'appetite',\n",
       " 'appetitesleeping',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approval',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'apraxia',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'areas',\n",
       " 'aricept',\n",
       " 'arm',\n",
       " 'arms',\n",
       " 'armusoft',\n",
       " 'arom',\n",
       " 'arose',\n",
       " 'arousal',\n",
       " 'arousalattentioncognitioncommunication',\n",
       " 'arousalattentioncommunicationcognition',\n",
       " 'arrhyth',\n",
       " 'arrhythmia',\n",
       " 'artery',\n",
       " 'arthritis',\n",
       " 'arthropathy',\n",
       " 'arthroplasty',\n",
       " 'arthroscopy',\n",
       " 'articulates',\n",
       " 'articulation',\n",
       " 'artificial',\n",
       " 'ary',\n",
       " 'asa',\n",
       " 'asbestos',\n",
       " 'ascending',\n",
       " 'ascvd',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'aspirin',\n",
       " 'ass',\n",
       " 'assess',\n",
       " 'assessed',\n",
       " 'assesseed',\n",
       " 'assessment',\n",
       " 'assessmentimpressions',\n",
       " 'assessmentplan',\n",
       " 'assessmentufd',\n",
       " 'assets',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assoc',\n",
       " 'associa',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'associations',\n",
       " 'assured',\n",
       " 'ast',\n",
       " 'asthma',\n",
       " 'asymmetric',\n",
       " 'asymmetry',\n",
       " 'asymptomatic',\n",
       " 'ataxia',\n",
       " 'ataxiaufd',\n",
       " 'ate',\n",
       " 'atenolol',\n",
       " 'ateral',\n",
       " 'athappilly',\n",
       " 'ating',\n",
       " 'ations',\n",
       " 'ativan',\n",
       " 'atrial',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attempts',\n",
       " 'attemptscompleted',\n",
       " 'attends',\n",
       " 'attentio',\n",
       " 'attention',\n",
       " 'attentionc',\n",
       " 'attentioncon',\n",
       " 'attentionconc',\n",
       " 'attentionconcentration',\n",
       " 'attentionconcentrationintact',\n",
       " 'attentionmemory',\n",
       " 'attentive',\n",
       " 'attitude',\n",
       " 'attributes',\n",
       " 'audit',\n",
       " 'auditory',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntsuncle',\n",
       " 'auntuncle',\n",
       " 'auscultation',\n",
       " 'autobiographical',\n",
       " 'averag',\n",
       " 'average',\n",
       " 'averse',\n",
       " 'avg',\n",
       " 'avh',\n",
       " 'avoid',\n",
       " 'avr',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'awarenessjudgment',\n",
       " 'awarenesssafety',\n",
       " 'away',\n",
       " 'awoke',\n",
       " 'axillary',\n",
       " 'backwards',\n",
       " 'backyard',\n",
       " 'bactrim',\n",
       " 'bad',\n",
       " 'balance',\n",
       " 'banging',\n",
       " 'barbiturates',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barrierslumbar',\n",
       " 'basal',\n",
       " 'based',\n",
       " 'baseline',\n",
       " 'basic',\n",
       " 'bathing',\n",
       " 'beats',\n",
       " 'bed',\n",
       " 'bedside',\n",
       " 'bedtime',\n",
       " 'beer',\n",
       " 'behavior',\n",
       " 'behavioral',\n",
       " 'behaviors',\n",
       " 'believes',\n",
       " 'ben',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benign',\n",
       " 'benson',\n",
       " 'berman',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bf',\n",
       " 'bialterally',\n",
       " 'biceps',\n",
       " 'bicycling',\n",
       " 'bid',\n",
       " 'biggest',\n",
       " 'bil',\n",
       " 'bila',\n",
       " 'bilateral',\n",
       " 'bilaterally',\n",
       " 'bilaterallyufd',\n",
       " 'bile',\n",
       " 'biliary',\n",
       " 'bims',\n",
       " 'biopsy',\n",
       " 'bipolar',\n",
       " 'birth',\n",
       " 'bit',\n",
       " 'bizarre',\n",
       " 'bl',\n",
       " 'bladder',\n",
       " 'ble',\n",
       " 'bleed',\n",
       " 'bleeding',\n",
       " 'blindness',\n",
       " 'bloating',\n",
       " 'blockage',\n",
       " 'blood',\n",
       " 'blurred',\n",
       " 'blurry',\n",
       " 'bmd',\n",
       " 'bmi',\n",
       " 'bmp',\n",
       " 'bolus',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'book',\n",
       " 'born',\n",
       " 'boston',\n",
       " 'bottlesday',\n",
       " 'bound',\n",
       " 'bowel',\n",
       " 'box',\n",
       " 'bp',\n",
       " 'bpad',\n",
       " 'br',\n",
       " 'brachioradialis',\n",
       " 'bradford',\n",
       " 'brain',\n",
       " 'brca',\n",
       " 'break',\n",
       " 'breast',\n",
       " 'breath',\n",
       " 'breathing',\n",
       " 'brian',\n",
       " 'brief',\n",
       " 'brigham',\n",
       " 'briskly',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'brokerage',\n",
       " 'brother',\n",
       " 'brotheralive',\n",
       " 'brotherdmlifestyle',\n",
       " 'brothers',\n",
       " 'bruising',\n",
       " 'bsip',\n",
       " 'buffalo',\n",
       " 'bulk',\n",
       " 'bumped',\n",
       " 'bun',\n",
       " 'burke',\n",
       " 'bursitis',\n",
       " 'business',\n",
       " 'buts',\n",
       " 'buy',\n",
       " 'bwfh',\n",
       " 'bwh',\n",
       " 'bx',\n",
       " 'bypass',\n",
       " 'ca',\n",
       " 'cabg',\n",
       " 'cabro',\n",
       " 'cad',\n",
       " 'cadcva',\n",
       " 'cadence',\n",
       " 'cadf',\n",
       " 'cadmi',\n",
       " 'cal',\n",
       " 'calcifications',\n",
       " 'calcium',\n",
       " 'calculate',\n",
       " 'calculation',\n",
       " 'calculations',\n",
       " 'california',\n",
       " 'caller',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'came',\n",
       " 'camgf',\n",
       " 'camp',\n",
       " 'cance',\n",
       " 'cancer',\n",
       " 'cancers',\n",
       " 'cane',\n",
       " 'cape',\n",
       " 'caplanneurol',\n",
       " 'caps',\n",
       " 'capsule',\n",
       " 'capsulotomy',\n",
       " 'car',\n",
       " 'carbidopaleva',\n",
       " 'carbonatevitamin',\n",
       " 'carcinoma',\n",
       " 'card',\n",
       " 'cardiac',\n",
       " 'cardiacother',\n",
       " 'cardioembolic',\n",
       " 'cardiologist',\n",
       " 'cardiomyopathy',\n",
       " 'cardiopulmonary',\n",
       " 'cardiovascular',\n",
       " 'care',\n",
       " 'careend',\n",
       " 'caregiver',\n",
       " 'carotid',\n",
       " 'carpal',\n",
       " 'carryover',\n",
       " 'case',\n",
       " 'cataract',\n",
       " 'cataracts',\n",
       " 'categorical',\n",
       " 'catgut',\n",
       " 'catheter',\n",
       " 'cauc',\n",
       " 'caus',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'cavity',\n",
       " 'cayman',\n",
       " 'cb',\n",
       " 'cbc',\n",
       " 'cce',\n",
       " 'ccident',\n",
       " 'cco',\n",
       " 'cea',\n",
       " 'ceftriaxone',\n",
       " 'ceiling',\n",
       " 'celexa',\n",
       " 'celiac',\n",
       " 'cell',\n",
       " 'centered',\n",
       " 'centration',\n",
       " 'cephalosporin',\n",
       " 'ceps',\n",
       " 'cerebellar',\n",
       " 'cerebral',\n",
       " 'cerebrovascular',\n",
       " 'cerumen',\n",
       " 'cervical',\n",
       " 'cessation',\n",
       " 'cga',\n",
       " 'ch',\n",
       " 'chair',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'characterized',\n",
       " 'chart',\n",
       " 'checkbook',\n",
       " 'checking',\n",
       " 'checks',\n",
       " 'cheese',\n",
       " 'chem',\n",
       " 'chemistry',\n",
       " 'chemo',\n",
       " 'chemotherapy',\n",
       " 'chest',\n",
       " 'chf',\n",
       " 'chiatrist',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childbirth',\n",
       " 'children',\n",
       " 'chilles',\n",
       " 'chills',\n",
       " 'chloride',\n",
       " 'chocolate',\n",
       " 'chol',\n",
       " 'chole',\n",
       " 'cholecalciferol',\n",
       " 'cholecystectomy',\n",
       " 'cholesterol',\n",
       " 'chondroplasty',\n",
       " 'christopher',\n",
       " 'chronic',\n",
       " 'chronically',\n",
       " 'chu',\n",
       " 'cigarettes',\n",
       " 'cigars',\n",
       " 'cinations',\n",
       " 'circle',\n",
       " 'circumcised',\n",
       " 'circumduction',\n",
       " 'circumstances',\n",
       " 'circumstantial',\n",
       " 'cirrhosis',\n",
       " 'citalo',\n",
       " 'citalopram',\n",
       " 'city',\n",
       " 'ciwa',\n",
       " 'cjd',\n",
       " 'ck',\n",
       " 'ckd',\n",
       " 'clarification',\n",
       " 'class',\n",
       " 'clea',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clerk',\n",
       " 'click',\n",
       " 'client',\n",
       " 'climber',\n",
       " 'clinic',\n",
       " 'clinical',\n",
       " 'clipping',\n",
       " 'clock',\n",
       " 'clonus',\n",
       " 'close',\n",
       " 'clot',\n",
       " 'clubbing',\n",
       " 'cm',\n",
       " 'cmp',\n",
       " 'cn',\n",
       " 'cns',\n",
       " 'coaching',\n",
       " 'coag',\n",
       " 'cocaine',\n",
       " 'code',\n",
       " 'coded',\n",
       " 'codeine',\n",
       " 'coffee',\n",
       " 'cog',\n",
       " 'cognition',\n",
       " 'cognitionmental',\n",
       " 'cognitionshort',\n",
       " 'cognitive',\n",
       " 'cognitivedialectical',\n",
       " 'cognitivelinguistic',\n",
       " 'cognitively',\n",
       " 'cogwheeling',\n",
       " 'coherent',\n",
       " 'colitis',\n",
       " 'college',\n",
       " 'colon',\n",
       " 'colonic',\n",
       " 'colonoscopy',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colorectal',\n",
       " 'coma',\n",
       " 'combining',\n",
       " 'come',\n",
       " 'comfortably',\n",
       " 'comm',\n",
       " 'commands',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commerical',\n",
       " 'committed',\n",
       " 'communication',\n",
       " 'communit',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'company',\n",
       " 'compensatory',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completing',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'complications',\n",
       " 'component',\n",
       " 'comportment',\n",
       " 'comprehension',\n",
       " 'comprehensioncognition',\n",
       " 'concentration',\n",
       " 'concentrationnormal',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerning',\n",
       " 'concerns',\n",
       " 'concernsfall',\n",
       " 'condition',\n",
       " 'condo',\n",
       " 'conducted',\n",
       " 'confirmed',\n",
       " 'confluent',\n",
       " 'confrontation',\n",
       " 'confused',\n",
       " 'confusion',\n",
       " 'congestive',\n",
       " 'congruent',\n",
       " 'conjugate',\n",
       " 'conjunctivitis',\n",
       " 'connect',\n",
       " 'cons',\n",
       " 'consanguinit',\n",
       " 'consistent',\n",
       " 'constant',\n",
       " 'consti',\n",
       " 'constipation',\n",
       " 'constitutional',\n",
       " 'construction',\n",
       " 'consult',\n",
       " 'consulted',\n",
       " 'consults',\n",
       " 'consumed',\n",
       " 'cont',\n",
       " 'contact',\n",
       " 'contacted',\n",
       " 'contd',\n",
       " 'content',\n",
       " 'contingency',\n",
       " 'contingent',\n",
       " 'continue',\n",
       " 'continues',\n",
       " 'contnues',\n",
       " 'contracture',\n",
       " 'contrast',\n",
       " 'contributory',\n",
       " 'control',\n",
       " 'controlled',\n",
       " 'conversant',\n",
       " 'conversation',\n",
       " 'cooperative',\n",
       " 'coordinate',\n",
       " 'coordination',\n",
       " 'copd',\n",
       " 'cord',\n",
       " 'cords',\n",
       " 'coronary',\n",
       " 'correct',\n",
       " 'correcting',\n",
       " 'correction',\n",
       " 'correctl',\n",
       " 'correctly',\n",
       " 'correlate',\n",
       " 'correlates',\n",
       " 'cortical',\n",
       " 'corticalsubcortical',\n",
       " 'cottage',\n",
       " 'cough',\n",
       " 'coumadin',\n",
       " 'counselin',\n",
       " 'counseling',\n",
       " 'count',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'coverag',\n",
       " 'covered',\n",
       " 'covid',\n",
       " 'cp',\n",
       " 'cr',\n",
       " 'crab',\n",
       " 'cramping',\n",
       " 'cranial',\n",
       " 'craniotomy',\n",
       " 'crash',\n",
       " 'crc',\n",
       " 'crea',\n",
       " 'cremens',\n",
       " 'crohns',\n",
       " 'crutchfeldjakob',\n",
       " 'crystal',\n",
       " 'ct',\n",
       " 'ctive',\n",
       " 'ctr',\n",
       " 'cts',\n",
       " 'cubital',\n",
       " 'cues',\n",
       " 'cuesrepitition',\n",
       " 'cuestactile',\n",
       " 'cuesverbal',\n",
       " 'cuff',\n",
       " 'culture',\n",
       " 'cups',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cv',\n",
       " 'cva',\n",
       " 'cvas',\n",
       " 'cvd',\n",
       " 'cvs',\n",
       " 'cw',\n",
       " 'cyanocobalamin',\n",
       " 'cyanosis',\n",
       " 'cycles',\n",
       " 'cyclobenzaprine',\n",
       " 'dad',\n",
       " 'dadada',\n",
       " 'daily',\n",
       " 'dal',\n",
       " 'daley',\n",
       " 'damage',\n",
       " 'data',\n",
       " 'dataresults',\n",
       " 'date',\n",
       " 'dates',\n",
       " 'datetime',\n",
       " 'daugher',\n",
       " 'daughter',\n",
       " 'david',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'ddx',\n",
       " 'dear',\n",
       " 'deasla',\n",
       " 'death',\n",
       " 'deborahs',\n",
       " 'dec',\n",
       " 'decade',\n",
       " 'decades',\n",
       " 'deceased',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decline',\n",
       " 'declining',\n",
       " 'deconditioning',\n",
       " 'decreased',\n",
       " 'deep',\n",
       " 'default',\n",
       " 'defect',\n",
       " 'deferred',\n",
       " 'defers',\n",
       " 'defic',\n",
       " 'defici',\n",
       " 'deficiency',\n",
       " 'deficit',\n",
       " 'deficits',\n",
       " 'deficitsconcentration',\n",
       " 'deficitsno',\n",
       " 'degenera',\n",
       " 'degeneration',\n",
       " 'degenerative',\n",
       " 'degree',\n",
       " 'dehydration',\n",
       " 'delayed',\n",
       " 'delirium',\n",
       " 'deliveries',\n",
       " 'delusional',\n",
       " 'delusions',\n",
       " 'demands',\n",
       " 'dementi',\n",
       " 'dementia',\n",
       " 'dementing',\n",
       " 'demonstrate',\n",
       " 'demonstrating',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'density',\n",
       " 'denying',\n",
       " 'depakote',\n",
       " 'department',\n",
       " 'dependence',\n",
       " 'depressed',\n",
       " 'depression',\n",
       " 'depressionanxiety',\n",
       " 'depressionubipolar',\n",
       " 'depressive',\n",
       " 'dermatitis',\n",
       " 'described',\n",
       " 'describes',\n",
       " 'descriptive',\n",
       " 'descriptors',\n",
       " 'destructive',\n",
       " 'detachment',\n",
       " 'detailed',\n",
       " 'details',\n",
       " 'detect',\n",
       " 'detox',\n",
       " 'detrol',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'developedufd',\n",
       " 'development',\n",
       " 'devoid',\n",
       " 'diabe',\n",
       " 'diabetes',\n",
       " 'diabetic',\n",
       " 'diagnosed',\n",
       " 'diagnoses',\n",
       " 'diagnosis',\n",
       " 'diazoxide',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diet',\n",
       " 'dietexercise',\n",
       " 'difference',\n",
       " 'differences',\n",
       " 'difficile',\n",
       " 'difficult',\n",
       " 'difficulties',\n",
       " 'difficulty',\n",
       " 'diffuse',\n",
       " 'diffusion',\n",
       " 'digit',\n",
       " 'digital',\n",
       " 'dilation',\n",
       " 'dimension',\n",
       " 'diminished',\n",
       " 'dinners',\n",
       " 'dipstick',\n",
       " 'dire',\n",
       " 'direct',\n",
       " 'directions',\n",
       " 'directiv',\n",
       " 'dis',\n",
       " 'disabilities',\n",
       " 'disases',\n",
       " 'disc',\n",
       " 'discharge',\n",
       " 'discharged',\n",
       " 'discontinue',\n",
       " 'discontinued',\n",
       " 'discrete',\n",
       " 'discs',\n",
       " 'discstenosis',\n",
       " 'discus',\n",
       " 'discuss',\n",
       " 'discussed',\n",
       " 'discussedpt',\n",
       " 'dise',\n",
       " 'disease',\n",
       " 'dishes',\n",
       " 'disorder',\n",
       " 'disorders',\n",
       " 'disorganized',\n",
       " 'dispense',\n",
       " 'displays',\n",
       " 'dispo',\n",
       " 'disputation',\n",
       " 'distal',\n",
       " 'distant',\n",
       " 'distorted',\n",
       " 'distractability',\n",
       " 'distracted',\n",
       " 'distress',\n",
       " 'distressed',\n",
       " 'disturbance',\n",
       " 'disturbances',\n",
       " 'diverticulosis',\n",
       " 'dizziness',\n",
       " 'djd',\n",
       " 'dlbcl',\n",
       " 'dm',\n",
       " 'dmii',\n",
       " 'document',\n",
       " 'documentation',\n",
       " 'documented',\n",
       " 'doe',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'domestic',\n",
       " 'dominant',\n",
       " 'donated',\n",
       " 'dondoff',\n",
       " 'door',\n",
       " 'dopa',\n",
       " 'dorsalis',\n",
       " 'dorzolamide',\n",
       " 'dosage',\n",
       " 'dose',\n",
       " 'dotwb',\n",
       " 'dow',\n",
       " 'dowb',\n",
       " 'downgoing',\n",
       " 'downturn',\n",
       " 'doxycycline',\n",
       " 'dr',\n",
       " 'drama',\n",
       " 'drank',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'dreamsnightmares',\n",
       " 'dressing',\n",
       " 'drew',\n",
       " 'drift',\n",
       " 'drink',\n",
       " 'drinkmonth',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'drowsy',\n",
       " 'drs',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'dry',\n",
       " 'dt',\n",
       " 'dtrs',\n",
       " 'duct',\n",
       " 'dupuytrens',\n",
       " 'dust',\n",
       " 'duxbury',\n",
       " 'dv',\n",
       " 'dvt',\n",
       " 'dx',\n",
       " 'dysarthria',\n",
       " 'dysautonomia',\n",
       " 'dysfunction',\n",
       " 'dysmetria',\n",
       " 'dysphoric',\n",
       " 'dyspnea',\n",
       " 'dystrophy',\n",
       " 'dz',\n",
       " 'eager',\n",
       " 'ear',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earlyonset',\n",
       " 'ease',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'echolalic',\n",
       " 'ect',\n",
       " 'ectatic',\n",
       " 'ed',\n",
       " 'edema',\n",
       " 'edibles',\n",
       " 'educated',\n",
       " 'education',\n",
       " 'eeg',\n",
       " 'eent',\n",
       " 'effort',\n",
       " 'egfr',\n",
       " 'eid',\n",
       " 'ekg',\n",
       " 'el',\n",
       " 'elbow',\n",
       " 'elderly',\n",
       " 'elementary',\n",
       " 'elev',\n",
       " 'elevated',\n",
       " 'elevator',\n",
       " 'elicit',\n",
       " 'elicited',\n",
       " 'embolism',\n",
       " 'emergency',\n",
       " 'emerich',\n",
       " 'emphasis',\n",
       " 'emphysema',\n",
       " 'en',\n",
       " 'enactment',\n",
       " 'encounter',\n",
       " 'encounters',\n",
       " 'encouraged',\n",
       " 'end',\n",
       " ...]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_valid.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vectorizer, open(\"feature.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df = pd.concat([tfidf_train, y_train.reset_index(drop=True)], axis=1) #ONLY run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_features_df = pd.concat([tfidf_test, y_test.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_valid_features_df = pd.concat([tfidf_valid, y_valid.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7011, 10646)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_data = tfidf_train_features_df.values\n",
    "X_smote, y_smote = tfidf_train_features_df_data[:, :-1], tfidf_train_features_df_data[:, -1]\n",
    "\n",
    "tfidf_test_features_df_data = tfidf_test_features_df.values\n",
    "X_smote_test, y_smote_test = tfidf_test_features_df_data[:, :-1], tfidf_test_features_df_data[:, -1]\n",
    "\n",
    "tfidf_valid_features_df_data = tfidf_valid_features_df.values\n",
    "X_smote_valid, y_smote_valid = tfidf_valid_features_df_data[:, :-1], tfidf_valid_features_df_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data after SMOTE\n",
      "Class=0, n=3793\n",
      "Class=1, n=3793\n",
      "Class=2, n=3793\n"
     ]
    }
   ],
   "source": [
    "#X_smote_test, y_smote_test = \n",
    "oversample = SMOTE()\n",
    "X_train_new, y_train_new = oversample.fit_resample(X_smote, y_smote)\n",
    "\n",
    "counter = Counter(y_train_new)\n",
    "print(\"Train Data after SMOTE\")\n",
    "for k,v in counter.items():\n",
    "    print('Class=%d, n=%d' % (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data after SMOTE\n",
      "Class=1, n=469\n",
      "Class=2, n=469\n",
      "Class=0, n=469\n"
     ]
    }
   ],
   "source": [
    "oversample2 = SMOTE()\n",
    "X_test_new, y_test_new = oversample2.fit_resample(X_smote_test, y_smote_test)\n",
    "counter2 = Counter(y_test_new)\n",
    "print(\"Test Data after SMOTE\")\n",
    "for k,v in counter2.items():\n",
    "    print('Class=%d, n=%d' % (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data after SMOTE\n",
      "Class=1, n=421\n",
      "Class=2, n=421\n",
      "Class=0, n=421\n"
     ]
    }
   ],
   "source": [
    "oversample3 = SMOTE()\n",
    "X_valid_new, y_valid_new = oversample3.fit_resample(X_smote_valid, y_smote_valid)\n",
    "counter3 = Counter(y_valid_new)\n",
    "print(\"Test Data after SMOTE\")\n",
    "for k,v in counter3.items():\n",
    "    print('Class=%d, n=%d' % (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "X_train_new = pd.DataFrame(X_train_new)\n",
    "y_train_new = pd.Series(y_train_new)\n",
    "\n",
    "X_test_new = pd.DataFrame(X_test_new)\n",
    "y_test_new = pd.Series(y_test_new)\n",
    "\n",
    "X_valid_new = pd.DataFrame(X_valid_new)\n",
    "y_valid_new = pd.Series(y_valid_new)\n",
    "\n",
    "print(type(y_valid_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_new = pd.concat([X_train_new, y_train_new.reset_index(drop=True)], axis=1)\n",
    "tfidf_train_features_df_new.columns = tfidf_train_features_df.columns\n",
    "\n",
    "tfidf_test_features_df_new = pd.concat([X_test_new, y_test_new.reset_index(drop=True)], axis=1)\n",
    "tfidf_test_features_df_new.columns = tfidf_test_features_df.columns\n",
    "\n",
    "tfidf_valid_features_df_new = pd.concat([X_valid_new, y_valid_new.reset_index(drop=True)], axis=1)\n",
    "tfidf_valid_features_df_new.columns = tfidf_valid_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_new = tfidf_train_features_df\n",
    "tfidf_test_features_df_new = tfidf_test_features_df\n",
    "tfidf_valid_features_df_new = tfidf_valid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaafpat</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaox</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacavir</th>\n",
       "      <th>abated</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zing</th>\n",
       "      <th>zocor</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zolpidem</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zostavax</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zucker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14049 rows Ã— 10646 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaa  aaafpat  aand  aaox   ab  abacavir  abated  abbey  abby  abc  ...  \\\n",
       "0     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "2     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "3     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "4     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "...   ...      ...   ...   ...  ...       ...     ...    ...   ...  ...  ...   \n",
       "1258  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1259  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1260  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1261  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1262  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "\n",
       "      zing  zocor  zoloft  zolpidem  zoo  zoom  zostavax  zoster  zucker  \\\n",
       "0      0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1      0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "2      0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "3      0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "4      0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "...    ...    ...     ...       ...  ...   ...       ...     ...     ...   \n",
       "1258   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1259   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1260   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1261   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "1262   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0     0.0   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "1258      2  \n",
       "1259      2  \n",
       "1260      2  \n",
       "1261      2  \n",
       "1262      2  \n",
       "\n",
       "[14049 rows x 10646 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings = {0.0 : 0, 1.0 : 1, 2.0 : 2}\n",
    "\n",
    "tfidf_train_features_df_new.label = [mappings[item] for item in tfidf_train_features_df_new.label]\n",
    "tfidf_test_features_df_new.label = [mappings[item] for item in tfidf_test_features_df_new.label]\n",
    "tfidf_valid_features_df_new.label = [mappings[item] for item in tfidf_valid_features_df_new.label]\n",
    "\n",
    "# tfidf_train_features_df_new.to_csv(\"Data/SMOTE_train.csv\")\n",
    "# tfidf_test_features_df_new.to_csv(\"Data/SMOTE_test.csv\")\n",
    "# tfidf_valid_features_df_new.to_csv(\"Data/SMOTE_val.csv\")\n",
    "\n",
    "tfidf_features_master = pd.concat([tfidf_train_features_df_new,tfidf_test_features_df_new,tfidf_valid_features_df_new])\n",
    "\n",
    "tfidf_features_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaafpat</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaox</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacavir</th>\n",
       "      <th>abated</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zietmen</th>\n",
       "      <th>zing</th>\n",
       "      <th>zocor</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zolpidem</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zostavax</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zucker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263 rows Ã— 10645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaa  aaafpat  aand  aaox   ab  abacavir  abated  abbey  abby  abc  ...  \\\n",
       "0     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "2     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "3     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "4     0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "...   ...      ...   ...   ...  ...       ...     ...    ...   ...  ...  ...   \n",
       "1258  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1259  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1260  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1261  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "1262  0.0      0.0   0.0   0.0  0.0       0.0     0.0    0.0   0.0  0.0  ...   \n",
       "\n",
       "      zietmen  zing  zocor  zoloft  zolpidem  zoo  zoom  zostavax  zoster  \\\n",
       "0         0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "1         0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "2         0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "3         0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "4         0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "...       ...   ...    ...     ...       ...  ...   ...       ...     ...   \n",
       "1258      0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "1259      0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "1260      0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "1261      0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "1262      0.0   0.0    0.0     0.0       0.0  0.0   0.0       0.0     0.0   \n",
       "\n",
       "      zucker  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "1258     0.0  \n",
       "1259     0.0  \n",
       "1260     0.0  \n",
       "1261     0.0  \n",
       "1262     0.0  \n",
       "\n",
       "[1263 rows x 10645 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_features_df_new.drop(columns = 'label')\n",
    "tfidf_test_features_df_new.drop(columns = 'label')\n",
    "tfidf_valid_features_df_new.drop(columns = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "R6NodJua7hCH"
   },
   "outputs": [],
   "source": [
    "def filter_features_by_cor(df):\n",
    "    m = len(df.columns)\n",
    "    output = df.iloc[:,m-1] \n",
    "    output_list = output.tolist()\n",
    "    corrcoef_array = []\n",
    "\n",
    "    for i in range(0,m-2):\n",
    "        input_list = df.iloc[:,i].tolist()\n",
    "        cols = [input_list, output_list]\n",
    "        corrcoef = abs(np.corrcoef(cols)) \n",
    "        #print(type(corrcoef))\n",
    "        corrcoef_array = np.append(corrcoef_array,corrcoef[0,1])\n",
    "\n",
    "    feature_names = list(df)\n",
    "    feature_names = feature_names[0:m-2]\n",
    "    output_df = pd.DataFrame(feature_names, columns=['Features'])\n",
    "    output_df['CorrCoef'] = corrcoef_array\n",
    "    output_df = output_df.sort_values('CorrCoef')\n",
    "    output_df = output_df.reset_index()\n",
    "    output_df = output_df.drop(columns = \"index\")\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Dz-msJeB7pxz",
    "outputId": "cca008b4-d250-4696-e247-7fee23febeb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>CorrCoef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10643</th>\n",
       "      <td>intact</td>\n",
       "      <td>0.630383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10642</th>\n",
       "      <td>oriented</td>\n",
       "      <td>0.510477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>orientationsensorium</td>\n",
       "      <td>0.402591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10640</th>\n",
       "      <td>reports</td>\n",
       "      <td>0.382848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10639</th>\n",
       "      <td>immediate</td>\n",
       "      <td>0.371857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wassociated</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>artment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>heaviness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ventolin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ribs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10644 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Features  CorrCoef\n",
       "10643                intact  0.630383\n",
       "10642              oriented  0.510477\n",
       "10641  orientationsensorium  0.402591\n",
       "10640               reports  0.382848\n",
       "10639             immediate  0.371857\n",
       "...                     ...       ...\n",
       "15              wassociated  0.000000\n",
       "16                  artment  0.000000\n",
       "17                heaviness  0.000000\n",
       "18                 ventolin  0.000000\n",
       "0                      ribs  0.000000\n",
       "\n",
       "[10644 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_output_df = filter_features_by_cor(tfidf_train_features_df_new)\n",
    "tfidf_output_df = tfidf_output_df.sort_values(by=['CorrCoef'],ascending = False)\n",
    "#tfidf_output_df.to_csv('Data/tfidf_vector_feature_corr.csv')\n",
    "\n",
    "tfidf_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "ucYzO4Db76AW"
   },
   "outputs": [],
   "source": [
    "# Setting Correlation threshold\n",
    "top_tfidf_features_df = tfidf_output_df#[:500]\n",
    "filtered_tfidf_train = tfidf_train_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "filtered_tfidf_test = tfidf_test_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "filtered_tfidf_valid   = tfidf_valid_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "#filtered_tfidf_tanish = tfidf_tanish.filter(items = top_tfidf_features_df['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tfidf_train.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_train_no_smote.csv\", index = False)\n",
    "filtered_tfidf_test.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_test_no_smote.csv\", index = False)\n",
    "filtered_tfidf_valid.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_validation_no_smote.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_train_no_smote.csv\", index = False)\n",
    "y_test.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_test_no_smote.csv\", index = False)\n",
    "y_valid.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_validation_no_smote.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0.1\n",
    "\n",
    "svm = SVC(probability = True, C=c, kernel='linear',  random_state=42)\n",
    "svm.fit(filtered_tfidf_train, y_train_new)\n",
    "\n",
    "print(\"C:\", c, \"\\n\")\n",
    "\n",
    "scores = cross_val_score(svm, tfidf_features_master, tfidf_features_master[\"label\"], cv=5)\n",
    "\n",
    "y_prob_svm = svm.predict_proba(filtered_tfidf_valid)\n",
    "labels_svm = svm.predict(filtered_tfidf_valid)\n",
    "\n",
    "target_names = ['NO', 'NTR', 'YES']\n",
    "results_svm = classification_report(y_valid_new, labels_svm, target_names=target_names, output_dict=True)\n",
    "results_svm = pd.DataFrame(results_svm).transpose()\n",
    "\n",
    "#     results_svm_tanish = classification_report(real_tanish_labels, tanish_svm_labels, target_names=target_names, output_dict=True)\n",
    "#     results_svm_tanish = pd.DataFrame(results_svm_tanish).transpose()\n",
    "\n",
    "print(results_svm)\n",
    "print(\"CV Scores: \", scores)\n",
    "print(\"AUC: \", roc_auc_score(y_valid_new, y_prob_svm, multi_class = 'ovr'))\n",
    "print(\"ACC: \", accuracy_score(y_valid_new, labels_svm))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_valid_new, labels_svm))\n",
    "print(\"---------------------------------------------------------------------\\n\")\n",
    "    \n",
    "# y_prob_svm_tanish = svm.predict_proba(filtered_tfidf_tanish)\n",
    "# tanish_svm_labels = svm.predict(filtered_tfidf_tanish)\n",
    "# real_tanish_labels = tanish_df[\"label\"].to_numpy()\n",
    "\n",
    "# print(\"\\nTanish Data: \\n\")\n",
    "# print(results_svm_tanish)\n",
    "# print(\"AUC: \", roc_auc_score(real_tanish_labels, y_prob_svm_tanish, multi_class='ovr'))\n",
    "# print(\"\\nConfusion Matrix: \\n\", confusion_matrix(real_tanish_labels, tanish_svm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVIWEYdl7-cc",
    "outputId": "2a01bb31-d681-4678-bfb7-ea3589ebac75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, random_state=42)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0.1\n",
    "labels_lgr_tfidf = np.array([])\n",
    "\n",
    "lr = LogisticRegression(C = c, random_state = 42)\n",
    "lr.fit(filtered_tfidf_train, y_train_new)\n",
    "\n",
    "#scores = cross_val_score(lr, tfidf_features_master, tfidf_features_master[\"label\"], cv=5)\n",
    "#print(\"CV:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588281868566905"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(filtered_tfidf_valid, y_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\n",
    "test = tfidf_vectorizer.fit_transform(np.array([\" ------- School of Drama. His daugher is a pediatric nurse practitioner at Longwood Pediatrics. EtOH: None x 14 years. Occupation: Now retired-but looking to go back to work in human resources. Exercise: Plans to restart Family History Alcoholism Relative: Father Relative: Paternal Half-Sister Alzheimer's Disease Relative: Mother [Negative] Colon Cancer [Negative] Coronary Artery Disease Diabetes Mell ------- rinary: Testes normal and penis normal. Right testis shows no mass. Left testis shows no mass. Circumcised. Musculoskeletal: Normal range of motion. He exhibits no edema or tenderness. Lymphadenopathy: He has no cervical adenopathy. Neurological: He is alert and oriented to person, place, and time. No cranial nerve deficit. Skin: Skin is warm and dry. No rash noted. No erythema. Psychia\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x73 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 73 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 73 features per sample; expecting 10644",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-b6f996497a6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(\"C:\", c, \"\\n: \", )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlabels_lgr_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     y_prob_lgr_tanish = lr.predict_proba(filtered_tfidf_tanish)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[0;32m    289\u001b[0m                              % (X.shape[1], n_features))\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 73 features per sample; expecting 10644"
     ]
    }
   ],
   "source": [
    "y_prob_lr = lr.predict_proba(filtered_tfidf_valid)\n",
    "#print(\"C:\", c, \"\\n: \", )\n",
    "\n",
    "labels_lgr_tfidf = lr.predict(test)\n",
    "\n",
    "#     y_prob_lgr_tanish = lr.predict_proba(filtered_tfidf_tanish)\n",
    "\n",
    "#     tanish_lr_labels = lr.predict(filtered_tfidf_tanish)\n",
    "#     real_tanish_labels = tanish_df[\"label\"].to_numpy()\n",
    "\n",
    "#     results_lr_tanish = classification_report(real_tanish_labels, tanish_lr_labels, target_names=target_names, output_dict=True)\n",
    "#     results_lr_tanish = pd.DataFrame(results_lr_tanish).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:  0.1 \n",
      "               precision    recall  f1-score      support\n",
      "NO             0.954436  0.945368  0.949881   421.000000\n",
      "NTR            0.903297  0.976247  0.938356   421.000000\n",
      "YES            0.953964  0.885986  0.918719   421.000000\n",
      "accuracy       0.935867  0.935867  0.935867     0.935867\n",
      "macro avg      0.937232  0.935867  0.935652  1263.000000\n",
      "weighted avg   0.937232  0.935867  0.935652  1263.000000\n",
      "AUC:  0.9903492984128954\n",
      "ACC:  0.9358669833729216\n",
      "\n",
      "Confusion Matrix: \n",
      " [[398  15   8]\n",
      " [  0 411  10]\n",
      " [ 19  29 373]]\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['NO', 'NTR', 'YES']\n",
    "\n",
    "results_lgr = classification_report(y_valid_new, labels_lgr_tfidf, target_names=target_names, output_dict=True)\n",
    "results_lgr = pd.DataFrame(results_lgr).transpose()\n",
    "\n",
    "print(\"\\nC: \", c, \"\\n\", results_lgr)\n",
    "print(\"AUC: \", roc_auc_score(y_valid_new, y_prob_lr, multi_class='ovr'))\n",
    "print(\"ACC: \", accuracy_score(y_valid_new, labels_lgr_tfidf))\n",
    "#print(\"MCC: \", matthews_corrcoef(y_valid, y_prob_lr))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_valid_new, labels_lgr_tfidf))\n",
    "#     print(\"\\nTanish Data: \\n\", )\n",
    "#     print(results_lr_tanish)\n",
    "#     print(\"AUC: \", roc_auc_score(real_tanish_labels, y_prob_lgr_tanish, multi_class='ovr'))\n",
    "#     print(\"\\nConfusion Matrix: \\n\", confusion_matrix(real_tanish_labels, tanish_lr_labels))\n",
    "print(\"---------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_forest = RandomForestClassifier()\n",
    "clf_random_forest.fit(filtered_tfidf_train, y_train_new)\n",
    "\n",
    "y_prob_rf = clf_random_forest.predict_proba(filtered_tfidf_valid)\n",
    "\n",
    "labels_rf_tfidf = clf_random_forest.predict(filtered_tfidf_valid)\n",
    "\n",
    "results_rf = classification_report(y_valid_new, labels_rf_tfidf, target_names=target_names, output_dict=True)\n",
    "results_rf = pd.DataFrame(results_rf).transpose()\n",
    "\n",
    "print(results_rf)\n",
    "\n",
    "scores = cross_val_score(clf_random_forest, tfidf_features_master, tfidf_features_master[\"label\"], cv=5)\n",
    "print(\"CV:\", scores)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(y_valid_new, y_prob_rf, multi_class='ovr'))\n",
    "print(\"ACC: \", accuracy_score(y_valid_new, labels_rf_tfidf))\n",
    "#print(\"MCC: \", matthews_corrcoef(y_valid, y_prob_lr))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_valid_new, labels_rf_tfidf))\n",
    "\n",
    "# print(\"\\nTanish Data: \\n\")\n",
    "\n",
    "# y_prob_rf_tanish = clf_random_forest.predict_proba(filtered_tfidf_tanish)\n",
    "\n",
    "# tanish_rf_labels = clf_random_forest.predict(filtered_tfidf_tanish)\n",
    "# real_tanish_labels = tanish_df[\"label\"].to_numpy()\n",
    "\n",
    "# results_rf_tanish = classification_report(real_tanish_labels, tanish_rf_labels, target_names=target_names, output_dict=True)\n",
    "# results_rf_tanish = pd.DataFrame(results_rf_tanish).transpose()\n",
    "# print(results_rf_tanish)\n",
    "\n",
    "# print(\"AUC: \", roc_auc_score(real_tanish_labels, y_prob_rf_tanish, multi_class='ovr'))\n",
    "# print()\n",
    "# #print(\"MCC: \", matthews_corrcoef(y_valid, y_prob_lr))\n",
    "# print(\"\\nConfusion Matrix: \\n\", confusion_matrix(real_tanish_labels, tanish_rf_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression, SVM with TFIDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
