{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 844 ms\n"
     ]
    }
   ],
   "source": [
    "# Data Science\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandasgui import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from datetime import date\n",
    "import warnings\n",
    "current_date = date.today()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "%load_ext autotime\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 3500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-2.9.1-py3-none-any.whl (302 kB)\n",
      "time: 14 sRequirement already satisfied: tqdm in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (4.50.2)\n",
      "\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (20.4)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.6.5-py2.py3-none-any.whl (164 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (1.3.20)\n",
      "Requirement already satisfied: numpy in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (1.19.2)\n",
      "Requirement already satisfied: scipy!=1.4.0 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from optuna) (1.5.2)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from cliff->optuna) (2.4.7)\n",
      "Collecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
      "Requirement already satisfied: six in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from packaging>=20.0->optuna) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from alembic->optuna) (2.8.1)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.4)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
      "Collecting pyreadline3; sys_platform == \"win32\" and python_version >= \"3.8\"\n",
      "  Downloading pyreadline3-3.3-py3-none-any.whl (95 kB)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\mind_ds\\anaconda3\\lib\\site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11112 sha256=d82cd24c5aeb4f49d6cac93410b2bebe83e37bc30bfc78a9ce3b84321bc9f15e\n",
      "  Stored in directory: c:\\users\\mind_ds\\appdata\\local\\pip\\cache\\wheels\\7f\\1a\\65\\84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: pyreadline3, pyperclip, cmd2, pbr, stevedore, PrettyTable, cliff, python-editor, Mako, alembic, colorlog, cmaes, optuna\n",
      "Successfully installed Mako-1.1.4 PrettyTable-2.1.0 alembic-1.6.5 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 colorlog-5.0.1 optuna-2.9.1 pbr-5.6.0 pyperclip-1.8.2 pyreadline3-3.3 python-editor-1.0.4 stevedore-3.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?  No\n",
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# set seeds to make computations deterministic\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? \", \"Yes\" if cuda_available else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# configure logging options\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "always_patterns = pd.read_csv(\"input_optimized.csv\") \n",
    "manual_review = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\test_and_validation.csv\")\n",
    "manual_review = manual_review[['patient_id', 'sequence', 'annotator_label']]\n",
    "always_patterns = always_patterns[['patient_id', 'sequence', 'annotator_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>annotator_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z15564314</td>\n",
       "      <td>s other free text-see phs viewer social histor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z10171706</td>\n",
       "      <td>------- fusion: no sleep disturbance: no socia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z8935348</td>\n",
       "      <td>ain spasm). 30 tablet 0 unknown (outside pharm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z12212893</td>\n",
       "      <td>------- on 112mcg dose. maria will check dose ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z9598376</td>\n",
       "      <td>------- (vibramycin) 100 mg capsule take 1 cap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>Z6595984</td>\n",
       "      <td>------- gnitive concerns. interim history pati...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>Z10123442</td>\n",
       "      <td>------- rm and well perfused, no evidence of e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>Z6694483</td>\n",
       "      <td>: 77.7 kg (171 lb 4.8 oz) spo2: 98% 99% 98% cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>Z9195765</td>\n",
       "      <td>mr. donovan lives with his wife with whom he h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>Z9687259</td>\n",
       "      <td>------- ssified elsewhere past surgical histor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id                                           sequence  \\\n",
       "0     Z15564314  s other free text-see phs viewer social histor...   \n",
       "1     Z10171706  ------- fusion: no sleep disturbance: no socia...   \n",
       "2      Z8935348  ain spasm). 30 tablet 0 unknown (outside pharm...   \n",
       "3     Z12212893  ------- on 112mcg dose. maria will check dose ...   \n",
       "4      Z9598376  ------- (vibramycin) 100 mg capsule take 1 cap...   \n",
       "...         ...                                                ...   \n",
       "8045   Z6595984  ------- gnitive concerns. interim history pati...   \n",
       "8046  Z10123442  ------- rm and well perfused, no evidence of e...   \n",
       "8047   Z6694483  : 77.7 kg (171 lb 4.8 oz) spo2: 98% 99% 98% cu...   \n",
       "8048   Z9195765  mr. donovan lives with his wife with whom he h...   \n",
       "8049   Z9687259  ------- ssified elsewhere past surgical histor...   \n",
       "\n",
       "      annotator_label  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   2  \n",
       "4                   1  \n",
       "...               ...  \n",
       "8045                2  \n",
       "8046                0  \n",
       "8047                0  \n",
       "8048                1  \n",
       "8049                1  \n",
       "\n",
       "[8050 rows x 3 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "always_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", \n",
    "                                                      num_labels = 3, \n",
    "                                                      output_attentions = False, \n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def split_data(trial):\n",
    "    # stratify across sequences with and without always pattern matches and class_label (Y, N, NTR)\n",
    "    \n",
    "    # stratifying across sequences with always pattern\n",
    "    X_train, X_other = train_test_split(always_patterns, random_state = 0,test_size = 0.1, stratify = always_patterns[\"annotator_label\"].to_numpy())\n",
    "\n",
    "    X_valid, X_test = train_test_split(X_other, random_state = 0, test_size = 0.25, stratify = X_other[\"annotator_label\"].to_numpy())\n",
    "    \n",
    "    # stratifying across sequences without always pattern\n",
    "    X_train_2, X_other_2 = train_test_split(manual_review, random_state = 0,test_size = 0.6, stratify = manual_review[\"annotator_label\"].to_numpy())\n",
    "\n",
    "    X_valid_2, X_test_2 = train_test_split(X_other_2, random_state = 0, test_size = (0.25/0.6), stratify = X_test_2[\"annotator_label\"].to_numpy())\n",
    "    \n",
    "    # combining to get final train, test, validation splits\n",
    "    X_train = X_train.append(X_train_2)\n",
    "    X_valid = X_valid.append(X_valid_2)\n",
    "    X_test = X_test.append(X_test_2)\n",
    "\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "def define_model(trial, trial_dir):\n",
    "    # set learning rate\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    # define model name\n",
    "    model_type = \"bert\"\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    max_seq_length = 512\n",
    "\n",
    "    model_args = ClassificationArgs (\n",
    "        ## NLP ARGUMENTS\n",
    "        sliding_window = False,\n",
    "        learning_rate = learning_rate, # default 4e-5\n",
    "        adam_epsilon = 1e-8, # default 1e-8\n",
    "        train_batch_size = 8, # default 8\n",
    "        eval_batch_size = 4, # default 8\n",
    "        num_train_epochs = 3,  # default 1 (number of epochs model will be trained for)\n",
    "        do_lower_case = False, # default False\n",
    "        max_seq_length = max_seq_length, # default 128 (maximum sequence length the model will support)\n",
    "\n",
    "        ## TRAINING LOOP\n",
    "        logging_steps = 50, # default 50\n",
    "        manual_seed = 1234, # default None (necessary for reproducible results)\n",
    "        n_gpu = 0, # default 1 (number of GPUs to use)\n",
    "        save_steps = 2000, # default 2000 (save a model checkpoint at every specified number of steps)\n",
    "        output_dir = trial_dir, \n",
    "        overwrite_output_dir = True, # default False (if True, then the trained model will be saved to the ouput_dir and will overwrite existing saved models in the same directory)\n",
    "\n",
    "        ## EVALUATE DURING TRAINING\n",
    "        evaluate_during_training = True, # default False\n",
    "        evaluate_during_training_steps = 2000, # default  2000  \n",
    "        evaluate_during_training_verbose = True, # default False\n",
    "\n",
    "        ## EARLY STOPPING\n",
    "        use_early_stopping = True, # default False\n",
    "        early_stopping_delta = 0, # default 0 (improvement over best_eval_loss necessary to count as a better checkpoint)\n",
    "        early_stopping_metric = \"auc\", # default eval_loss \n",
    "        early_stopping_metric_minimize = True, # default True\n",
    "        early_stopping_patience = 2, # default value 3 (terminate training after these many epochs if there is no improvement in early_stopping_metric then early_stopping_delta)\n",
    "    )\n",
    "\n",
    "    # create the classification model\n",
    "    model = ClassificationModel (\n",
    "        model_type, model_name,\n",
    "        args = model_args,\n",
    "        use_cuda = cuda_available\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "X = always_patterns[\"sequence\"]\n",
    "y = always_patterns[\"annotator_label\"]\n",
    "\n",
    "y_label = y.to_numpy()\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X,y,random_state=0,test_size=0.1, stratify=y_label)\n",
    "\n",
    "y_test_valid_label = y_test_valid.to_numpy()\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test_valid, y_test_valid, random_state=0, test_size=0.25, stratify=y_test_valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # log time\n",
    "    start_time = time.localtime()\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n-------- TRIAL #{} --------\".format(trial.number))\n",
    "\n",
    "    # create output directory\n",
    "    trial_dir = \"../../BigDataSets/Results/Model/trial_{}\".format(trial.number)\n",
    "    if os.path.isdir(trial_dir):\n",
    "        shutil.rmtree(trial_dir)\n",
    "        print(\"\\n>>> {}: Removing Directory {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial_dir))\n",
    "    os.mkdir(trial_dir)\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Preparing Data\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    train_data, val_data, test_data = prepare_data(trial)\n",
    "    \n",
    "    # save test dataset to file\n",
    "    f = open(Path(trial_dir, \"data_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([train_data, val_data, test_data], f)\n",
    "    f.close()\n",
    "    \n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Defining Model\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    model = define_model(trial, trial_dir)\n",
    "    \n",
    "        # log message\n",
    "    print(\"\\n>>> {}: Started Training\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    # train model\n",
    "    model.train_model(\n",
    "        train_data,\n",
    "        eval_df = val_data,\n",
    "        auc = sk.roc_auc_score,\n",
    "        acc = sk.accuracy_score\n",
    "    )\n",
    "    \n",
    "    results, model_outputs, wrong_predictions = model.eval_model(\n",
    "        val_data,\n",
    "        auc = sk.roc_auc_score,\n",
    "        acc = sk.accuracy_score\n",
    "    )\n",
    "    \n",
    "    # save to file\n",
    "    f = open(Path(trial_dir, \"training_results_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([model, results, model_outputs, wrong_predictions], f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "X_2 = manual_review[\"sequence\"]\n",
    "y_2 = manual_review[\"annotator_label\"]\n",
    "\n",
    "y_label_2 = y_2.to_numpy()\n",
    "X_train_2, X_test_valid_2, y_train_2, y_test_valid_2 = train_test_split(X_2,y_2,random_state=0,test_size=0.6, stratify=y_label_2)\n",
    "\n",
    "y_test_valid_label_2 = y_test_valid_2.to_numpy()\n",
    "X_valid_2, X_test_2, y_valid_2, y_test_2 = train_test_split(X_test_valid_2, y_test_valid_2, random_state=0, test_size=(0.25/0.6), stratify=y_test_valid_label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.append(X_train_2)\n",
    "y_train = y_train.append(y_train_2)\n",
    "\n",
    "X_test = X_test.append(X_test_2)\n",
    "y_test = y_test.append(y_test_2)\n",
    "\n",
    "X_valid = X_valid.append(X_valid_2)\n",
    "y_valid = y_valid.append(y_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.append(X_valid)\n",
    "y_train = y_train.append(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "manual_review = manual_review[['Unnamed: 0', 'patient_id', 'sequence','original', 'annotator_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_df, manual_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "df['split'] = ['not_set'] * df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train      8302\n",
       "not_set     354\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8302/8302 [00:15<00:00, 534.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(X_train))):\n",
    "    df.loc[df[\"sequence\"] == X_train[i], [\"split\"]] = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 354/354 [00:00<00:00, 556.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 641 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(X_test))):\n",
    "    df.loc[df[\"sequence\"] == X_test[i], [\"split\"]] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical BERT Tokenizer and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', do_lower_case=True)\n",
    "\n",
    "# encode data\n",
    "seq_len = 512\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus (\n",
    "    df[df.split == 'train'].sequence.values, \n",
    "    add_special_tokens = True, \n",
    "    return_attention_mask = True, \n",
    "    pad_to_max_length = True, \n",
    "    max_length = seq_len, \n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus (\n",
    "    df[df.split == 'test'].sequence.values, \n",
    "    add_special_tokens = True, \n",
    "    return_attention_mask = True, \n",
    "    pad_to_max_length = True, \n",
    "    max_length = seq_len, \n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.split == 'train'].annotator_label.values)\n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(df[df.split == 'test'].annotator_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>original</th>\n",
       "      <th>annotator_label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Z15564314</td>\n",
       "      <td>s other free text-see phs viewer social histor...</td>\n",
       "      <td>s other free text-see phs viewer Social Histor...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Z10171706</td>\n",
       "      <td>------- fusion: no sleep disturbance: no socia...</td>\n",
       "      <td>------- fusion: No Sleep disturbance: No Socia...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Z8935348</td>\n",
       "      <td>ain spasm). 30 tablet 0 unknown (outside pharm...</td>\n",
       "      <td>ain/spasm). 30 tablet 0 Unknown (outside pharm...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Z12212893</td>\n",
       "      <td>------- on 112mcg dose. maria will check dose ...</td>\n",
       "      <td>-------  on 112mcg dose. Maria will check dos...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Z9598376</td>\n",
       "      <td>------- (vibramycin) 100 mg capsule take 1 cap...</td>\n",
       "      <td>------- (VIBRAMYCIN) 100 MG capsule Take 1 ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>8594</td>\n",
       "      <td>Z9536296</td>\n",
       "      <td>ecreased contraversive pushing. ot: pt tolerat...</td>\n",
       "      <td>ecreased contraversive pushing. OT: Pt tolerat...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8652</th>\n",
       "      <td>8605</td>\n",
       "      <td>Z9857663</td>\n",
       "      <td>------- not tolerate cpap following with sleep...</td>\n",
       "      <td>------- not tolerate CPAP/ Following with Sle...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>8606</td>\n",
       "      <td>Z12110231</td>\n",
       "      <td>---------------------------------------follow-...</td>\n",
       "      <td>---------------------------------------Follow-...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>8609</td>\n",
       "      <td>Z8822410</td>\n",
       "      <td>le. 2. no acute fracture or dislocation of the...</td>\n",
       "      <td>le. 2. No acute fracture or dislocation of the...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>8643</td>\n",
       "      <td>Z8347238</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8656 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 patient_id  \\\n",
       "0              0  Z15564314   \n",
       "1              1  Z10171706   \n",
       "2              2   Z8935348   \n",
       "3              3  Z12212893   \n",
       "4              4   Z9598376   \n",
       "...          ...        ...   \n",
       "8651        8594   Z9536296   \n",
       "8652        8605   Z9857663   \n",
       "8653        8606  Z12110231   \n",
       "8654        8609   Z8822410   \n",
       "8655        8643   Z8347238   \n",
       "\n",
       "                                               sequence  \\\n",
       "0     s other free text-see phs viewer social histor...   \n",
       "1     ------- fusion: no sleep disturbance: no socia...   \n",
       "2     ain spasm). 30 tablet 0 unknown (outside pharm...   \n",
       "3     ------- on 112mcg dose. maria will check dose ...   \n",
       "4     ------- (vibramycin) 100 mg capsule take 1 cap...   \n",
       "...                                                 ...   \n",
       "8651  ecreased contraversive pushing. ot: pt tolerat...   \n",
       "8652  ------- not tolerate cpap following with sleep...   \n",
       "8653  ---------------------------------------follow-...   \n",
       "8654  le. 2. no acute fracture or dislocation of the...   \n",
       "8655  ----------------------------------------------...   \n",
       "\n",
       "                                               original  annotator_label  \\\n",
       "0     s other free text-see phs viewer Social Histor...                1   \n",
       "1     ------- fusion: No Sleep disturbance: No Socia...                1   \n",
       "2     ain/spasm). 30 tablet 0 Unknown (outside pharm...                1   \n",
       "3      -------  on 112mcg dose. Maria will check dos...                2   \n",
       "4      ------- (VIBRAMYCIN) 100 MG capsule Take 1 ca...                1   \n",
       "...                                                 ...              ...   \n",
       "8651  ecreased contraversive pushing. OT: Pt tolerat...                0   \n",
       "8652   ------- not tolerate CPAP/ Following with Sle...                0   \n",
       "8653  ---------------------------------------Follow-...                0   \n",
       "8654  le. 2. No acute fracture or dislocation of the...                0   \n",
       "8655  ----------------------------------------------...                1   \n",
       "\n",
       "      split  \n",
       "0     train  \n",
       "1     train  \n",
       "2     train  \n",
       "3     train  \n",
       "4     train  \n",
       "...     ...  \n",
       "8651   test  \n",
       "8652   test  \n",
       "8653  train  \n",
       "8654  train  \n",
       "8655  train  \n",
       "\n",
       "[8656 rows x 6 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns = ['data_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Clinical BERT pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", \n",
    "                                                      num_labels = 3, \n",
    "                                                      output_attentions = False, \n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler = RandomSampler(dataset_train), \n",
    "                              batch_size = batch_size)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, \n",
    "                                   sampler = SequentialSampler(dataset_test), \n",
    "                                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 3\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = len(dataloader_train) * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals, probabilities, = [], [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "        probabilities.append(nnf.softmax(logits, dim = 1))\n",
    "    \n",
    "    loss_val_avg = loss_val_total / len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis = 0)\n",
    "    true_vals = np.concatenate(true_vals, axis = 0)\n",
    "    probabilities = np.concatenate(probabilities, axis = 0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                | 0/2768 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [3, 1024].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-1992dcd2cbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                  }       \n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1523\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [3, 1024].  Tensor sizes: [1, 512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    test_loss, predictions, true_vals, probabilities = evaluate(dataloader_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(true_vals, predictions)\n",
    "    auc = metrics.roc_auc_score(true_vals, probabilities, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    tqdm.write(f'Test loss: {test_loss}')\n",
    "    tqdm.write(f'acc: {acc}')\n",
    "    tqdm.write(f'auc: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-b2aadcd0b4ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'label_dict' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "label_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
