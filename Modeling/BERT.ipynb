{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 844 ms\n"
     ]
    }
   ],
   "source": [
    "# Data Science\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandasgui import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from datetime import date\n",
    "import warnings\n",
    "current_date = date.today()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "%load_ext autotime\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 3500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?  No\n",
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# set seeds to make computations deterministic\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? \", \"Yes\" if cuda_available else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# configure logging options\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "always_patterns = pd.read_csv(\"input_optimized.csv\") \n",
    "manual_review = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\test_and_validation.csv\")\n",
    "manual_review = manual_review[['patient_id', 'sequence', 'annotator_label']]\n",
    "always_patterns = always_patterns[['patient_id', 'sequence', 'annotator_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>annotator_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z15564314</td>\n",
       "      <td>s other free text-see phs viewer social histor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z10171706</td>\n",
       "      <td>------- fusion: no sleep disturbance: no socia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z8935348</td>\n",
       "      <td>ain spasm). 30 tablet 0 unknown (outside pharm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z12212893</td>\n",
       "      <td>------- on 112mcg dose. maria will check dose ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z9598376</td>\n",
       "      <td>------- (vibramycin) 100 mg capsule take 1 cap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>Z6595984</td>\n",
       "      <td>------- gnitive concerns. interim history pati...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>Z10123442</td>\n",
       "      <td>------- rm and well perfused, no evidence of e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>Z6694483</td>\n",
       "      <td>: 77.7 kg (171 lb 4.8 oz) spo2: 98% 99% 98% cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>Z9195765</td>\n",
       "      <td>mr. donovan lives with his wife with whom he h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>Z9687259</td>\n",
       "      <td>------- ssified elsewhere past surgical histor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8050 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id                                           sequence  \\\n",
       "0     Z15564314  s other free text-see phs viewer social histor...   \n",
       "1     Z10171706  ------- fusion: no sleep disturbance: no socia...   \n",
       "2      Z8935348  ain spasm). 30 tablet 0 unknown (outside pharm...   \n",
       "3     Z12212893  ------- on 112mcg dose. maria will check dose ...   \n",
       "4      Z9598376  ------- (vibramycin) 100 mg capsule take 1 cap...   \n",
       "...         ...                                                ...   \n",
       "8045   Z6595984  ------- gnitive concerns. interim history pati...   \n",
       "8046  Z10123442  ------- rm and well perfused, no evidence of e...   \n",
       "8047   Z6694483  : 77.7 kg (171 lb 4.8 oz) spo2: 98% 99% 98% cu...   \n",
       "8048   Z9195765  mr. donovan lives with his wife with whom he h...   \n",
       "8049   Z9687259  ------- ssified elsewhere past surgical histor...   \n",
       "\n",
       "      annotator_label  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   2  \n",
       "4                   1  \n",
       "...               ...  \n",
       "8045                2  \n",
       "8046                0  \n",
       "8047                0  \n",
       "8048                1  \n",
       "8049                1  \n",
       "\n",
       "[8050 rows x 3 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "always_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LongformerForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-f58036b49590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = LongformerForSequenceClassification.from_pretrained(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m'emilyalsentzer/Bio_ClinicalBERT'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# The number of output labels--3 for multi-label classification.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moutput_attentions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Whether the model returns attentions weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moutput_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Whether the model returns all hidden-states.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LongformerForSequenceClassification' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    num_labels = 3, # The number of output labels--3 for multi-label classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def split_data(trial):\n",
    "    # stratify across sequences with and without always pattern matches and class_label (Y, N, NTR)\n",
    "    \n",
    "    # stratifying across sequences with always pattern\n",
    "    X_train, X_other = train_test_split(always_patterns, random_state = 0,test_size = 0.1, stratify = always_patterns[\"annotator_label\"].to_numpy())\n",
    "\n",
    "    X_valid, X_test = train_test_split(X_other, random_state = 0, test_size = 0.25, stratify = X_other[\"annotator_label\"].to_numpy())\n",
    "    \n",
    "    # stratifying across sequences without always pattern\n",
    "    X_train_2, X_other_2 = train_test_split(manual_review, random_state = 0,test_size = 0.6, stratify = manual_review[\"annotator_label\"].to_numpy())\n",
    "\n",
    "    X_valid_2, X_test_2 = train_test_split(X_other_2, random_state = 0, test_size = (0.25/0.6), stratify = X_test_2[\"annotator_label\"].to_numpy())\n",
    "    \n",
    "    # combining to get final train, test, validation splits\n",
    "    X_train = X_train.append(X_train_2)\n",
    "    X_valid = X_valid.append(X_valid_2)\n",
    "    X_test = X_test.append(X_test_2)\n",
    "\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "def define_model(trial, trial_dir):\n",
    "    # set learning rate\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    # define model name\n",
    "    model_type = \"bert\"\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    max_seq_length = 512\n",
    "\n",
    "    model_args = ClassificationArgs (\n",
    "        ## NLP ARGUMENTS\n",
    "        sliding_window = False,\n",
    "        learning_rate = learning_rate, # default 4e-5\n",
    "        adam_epsilon = 1e-8, # default 1e-8\n",
    "        train_batch_size = 8, # default 8\n",
    "        eval_batch_size = 4, # default 8\n",
    "        num_train_epochs = 3,  # default 1 (number of epochs model will be trained for)\n",
    "        do_lower_case = False, # default False\n",
    "        max_seq_length = max_seq_length, # default 128 (maximum sequence length the model will support)\n",
    "\n",
    "        ## TRAINING LOOP\n",
    "        logging_steps = 50, # default 50\n",
    "        manual_seed = 1234, # default None (necessary for reproducible results)\n",
    "        n_gpu = 0, # default 1 (number of GPUs to use)\n",
    "        save_steps = 2000, # default 2000 (save a model checkpoint at every specified number of steps)\n",
    "        output_dir = trial_dir, \n",
    "        overwrite_output_dir = True, # default False (if True, then the trained model will be saved to the ouput_dir and will overwrite existing saved models in the same directory)\n",
    "\n",
    "        ## EVALUATE DURING TRAINING\n",
    "        evaluate_during_training = True, # default False\n",
    "        evaluate_during_training_steps = 2000, # default  2000  \n",
    "        evaluate_during_training_verbose = True, # default False\n",
    "\n",
    "        ## EARLY STOPPING\n",
    "        use_early_stopping = True, # default False\n",
    "        early_stopping_delta = 0, # default 0 (improvement over best_eval_loss necessary to count as a better checkpoint)\n",
    "        early_stopping_metric = \"auc\", # default eval_loss \n",
    "        early_stopping_metric_minimize = True, # default True\n",
    "        early_stopping_patience = 2, # default value 3 (terminate training after these many epochs if there is no improvement in early_stopping_metric then early_stopping_delta)\n",
    "    )\n",
    "\n",
    "    # create the classification model\n",
    "    model = ClassificationModel (\n",
    "        model_type, model_name,\n",
    "        args = model_args,\n",
    "        use_cuda = cuda_available\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # log time\n",
    "    start_time = time.localtime()\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n-------- TRIAL #{} --------\".format(trial.number))\n",
    "\n",
    "    # create output directory\n",
    "    trial_dir = \"../../BigDataSets/Results/Model/trial_{}\".format(trial.number)\n",
    "    if os.path.isdir(trial_dir):\n",
    "        shutil.rmtree(trial_dir)\n",
    "        print(\"\\n>>> {}: Removing Directory {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial_dir))\n",
    "    os.mkdir(trial_dir)\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Preparing Data\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    train_data, val_data, test_data = prepare_data(trial)\n",
    "    \n",
    "    # save test dataset to file\n",
    "    f = open(Path(trial_dir, \"data_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([train_data, val_data, test_data], f)\n",
    "    f.close()\n",
    "    \n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Defining Model\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    model = define_model(trial, trial_dir)\n",
    "    \n",
    "        # log message\n",
    "    print(\"\\n>>> {}: Started Training\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    # train model\n",
    "    model.train_model(\n",
    "        train_data,\n",
    "        eval_df = val_data,\n",
    "        auc = sk.roc_auc_score,\n",
    "        acc = sk.accuracy_score\n",
    "    )\n",
    "    \n",
    "    results, model_outputs, wrong_predictions = model.eval_model(\n",
    "        val_data,\n",
    "        auc = sk.roc_auc_score,\n",
    "        acc = sk.accuracy_score\n",
    "    )\n",
    "    \n",
    "    # save to file\n",
    "    f = open(Path(trial_dir, \"training_results_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([model, results, model_outputs, wrong_predictions], f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals, probabilities, = [], [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "        probabilities.append(nnf.softmax(logits, dim = 1))\n",
    "    \n",
    "    loss_val_avg = loss_val_total / len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis = 0)\n",
    "    true_vals = np.concatenate(true_vals, axis = 0)\n",
    "    probabilities = np.concatenate(probabilities, axis = 0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                | 0/2768 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [3, 1024].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-1992dcd2cbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                  }       \n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1523\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [3, 1024].  Tensor sizes: [1, 512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    test_loss, predictions, true_vals, probabilities = evaluate(dataloader_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(true_vals, predictions)\n",
    "    auc = metrics.roc_auc_score(true_vals, probabilities, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    tqdm.write(f'Test loss: {test_loss}')\n",
    "    tqdm.write(f'acc: {acc}')\n",
    "    tqdm.write(f'auc: {auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
