{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee3773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78621a1",
   "metadata": {},
   "source": [
    "# Spacy Best Practices\n",
    "1. If need to output strings, convert to strings as late as possible to main complex relationships with hashs, etc.\n",
    "2. Use token attributes whenever possible. E.g. token.pos_, token.text, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac700eb",
   "metadata": {},
   "source": [
    "# Training Best Practices\n",
    "\n",
    "1. Model can forget certain things it already learned - for example, if it learned to identify people and now you are teaching it how to identify a website, it might forget how to identify a person. A solution to this is to constantly mix in examples of what it previously learned so that it can be reinforced in the model's \"brain\"\n",
    "2. Label scheme needs to be consistent and not overly specific --> It can be difficult for a model to learn to identify adult clothing or children's clothing. Having the model identify just clothing would be better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f52637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "## BAD CODE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc] #BAD\n",
    "pos_tags = [token.pos_ for token in doc] #BAD\n",
    "\n",
    "#BAD - does not use token attributes\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8d35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "## GOOD CODE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc] #GOOD - uses token attributes\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03842c03",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dc0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "World\n",
      "chunk Hello World\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp(\"Hello World\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "word = doc[1]\n",
    "print(word)\n",
    "\n",
    "chunk = doc[0:]\n",
    "print(\"chunk\", chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a82c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índex:  [0, 1, 2, 3, 4]\n",
      "Text:  ['it', 'costs', '$', '5', '.']\n",
      "Is_alpha [True, True, False, False, False]\n",
      "is_punct [False, False, False, False, True]\n",
      "like_num [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# breaking down a sentence\n",
    "doc = nlp(\"it costs $5.\")\n",
    "print(\"Índex: \", [token.i for token in doc])\n",
    "print(\"Text: \", [token.text for token in doc])\n",
    "print(\"Is_alpha\", [token.is_alpha for token in doc]) #is_alpha means does characters at certain index consist of members of alphabet\n",
    "print(\"is_punct\", [token.is_punct for token in doc])\n",
    "print(\"like_num\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e695e08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. \" \"Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d967c5",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "### Allow for text to be interpreted in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88f792c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nen_core_web_sm\\nStatistical model allow you to generalize based on a set of training examples. \\nOnce they’re trained, they use binary weights to make predictions. \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") #small language model that allows for English lanuage to be understood\n",
    "\n",
    "\"\"\"\n",
    "en_core_web_sm\n",
    "Statistical model allow you to generalize based on a set of training examples. \n",
    "Once they’re trained, they use binary weights to make predictions. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f1264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple        apple      PROPN     NNP       nsubj      Xxxxx      1          0         \n",
      "is           be         VERB      VBZ       aux        xx         1          1         \n",
      "looking      look       VERB      VBG       ROOT       xxxx       1          0         \n",
      "at           at         ADP       IN        prep       xx         1          1         \n",
      "buying       buy        VERB      VBG       pcomp      xxxx       1          0         \n",
      "U.K.         u.k.       PROPN     NNP       compound   X.X.       0          0         \n",
      "startup      startup    NOUN      NN        dobj       xxxx       1          0         \n",
      "for          for        ADP       IN        prep       xxx        1          1         \n",
      "$            $          SYM       $         quantmod   $          0          0         \n",
      "1            1          NUM       CD        compound   d          0          0         \n",
      "billion      billion    NUM       CD        pobj       xxxx       1          0         \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "#always add _ to get string representation\n",
    "\n",
    "# {___:<12} 12 character gap between each value, formatting\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} {token.lemma_:<10} {token.pos_:<10}{token.tag_:<10}{token.dep_:<10} {token.shape_:<10} {token.is_alpha :<10} {token.is_stop :<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3339e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# doc.ents will return all real world objects that are assigned a name\n",
    "#e.g. person, name, org, country\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_) #label_ says type of real world object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90fd235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f4f1e",
   "metadata": {},
   "source": [
    "# Rule-Based Matching\n",
    "Allows us to create patterns that will find matches inside the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e767c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e169e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc7fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"PATTERN\", None, pattern)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e881ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end] # start and end are character numbers that show where the match is\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5843cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns that require NLP\n",
    "pattern_2 = [\n",
    "    {\"LEMMA:\", \"love\", \"POS:\", \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa60f7d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2a78ec8d3f6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PATTERN_2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#matches would be loved dogs and love cats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher._convert_strings\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "matcher.add(\"PATTERN_2\", None, pattern_2)\n",
    "matches = matcher(doc) \n",
    "\n",
    "#matches would be loved dogs and love cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70c9d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a56c816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 4\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "\n",
    "# \"OP\" --> define how often a token pattern should be matched:\n",
    "# ! -> Negate the pattern, by requiring it to match exactly 0 times.\n",
    "# ? -> Make the pattern optional, by allowing it to match 0 or 1 times.\n",
    "# + -> Require the pattern to match 1 or more times.\n",
    "# * -> Allow the pattern to match 0 or more times.\n",
    "\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1636f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "doc2 = nlp(\"According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14.\")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"IS_ALPHA\": False} , {\"LOWER\": \"free\"} ,{\"POS\": \"NOUN\"}] # ad-free is stored as ad,-,free\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43835001",
   "metadata": {},
   "source": [
    "# Phrase-Based Matching\n",
    "\n",
    "Takes doc objects as patterns <br>\n",
    "More efficient than Rule-Based <br>\n",
    "Great for matching lists of large words <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ca9e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Span:  golden retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\") #CASE-SENSITIVE!!!\n",
    "pattern2 = nlp(\"golden retriever\")\n",
    "\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "matcher.add(\"DOG2\", None, pattern2)\n",
    "\n",
    "doc = nlp(\"I have a golden retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched Span: \", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cfb07",
   "metadata": {},
   "source": [
    "# Shared Vocabulary\n",
    "\n",
    "1. Add any new work to Spacy shared vocab (nlp.vocab.strings)\n",
    "2. Automatically generates a hash value for this word using a hash function, which can be reversed to get original string, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb54963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214040188179850886\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings[\"Alzeheimer's\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cce634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081433347173383943\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings[\"Alzheimer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cae1ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n",
      "string value:  coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value: \", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value: \", nlp.vocab.strings[nlp.vocab.strings[\"coffee\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7d54777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab[\"coffee\"] #lexeme holds all attributes about a certain word\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa610f",
   "metadata": {},
   "source": [
    "# Doc + Span Class\n",
    "\n",
    "Manually creating spaCy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b2eb185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "label = nlp.vocab.strings[\"PERSON\"]\n",
    "span = Span(doc, 2, 4, label=label)\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90384304",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "\n",
    "Need medium or large models that has word vectors\n",
    "\n",
    "Allow for comparing objects and predicting similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59c922f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955339749893996\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc1 = nlp(\"I'm hungry\")\n",
    "doc2 = nlp(\"I'm starving\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e84cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "billion\n",
      "0.9368200277285751\n"
     ]
    }
   ],
   "source": [
    "span = doc1[1:3]\n",
    "print(token)\n",
    "print(doc1.similarity(span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a53e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5519e-01 -4.5706e-01  1.5926e-01 -2.8942e-01 -1.1358e-01  4.0340e-01\n",
      "  2.2152e-01  2.1713e-01  3.2619e-01  2.0210e+00 -1.4097e-01 -5.6283e-02\n",
      " -9.5612e-02 -5.6380e-01 -1.8752e-01 -5.8578e-02 -8.6614e-02  3.3660e-01\n",
      "  3.3917e-02  3.4581e-01 -8.0374e-03 -6.2276e-01 -3.4314e-01 -5.4515e-01\n",
      " -2.2171e-01 -3.8986e-01  2.0809e-01  1.9913e-01  1.6447e-01 -5.2185e-01\n",
      " -6.8712e-01 -6.3531e-01  5.3983e-02 -4.4942e-01  5.0645e-01  1.2429e-01\n",
      " -1.0032e-01 -1.1886e-01 -3.4388e-01  5.7359e-01 -2.2550e-01 -3.3255e-01\n",
      " -1.9401e-02 -3.4872e-01 -2.8209e-01 -2.3237e-01 -5.5767e-02  7.1624e-01\n",
      " -1.8586e-01  3.2787e-02 -4.4107e-01  2.6368e-01 -4.5362e-01 -2.7263e-01\n",
      "  1.5438e-01  2.3338e-01 -3.1650e-01  4.6242e-02  2.3259e-01 -9.2897e-03\n",
      " -2.8470e-01 -3.2090e-01  2.5173e-01 -8.8286e-01 -2.1064e-01 -8.7721e-01\n",
      "  5.7446e-01 -6.1218e-04  1.2941e-01  1.3231e-01 -3.9421e-02  2.7136e-01\n",
      "  1.6217e-01 -6.0974e-01 -4.0965e-02  1.3208e-01 -4.9916e-02  2.8669e-01\n",
      "  7.3196e-02  1.9804e-01 -7.7658e-03  3.6383e-01  7.4314e-02 -4.3517e-01\n",
      "  1.4387e-01 -1.0670e-01  7.1708e-01 -1.2917e-01 -3.5407e-02  2.2339e-01\n",
      " -3.9854e-01  3.4313e-01 -2.5159e-01 -3.9586e-01 -1.6639e-01  1.4464e-01\n",
      " -6.3724e-01 -4.2459e-01  3.0790e-01  2.1951e-01 -3.3221e-01 -2.1089e-01\n",
      "  2.0884e-01 -6.8699e-02 -8.1740e-02 -8.3154e-01  4.3431e-01 -2.2389e-01\n",
      "  1.7966e-01  2.6305e-01  3.6834e-01 -7.5906e-01  2.4483e-01 -2.8489e-01\n",
      "  1.9208e-01 -5.4392e-01 -2.8183e-01 -5.2351e-01  7.1991e-02  3.5376e-01\n",
      " -1.1521e-01 -9.6449e-02  2.0730e-01 -4.2612e-01  1.6544e-01 -2.4499e-01\n",
      "  4.0388e-01 -3.3993e-01 -2.8394e-01  1.3118e-01  2.6050e-01 -3.9646e-01\n",
      "  4.7144e-01  1.5602e-01 -2.3843e-01 -6.7725e-01 -4.6216e-01 -2.2629e-01\n",
      " -2.3463e-01 -2.6253e-01 -2.5454e+00 -8.6065e-02  6.5126e-02 -2.7098e-01\n",
      "  2.7150e-03 -5.5723e-01  1.9789e-01  4.8608e-01 -2.2609e-01 -3.1703e-01\n",
      " -3.5955e-01  1.3818e-01  8.2454e-02  3.1130e-01 -1.1733e-01  4.2298e-01\n",
      "  2.1018e-02 -7.3500e-02  1.4168e-01 -3.7140e-01 -2.1611e-01  1.1345e-01\n",
      " -4.2475e-01  1.7289e-01  5.0331e-01 -1.1323e-01  6.6084e-01 -3.4028e-01\n",
      " -1.0582e-01 -1.9826e-02  8.6084e-02 -2.2615e-01 -1.5241e-02  9.2642e-02\n",
      "  8.8548e-02  3.8445e-01 -2.7218e-01 -2.5467e-01 -2.6920e-01  3.3831e-01\n",
      "  3.3273e-01 -3.3480e-01 -1.3713e-01  6.1815e-02  1.7070e-01  4.6945e-02\n",
      " -1.7521e-01  6.6270e-02  3.8552e-01 -4.1403e-01  3.7334e-01  3.7956e-01\n",
      "  2.1860e-01  1.6566e-01 -1.7995e-01 -2.1862e-02  1.7377e-01  4.0323e-01\n",
      "  4.4253e-02 -7.1314e-01 -8.3347e-02  3.5012e-01 -2.7195e-01 -1.4363e-01\n",
      "  2.9013e-01  3.0414e-01 -5.2561e-02  3.1432e-01  3.3300e-01 -3.8583e-01\n",
      "  3.7583e-02 -8.6439e-02 -8.4244e-02  1.5418e-01  1.0505e-01 -2.2803e-01\n",
      " -1.3108e-01 -1.4381e-01  2.0465e-01 -2.3142e-01  7.7442e-03 -1.4665e-01\n",
      "  2.0149e-01  3.2766e-02  3.1841e-01  4.1682e-02 -3.0011e-01  2.0777e-01\n",
      " -1.1640e-01 -5.1054e-01 -1.9539e-01 -2.4979e-01  3.2322e-01 -3.0642e-01\n",
      "  6.5618e-01  1.6326e-01 -1.9047e-01 -7.5118e-01 -2.9291e-01  8.0872e-02\n",
      "  4.3263e-01 -5.0916e-02 -6.4850e-01  3.9048e-01  1.5451e-02  1.1623e-01\n",
      " -2.9597e-01  1.2133e-01 -2.2167e-02  9.2015e-01 -3.4111e-01  3.7131e-01\n",
      "  6.6315e-02  1.8838e-01  6.1757e-01  2.7099e-01 -2.4925e-01  1.5187e-01\n",
      " -2.7490e-01 -5.9662e-02 -3.9323e-01 -4.4454e-01 -1.7090e-01 -1.8081e-01\n",
      "  4.7611e-01  2.6044e-01 -2.7093e-01 -3.1856e-01 -1.3703e-01  1.9566e-01\n",
      " -8.0594e-02  2.1146e-01 -5.7289e-01 -2.6462e-01  4.8540e-02 -2.6464e-01\n",
      "  2.4356e-02  6.5016e-01 -9.5147e-02  5.4642e-01 -5.0394e-02  1.6098e-01\n",
      " -2.2720e-02 -2.1624e-01  2.5053e-01 -7.1103e-02  4.9248e-01 -9.4807e-02\n",
      " -3.4010e-01  6.0065e-01 -6.6423e-02 -2.2662e-01 -1.5354e-01 -2.0170e-01\n",
      " -9.2347e-02  4.3536e-01  1.4351e-01 -3.3573e-01 -2.4316e-01 -5.7779e-01]\n"
     ]
    }
   ],
   "source": [
    "print(doc1[2].vector) #getting word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf5d37",
   "metadata": {},
   "source": [
    "# Processing Pipelines\n",
    "\n",
    "Functions applied to doc to add pos tags, entities, etc.\n",
    "\n",
    "![title](Images/parts_of_processing_pipeline.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fde070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.Tagger object at 0x0C5D4FD0>), ('parser', <spacy.pipeline.DependencyParser object at 0x0A8C32A0>), ('ner', <spacy.pipeline.EntityRecognizer object at 0x0C5F3420>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d359b53",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "Spacy supports base set of piplines (parser, tagger, entity recognizer), and also has ways to add your own "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0cd70",
   "metadata": {},
   "source": [
    "### Options on where to add component\n",
    "1. last = True, adds component last\n",
    "2. first = True, adds component first\n",
    "3. before = \"ner\", adds component before ner component\n",
    "4. after = \"ner\", adds component after ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dc22810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_component(doc):\n",
    "    print(\"Length: \", len(doc))\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, first = True)\n",
    "\n",
    "doc = nlp(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d33d5261",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f8e66ba8aa2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0manimals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Golden Retriever\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"turtle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Rattus norvegicus\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manimal_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manimals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"animal_patterns:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manimal_patterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ANIMAL\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0manimal_patterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\myprograms\\anaconda332\\envs\\nlp_sentence\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\myprograms\\anaconda332\\envs\\nlp_sentence\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_pipe\u001b[1;34m(func, docs)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-f3a8c8438cea>\u001b[0m in \u001b[0;36manimal_component\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Create a Span for each match and assign the label \"ANIMAL\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mspans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ANIMAL\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Overwrite the doc.ents with the matched spans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-f3a8c8438cea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Create a Span for each match and assign the label \"ANIMAL\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mspans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ANIMAL\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Overwrite the doc.ents with the matched spans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=nlp.vocab.strings(\"ANIMAL\")) for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3fca1",
   "metadata": {},
   "source": [
    "# Optimizing Spacy Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35145a99",
   "metadata": {},
   "source": [
    "## Processing Large Volumes of Text\n",
    "\n",
    "1. Use nlp.pipe method\n",
    "\n",
    "```py\n",
    "#BAD\n",
    "\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "#GOOD\n",
    "\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "```\n",
    "\n",
    "2. Instead of running whole pipeline every time (which can be time-consuming), you can use methods that will do only what you need instead of the whole thing.\n",
    "\n",
    "```py\n",
    "doc = nlp(\"Hello World\") #whole thing\n",
    "doc = nlp.make_doc(\"Hello World\") #makes only tokenized doc object, no ner or any other components of the pipeline\n",
    "\n",
    "# you can also disable certain components:\n",
    "\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "    \n",
    "#disabled components are restored after with block is run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "587cdc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text 15\n",
      "Add another text 16\n"
     ]
    }
   ],
   "source": [
    "# nlp.pipe also allows you to pass in data through tuple format (text, context) tuples, yieldss (doc, context tuples)\n",
    "data = [\n",
    "    (\"This is some text\", {\"id\": 1, \"page_num\": 15}),\n",
    "    (\"Add another text\", {\"id\": 2, \"page_num\": 16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples = True):\n",
    "    print(doc.text, context[\"page_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abb7c4",
   "metadata": {},
   "source": [
    "# Custom Extension Attributes\n",
    "\n",
    "Allows you to add custom metadata to documents, tokens, and spans\n",
    "\n",
    "1. Attribute Extensions - set default val that can be overriden\n",
    "2. Property Extensions - def get/set methods, getter function is called wehn you retrieve the attribute value,, span extensions should mostly always use a a getter\n",
    "3. Method extensions make the attribute a callable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6428130",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "## attribute extensions\n",
    "\n",
    "Doc.set_extension(\"title\", default = None, force = True)\n",
    "Token.set_extension(\"is_color\", force = True, getter = get_is_color) #cant do default and getter within the same extension\n",
    "Span.set_extension(\"has_color\", default = False, force = True)\n",
    "\n",
    "doc._.title = \"My text\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "\n",
    "doc[3]._.is_color = True #overriding default val of false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "866549c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True -> blue\n"
     ]
    }
   ],
   "source": [
    "#getters\n",
    "\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "print(doc[3]._.is_color, \"->\", doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a3345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using getters with a span\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", force = True, getter = get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31436cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True  -> blue\n",
      "False  -> cloud\n"
     ]
    }
   ],
   "source": [
    "#method extensions\n",
    "def has_token(doc, token_text):\n",
    "    return token_text in[token.text for token in doc]\n",
    "\n",
    "Doc.set_extension(\"has_token\", force = True, method = has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue\")\n",
    "print(doc._.has_token(\"blue\"), \" -> blue\")\n",
    "print(doc._.has_token(\"cloud\"), \" -> cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3c1af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "#method extensions\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", force = True, method = to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea70af",
   "metadata": {},
   "source": [
    "# Creating Training Data for Model to Learn Generalizations Specific to Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9f56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "TEXTS = [\n",
    "  \"How to preorder the iPhone X\",\n",
    "  \"iPhone X is coming\",\n",
    "  \"Should I pay $1,000 for the iPhone X?\",\n",
    "  \"The iPhone 8 reviews are here\",\n",
    "  \"iPhone 11 vs iPhone 8: What's the difference?\",\n",
    "  \"I need a new phone! Any tips?\"\n",
    "]\n",
    "\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and check the result\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "475c7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "(\"iPhone 11 vs iPhone 8: What's the difference?\", {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "## Generating Training Data\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c5ad5",
   "metadata": {},
   "source": [
    "# Creating New Training Pipeline and Running Model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2adca4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.506747016371719}\n",
      "{'ner': 0.9478891206886438}\n",
      "{'ner': 0.0015377429287959771}\n",
      "{'ner': 8.57005368438091e-07}\n",
      "{'ner': 1.1351266841395656e-10}\n",
      "{'ner': 2.6114565732663255e-11}\n",
      "{'ner': 7.87826427000158e-12}\n",
      "{'ner': 4.708774008385329e-12}\n",
      "{'ner': 3.4910373842099664e-12}\n",
      "{'ner': 2.5390248219380898e-12}\n"
     ]
    }
   ],
   "source": [
    "## Creating a new training pipeline for Entity Recoginizing\n",
    "\n",
    "import random\n",
    "\n",
    "data = [\n",
    "    ['How to preorder the iPhone X', {'entities': [[20, 28, 'GADGET']]}], \n",
    "    ['iPhone X is coming', {'entities': [[0, 8, 'GADGET']]}], \n",
    "    ['Should I pay $1,000 for the iPhone X?', {'entities': [[28, 36, 'GADGET']]}], \n",
    "    ['The iPhone 8 reviews are here', {'entities': [[4, 12, 'GADGET']]}], \n",
    "    ['Your iPhone goes up to 11 today', {'entities': [[5, 11, 'GADGET']]}], \n",
    "    ['I need a new phone! Any tips?', {'entities': []}]\n",
    "]\n",
    "\n",
    "# Create a blank \"en\" model\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label \"GADGET\" to the entity recognizer\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168c54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
