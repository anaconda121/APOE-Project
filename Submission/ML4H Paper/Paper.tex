 % use the "wcp" class option for workshop and conference
 % proceedings
 %\documentclass[gray]{jmlr} % test grayscale version
 %\documentclass[tablecaption=bottom]{jmlr}% journal article
 \documentclass[pmlr,twocolumn,10pt]{jmlr} % W&CP article

% \usepackage{geometry}
% \geometry{margins=0.1in,textwidth=7in}

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
 %\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{tabto}    
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{url}

\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version 
%\usepackage{siunitx}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

% The following is to recognise equal contribution for authorship
\newcommand{\equal}[1]{{\hypersetup{linkcolor=black}\thanks{#1}}}

% Customized Tabs
\newcommand\mytab{\tab \hspace{-5cm}}

 % Define an unnumbered theorem just for this sample document for
 % illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{LEAVE UNSET}
\jmlryear{2021}
\jmlrsubmitted{LEAVE UNSET}
\jmlrpublished{LEAVE UNSET}
\jmlrworkshop{Machine Learning for Health (ML4H) 2021} % W&CP title

 % The optional argument of \title is used in the header
\title[Short Title]{Full Title of Article\titlebreak This Title Has
A Line Break}

 % Anything in the title that should appear in the main title but 
 % not in the article's header or the volume's table of
 % contents should be placed inside \titletag{}

 %\title{Title of the Article\titletag{\thanks{Some footnote}}}

\author{Tanish Tyagi \nametag{\thanks{Authors contributed equally}\SUP{1}},
Colin G. Magdamo\nametag{\footnotemark[1]\SUP{1}}, 
Ayush Noori \SUP{1},
Mayuresh Deodhar \SUP{1},
Zhuoqiao Hong \SUP{1},
Dimitry Propenko \SUP{1},
Rudy E. Tanzi \SUP{1},
Deborah Blacker \SUP{1},
Bradley T. Hyman \SUP{1},
Shibani S. Mukerji \SUP{1},
M. Brandon Westover \SUP{1},
Sudeshna Das\SUP{1},
\centering \Email{
\\[\bigskipamount] 
\SUP{1}\{ttanish, 
cmagdamo,
anoori1,
...
sdas5\}
@mgh.harvard.edu}
% 
\begin{center}\addr Massachusetts General Hospital, Boston, MA\end{center}
}

\begin{document}

\maketitle

\begin{abstract}
\tab Dementia is a neurodegenerative disorder that causes cognitive decline and affects more than 50 million people worldwide. Dementia is under-diagnosed by healthcare professionals—only one in four people who suffer from dementia are diagnosed. Even when a diagnosis is made, it may not be entered as a structured diagnosis code in a patient’s charts.  Information relevant to cognitive impairment is often found within electronic health records but manual review of clinician notes by experts is both time consuming and often prone to errors. Automated evaluation of these notes presents an opportunity to label patients with cognitive impairment (CI) in real-world data. In order to identify patients with cognitive impairment, we applied natural language processing (NLP) techniques. Model performance was compared to a baseline model that utilized structured features such as medication and ICD counts. % Add which model had highest performance %  
\end{abstract}

\begin{keywords}
EHR, NLP, Dementia
\end{keywords}

\section{Introduction}
\label{sec:intro} The possibility of early diagnosis of dementia is important for improving clinical outcomes and patient management of dementia. Often, diagnosis is given once patient has reached moderate dementia, but irreversible damage has already been done to the brain. 
% https://www.dementiacarecentral.com/aboutdementia/facts/stages/ %
Yet, dementia is not formally diagnosed or coded in claims data for over 50\% of older adults living with probable dementia. Tools that can efficiently and effectively analyze medical records for warning signs of dementia and recommend patients for care can be critical to obtaining early diagnosis for dementia. In this project, we aim to use NLP to detect signs of cognitive impairment from unstructured clinician notes in electronic health records (EHR) by using machine and deep learning techniques. 

\section{Related Works}
\label{sec:RelatedWorks}

\section{Dataset, Preprocessing, and Annotations}

\paragraph{Dataset}
\label{sec:Dataset} Our gold-standard dataset consisted of a cohort (N = 16,428) of patients from the Mass General Brigham (MGB) HealthCare (formerly Partner's Healthcare, comprising two major academic hospitals, community hospitals, and community health centers in the Boston area) system who were older than 60 years (as of July 13, 2021) and had APOE genotype (biggest genetic risk factor for dementia) data available from the BioBank. Each patients' EHR record was then annotated by neurologists using a web-based annotation tool (UI Interface in Appendix A) as 1) Yes, i.e., patient has CI; 2) No i.e., Patient does not have CI; and 3) Neither i.e., sequence has no information on patient’s cognition.

\paragraph{Preprocessing}
\label{sec:Preprocessing} For each patient in our gold-standard dataset, we extracted unstructured clinician notes, identified matches to dementia-related keywords (Appendix B), and constructed 800 character sequences from the note text around each of these matches. Our cohort of 16,428 patients had 279,224 sequences in total. 

\paragraph{Annotations}
\label{sec:Annotations} We initially assigned 5,000 sequences diversified by keyword matches from 5,000 unique patients for neurologist labeling. As the manual annotation of 279,000+ sequences in not feasible, we devised a scheme to expedite annotations known as "always patterns". An always pattern is defined as a phrase or regex expression that in any context indicates the phrase will be labeled with a particular class (i.e. yes, no, or neither). If an always pattern is inputted, all other sequences have language that matches with the phrase will be automatically labeled accordingly. % {List any other sequence assignment schemes} %
Currently, 8,656 sequences have been annotated, 8,050 through always patterns and 606 manually. % {Will change} %

The final dataset was split between train (95\%) and holdout test (5\%) sets, stratified across label and proportion of sequences annotated manually and through always patterns. % {Will change} % 
Validation datasets were split from the train set using techniques described in the Methodology section. Table 2 shows demographics of the cohort of patients.  

\section{Methodology}
\label{sec:Methodology}  

We built 4 models and compared performance to the baseline model. It is important to note that these models were trained on binary labels, despite annotators labeling sequences with 3 classes. This was done because our overall task was to classify if a patient had CI, not whether a sequence indicates CI. This means that a label of neither is not relevant, as a patient can only be classified as having or not having CI. In order to convert this problem to binary labels, we aggregated no and neither labels to class 0 and yes labels to class 1. 

\label{sec:Baseline}  
\paragraph{(1) Baseline Model} We performed L1 regularized logistic regression where the cognitive concern label was regressed on the counts of medications, keyword counts, and ICD codes relevant to CI. The lambda value was selected through 10-fold Cross Validation (CV) to maximize the average area under the Receiver Operator Characteristic (ROC). % {To be implemented} %

\label{sec:TFIDF}  
\paragraph{(2) Logistic Regression with TF-IDF Vectors} We performed TF-IDF (term frequency-inverse document frequency) vectorization on the annotated sequences and selected features based on a term's Pearson correlation coefficient with the outcome. L1 Regularized logistic regression was applied with the annotated cognitive impairment labels. We used different correlation coefficients as thresholds to select features and iterated over different lambda values to determine the optimal lambda value and correlation coefficient threshold. 

\label{sec:Baseline+TFIDF}  
\paragraph{(3) Model 1 + Model 2} % {TBD} % 

\label{sec:Transformer}  
\paragraph{(4) Transformer Based Sequence Classification Language Model} % {To be implemented} %

% binary classification model stats - probability threshold of 0.89
\begin{table*}[hbtp]
%   {\caption{TF-IDF Vector Features Correlation}}
  {\begin{tabular}{lccccccc}
    \toprule 
    
    \bfseries Model & \bfseries AUCROC & \bfseries ACC & \bfseries Sensitivity & \bfseries Specificity & \bfseries FP & \bfseries FN & \bfseries Prediction Threshold \\ 
    Model 2 & 0.94 & 0.90 & 0.73 & 0.94 & __ & __ & 0.89 \\
    
    \bottomrule
  \end{tabular}}
 
\end{table*}

\section{Results}
\label{sec:Results}  

% Talk about Stats
% TF-IDF model struggles with understanding context of words around keyword match
% Future plans include reducing false positives by gathering more annotated sequences and using deep learning natural language. Currently, many of our false positives are from the model identifying the presence of a keyword but struggling to understand the context around the keyword match; many of these contexts negate the presence of CI for the patient in question. Deep Learning can leverage contextual information, making it promising for this task.

We evaluated each model using a prediction threshold to maximize the accuracy and calculated performance metrics based on that threshold. Model performance for each model in shown in Table 2. 

\section{Conclusion and Future Work} 
\label{sec:ConclusionFutureWork} We applied innovative Machine Learning and Deep Learning NLP techniques to identify patients with CI in EHR. Our work can help combat the issues of underdiagnosis and misdiagnosis for dementia. % {More coming soon } %


\clearpage
\appendix

% Appendix A
\section{UI of Annotation Tool}
Placeholder

% Appendix B
\section{Keywords} % wi
\begin{table}[hbtp] 
\floatconts  
{tab:operatornames}
    {\caption{Keywords indicative of Cognitive Impairment}} \\
    {
        \begin{tabular}{lccc}
        \toprule
        \bfseries Keyword & \bfseries Match Count\\
        \midrule
        
        \textbf{Memory} & \fseries 109218 \\ 
        \textbf{Cognition}  & \fseries 87655 \\ 
        \textbf{Dementia} & \fseries 51034 \\ 
        \textbf{Cerebral} & \fseries 45886 \\ 
        \textbf{Cerebrovascular} & \fseries 36370 \\ 
        \textbf{Cerebellar} & \fseries 26863 \\
        \textbf{Cognitive Impairment} & \fseries 20267 \\ 
        \textbf{Alzheimer} & \fseries 20581 \\ 
        \textbf{MOCA} & \fseries 9767 \\ 
        \textbf{Neurocognitive} & \fseries 7711 \\ 
        \textbf{MCI} & \fseries 3889 \\ 
        \textbf{Amnesia} & \fseries 3695 \\ 
        \textbf{AD} & \fseries 2673 \\ 
        \textbf{Lewy} & \fseries 2561 \\ 
        \textbf{MMSE} & \fseries 2134 \\ 
        \textbf{LBD} & \fseries 224 \\ 
        \textbf{Corticobasal} & \fseries 147 \\ 
        \textbf{Pick's} & \fseries 41 \\ 
        
        \bottomrule
        \end{tabular}
    }
\end{table}

\end{document}
