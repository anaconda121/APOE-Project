{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "#  Author: Tanish Tyagi\n",
    "#  \n",
    "\n",
    "# base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import itertools\n",
    "import sklearn.metrics as sk\n",
    "from functools import reduce\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "# file system manipulation\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import time# base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import itertools\n",
    "import sklearn.metrics as sk\n",
    "from functools import reduce\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "# file system manipulation\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "data_dir = Path(\"Storage/Bert/\")\n",
    "results_dir = Path(\"Storage/Bert/Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to make computations deterministic\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? \", \"Yes\" if cuda_available else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging options\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(trial):\n",
    "    train_data = pd.read_csv(r\"Storage/Bert/train_8_11.csv\")\n",
    "    val_data = pd.read_csv(r\"Storage/Bert/valid_8_11.csv\")\n",
    "    test_data = pd.read_csv(r\"Storage/Bert/test_8_11.csv\")\n",
    "        \n",
    "    train_data.columns = val_data.columns = test_data.columns = [\"PatientID\", \"text\", \"labels\"]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining ClinicalBERT Model with Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial, trial_dir):\n",
    "    # hyperparameter tuning\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-4, log = True)\n",
    "    adam_epsilon = trial.suggest_float(\"adam_epilson\", 1e-8, 1e-4, log = True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 3)\n",
    "    early_stopping_patience = trial.suggest_int(\"early_stopping_patience\", 1, 3)\n",
    "\n",
    "    print(\"- Learning Rate: {}\".format(learning_rate))\n",
    "    print(\"- Adam Epsilon: {}\".format(adam_epsilon)) \n",
    "    print(\"- Training Epochs: {}\".format(num_train_epochs))\n",
    "    print(\"- Early Stopping Patience: {}\".format(early_stopping_patience))\n",
    "\n",
    "    # define model name\n",
    "    model_type = \"bert\"\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    max_seq_length = 512 \n",
    "\n",
    "    model_args = ClassificationArgs(\n",
    "\n",
    "    ## NLP ARGUMENTS\n",
    "    sliding_window = False,\n",
    "    learning_rate = learning_rate, # default 4e-5\n",
    "    adam_epsilon = adam_epsilon, # default 1e-8\n",
    "    train_batch_size = 8, # default 8\n",
    "    eval_batch_size = 4, # default 8\n",
    "    num_train_epochs = num_train_epochs,  # default 1 (number of epochs model will be trained for)\n",
    "    do_lower_case = False, # default False\n",
    "    max_seq_length = max_seq_length, # default 128 (maximum sequence length the model will support)\n",
    "    \n",
    "    ## TRAINING LOOP\n",
    "    logging_steps = 50, # default 50\n",
    "    manual_seed = 1234, # default None (necessary for reproducible results)\n",
    "    n_gpu = 2, # default 1 (number of GPUs to use)\n",
    "    save_steps = 2000, # default 2000 (save a model checkpoint at every specified number of steps)\n",
    "    output_dir = trial_dir, \n",
    "    overwrite_output_dir = True, # default False (if True, then the trained model will be saved to the ouput_dir and will overwrite existing saved models in the same directory)\n",
    "    \n",
    "    ## EVALUATE DURING TRAINING\n",
    "    evaluate_during_training = True, # default False\n",
    "    evaluate_during_training_steps = 2000, # default  2000  \n",
    "    evaluate_during_training_verbose = True, # default False\n",
    "    \n",
    "    ## EARLY STOPPING\n",
    "    use_early_stopping = True, # default False\n",
    "    early_stopping_delta = 0, # default 0 (improvement over best_eval_loss necessary to count as a better checkpoint)\n",
    "    early_stopping_metric = \"eval_loss\", # default eval_loss \n",
    "    early_stopping_metric_minimize = True, # default True\n",
    "    early_stopping_patience = early_stopping_patience, # default value 3 (terminate training after these many epochs if there is no improvement in early_stopping_metric then early_stopping_delta)\n",
    "    \n",
    "    )\n",
    "    \n",
    "    # create the classification model\n",
    "    model = ClassificationModel(\n",
    "        model_type, model_name,\n",
    "        num_labels = 3,\n",
    "        args = model_args,\n",
    "        use_cuda = cuda_available\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # log time\n",
    "    start_time = time.localtime()\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n-------- TRIAL #{} --------\".format(trial.number))\n",
    "\n",
    "    # create output directory\n",
    "    trial_dir = \"Storage/Bert/Results/trial_{}\".format(trial.number)\n",
    "    if os.path.isdir(trial_dir):\n",
    "        shutil.rmtree(trial_dir)\n",
    "        print(\"\\n>>> {}: Removing Directory {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial_dir))\n",
    "    os.mkdir(trial_dir)\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Preparing Data\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    train_data, val_data, test_data = prepare_data(trial)\n",
    "\n",
    "    assert len(train_data[\"labels\"].unique() == 3)\n",
    "    \n",
    "    # save test dataset to file\n",
    "    f = open(Path(trial_dir, \"data_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([train_data, val_data, test_data], f)\n",
    "    f.close()\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Defining Model\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    model = define_model(trial, trial_dir)\n",
    "\n",
    "    # log message\n",
    "    print(\"\\n>>> {}: Started Training\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    # train model\n",
    "    model.train_model(\n",
    "        train_data,\n",
    "        eval_df = val_data,\n",
    "        # auc = sk.roc_auc_score,\n",
    "        # acc = sk.accuracy_score\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> {}: Started Evaluation on Validation Set\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "    \n",
    "    results, model_outputs, wrong_predictions = model.eval_model(\n",
    "        val_data,\n",
    "        # auc = sk.roc_auc_score,\n",
    "        # acc = sk.accuracy_score\n",
    "    )\n",
    "\n",
    "    # save to file\n",
    "    f = open(Path(trial_dir, \"training_results_{}.pkl\".format(trial.number)), \"wb\")\n",
    "    pickle.dump([model, results, model_outputs, wrong_predictions], f)\n",
    "    f.close()\n",
    "\n",
    "    # output message, initialize empty list\n",
    "    print(\">>> {}: Get Sequence Probabilities\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "    df_list = []\n",
    "\n",
    "    # extract context window probabilities\n",
    "    max_prob_list = []\n",
    "    val_prob_list = []\n",
    "    val_pred_list = []\n",
    "    for i in range(len(val_data)):\n",
    "        # prob_list = list(torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)[:,1])\n",
    "        prob_list = torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)\n",
    "        #print(\"Prob List: \", prob_list, type(prob_list))\n",
    "\n",
    "        extracted_prob_list = []\n",
    "        for i in range(len(prob_list)):\n",
    "            extracted_prob_list.append(float(prob_list[i]))\n",
    "\n",
    "        #print(\"Extracted Prob List: \", extracted_prob_list)\n",
    "        # find max one in each submatrix of length 3\n",
    "        max_proba = max(extracted_prob_list)\n",
    "\n",
    "        # identify model prediction based on location of max_proba within extracted_prob_list\n",
    "        if (extracted_prob_list[0] == max_proba):\n",
    "            val_pred_list.append(0)\n",
    "        elif (extracted_prob_list[1] == max_proba):\n",
    "            val_pred_list.append(1)\n",
    "        else:\n",
    "            val_pred_list.append(2)\n",
    "\n",
    "        max_prob_list.append(max_proba)\n",
    "        val_prob_list.append(extracted_prob_list)\n",
    "    \n",
    "    cw_probs = pd.DataFrame(columns = [\"PatientID\", \"Prob\", \"Pred\"])\n",
    "    cw_probs[\"PatientID\"] = val_data[\"PatientID\"]\n",
    "    cw_probs[\"Prob\"] = max_prob_list\n",
    "    cw_probs[\"Pred\"] = val_pred_list\n",
    "    cw_probs.to_csv(trial_dir + \"/sequence_probabilities{}.csv\".format(trial.number))\n",
    "\n",
    "    # compute metrics\n",
    "    # print(\"Shapes of Y-True and Y-Pred\", val_data[\"labels\"].shape, cw_probs[\"Prob\"].shape) \n",
    "    best_auc = sk.roc_auc_score(val_data[\"labels\"].to_list(), val_prob_list, multi_class = \"ovr\", average = \"weighted\")\n",
    "    # best_auc = get_auc(val_prob_list, val_data[\"labels\"])\n",
    "    # best_acc, best_threshold = get_best_acc(cw_probs, val_data)\n",
    "    best_acc = sk.accuracy_score(val_data[\"labels\"].to_list(), cw_probs[\"Pred\"].to_list())\n",
    "    print(\">>> {}: Current AUC: {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), best_auc))\n",
    "    print(\">>> {}: Current ACC: {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), best_acc))\n",
    "    #print(\">>> {}: Threshold for Validation Accuracy: {}\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime()), best_threshold))\n",
    "    print(\">>> {}: Start Training Time\\n\".format(time.strftime(\"%H:%M:%S\", start_time)))\n",
    "    print(\">>> {}: Finish Training Time\\n\".format(time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Study Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# unique identifier of the study\n",
    "study_name = \"slat-study\" \n",
    "\n",
    "# create study database\n",
    "storage_name = \"sqlite:///{}.db\".format(\"Storage/Bert/Results/\" + study_name)\n",
    "study = optuna.create_study(direction = \"maximize\", sampler = TPESampler(seed = 1234, multivariate = True), study_name = study_name, storage = storage_name, load_if_exists = True)\n",
    "study.optimize(objective, n_trials = 20, gc_after_trial = True)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\nStudy Statistics:\")\n",
    "print(\"- Finished Trials: \", len(study.trials))\n",
    "print(\"- Pruned Trials: \", len(pruned_trials))\n",
    "print(\"- Complete Trials: \", len(complete_trials))\n",
    "\n",
    "print(\"\\nBest Trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"- Number: \", best_trial.number)\n",
    "print(\"- Value: \", best_trial.value)\n",
    "print(\"- Hyperparameters: \")\n",
    "\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"   - {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Held-Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv(r\"Storage/Bert/test_8_11.csv\")\n",
    "final_test.columns = [\"PatientID\", \"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"Storage/Bert/best_model/trial_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results, test_outputs, test_wrong = best_model.eval_model(\n",
    "    final_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Softmax to Convert Model output to Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob_list = []\n",
    "test_prob_list = []\n",
    "test_pred_list = []\n",
    "for i in range(len(final_test)):\n",
    "    # prob_list = list(torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)[:,1])\n",
    "    prob_list = torch.softmax(torch.from_numpy(test_outputs[i]), axis=0)\n",
    "    #print(\"Prob List: \", prob_list, type(prob_list))\n",
    "\n",
    "    extracted_prob_list = []\n",
    "    for i in range(len(prob_list)):\n",
    "        extracted_prob_list.append(float(prob_list[i]))\n",
    "\n",
    "    #print(\"Extracted Prob List: \", extracted_prob_list)\n",
    "    # find max one in each submatrix of length 3\n",
    "    max_proba = max(extracted_prob_list)\n",
    "\n",
    "    # identify model prediction based on location of max_proba within extracted_prob_list\n",
    "    if (extracted_prob_list[0] == max_proba):\n",
    "        test_pred_list.append(0)\n",
    "    elif (extracted_prob_list[1] == max_proba):\n",
    "        test_pred_list.append(1)\n",
    "    else:\n",
    "        test_pred_list.append(2)\n",
    "\n",
    "    max_prob_list.append(max_proba)\n",
    "    test_prob_list.append(extracted_prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test[\"pred\"] = test_pred_list\n",
    "final_test[\"proba\"] = max_prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.accuracy_score(test_pred_list, final_test[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.roc_auc_score(final_test[\"labels\"], test_prob_list, multi_class = \"ovr\", average = \"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"Storage/Bert/test_set_prob.pkl\", \"wb\")\n",
    "pickle.dump(str(test_prob_list), f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "target_names = ['Negative', 'Neither', 'Positive']\n",
    "results = metrics.classification_report(final_test[\"labels\"], final_test[\"pred\"], target_names = target_names, output_dict=True)\n",
    "results = pd.DataFrame(results).transpose()\n",
    "conf_mat = metrics.confusion_matrix(final_test[\"labels\"], final_test[\"pred\"])\n",
    "print(\"Micro F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"micro\"))\n",
    "print(\"Macro F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"macro\"))\n",
    "print(\"Weighted F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"weighted\"))\n",
    "print(\"Confusion Matrix: \\n\", conf_mat)\n",
    "print(\"Classification Report: \\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = conf_mat.sum(axis = 0) - np.diag(conf_mat) \n",
    "FN = conf_mat.sum(axis = 1) - np.diag(conf_mat)\n",
    "TP = np.diag(conf_mat)\n",
    "TN = conf_mat.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "print(\"Sensitivity: \", TPR)\n",
    "print(\"Specificity: \", TNR)\n",
    "print(\"NPV: \", NPV)\n",
    "print(\"PPV: \", PPV)\n",
    "print(\"FPR: \", FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 3\n",
    "y_test = final_test[\"labels\"]\n",
    "test_prob_list = np.array(test_prob_list)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = metrics.roc_curve(y_test, test_prob_list[:,i], pos_label = i)\n",
    "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "    \n",
    "lw = 2\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--', color='green', label='Positive Class: TF-IDF (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='red', label='Negative Class: TF-IDF (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='grey', label='Neither Class: TF-IDF (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "plt.savefig(\"bert_roc.svg\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0168968bb7ecfa78684047e3c95d55595c46869da584ba4da41f68e310286ae"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
