{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMfUEuGw7BvY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Train-Test-Val-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "610kf_Y97NHe",
    "outputId": "38d89897-db34-49f4-ef5f-c9bf41d6ad16"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Storage/Data/input_optimized.csv\") \n",
    "\n",
    "train_df = train_df[['Unnamed: 0', 'patient_id', 'sequence','original', 'annotator_label']]\n",
    "#train_df.columns = ['Unnamed: 0', 'patient_id', 'sequence','original', 'annotator_label']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[[\"sequence\"]]\n",
    "y = train_df[\"annotator_label\"]\n",
    "print(len(X))\n",
    "\n",
    "y_label = y.to_numpy()\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X,y,random_state=0,test_size=0.1, stratify=y_label)\n",
    "\n",
    "y_test_valid_label = y_test_valid.to_numpy()\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test_valid, y_test_valid, random_state=0, test_size=0.25, stratify=y_test_valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_review = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Storage\\Data\\test_and_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = manual_review[[\"sequence\"]]\n",
    "y_2 = manual_review[\"annotator_label\"]\n",
    "\n",
    "y_label_2 = y_2.to_numpy()\n",
    "X_train_2, X_test_valid_2, y_train_2, y_test_valid_2 = train_test_split(X_2,y_2,random_state=0,test_size=0.6, stratify=y_label_2)\n",
    "\n",
    "y_test_valid_label_2 = y_test_valid_2.to_numpy()\n",
    "X_valid_2, X_test_2, y_valid_2, y_test_2 = train_test_split(X_test_valid_2, y_test_valid_2, random_state=0, test_size=(0.25/0.6), stratify=y_test_valid_label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_train_2)\n",
    "y_train = y_train.append(y_train_2)\n",
    "\n",
    "X_test = X_test.append(X_test_2)\n",
    "y_test = y_test.append(y_test_2)\n",
    "\n",
    "X_valid = X_valid.append(X_valid_2)\n",
    "y_valid = y_valid.append(y_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"label\"] = y_train.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid[\"label\"] = y_valid.to_list()\n",
    "X_test[\"label\"] = y_test.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, X_valid], axis = 0)\n",
    "test = pd.concat([X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r\"Storage\\Data\\train_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r\"Storage\\Data\\test_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwfttvx-7RbZ"
   },
   "outputs": [],
   "source": [
    "X_train = train_df[\"sequence\"]\n",
    "y_train = train_df[\"annotator_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[train_df[\"always_pattern_match\"].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_df = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\test_and_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_val_df[\"sequence\"]\n",
    "y = test_val_df[\"annotator_label\"]\n",
    "y_label = y.to_numpy()\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X,y,random_state=0,test_size=0.4, stratify=y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sampling to Obtain Optimal Data Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating why TFIDF vectors are combining words together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'attentionconcentration', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate = []\n",
    "idxs = []\n",
    "counter = 0\n",
    "for seq in tqdm(train_df[\"sequence\"]):\n",
    "    m = pattern.search(seq)\n",
    "    if (m != None):\n",
    "        investigate.append(seq)\n",
    "        idxs.append(counter)\n",
    "        \n",
    "#         specials = '-\"/.~!' #etc\n",
    "#         trans = seq.translate(str.maketrans(specials, ' '*len(specials)))\n",
    "#         #print(trans)\n",
    "#         train_df.loc[i][\"sequence\"] = trans\n",
    "    counter += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing # of sequences that have always pattern matches to multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_patterns = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\EDA\\Data\\always_patterns_8_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_pattern_regex = always_patterns[\"Pattern\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(always_pattern_regex)):\n",
    "    always_pattern_regex[i] = re.compile(always_pattern_regex[i], re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\Diversity_Sampling\\20K_sample_8_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "c = []\n",
    "counter = 0\n",
    "\n",
    "for seq in tqdm(train_df[\"original\"]):\n",
    "    curr = []\n",
    "    classes = []\n",
    "    conversion = {2: \"Yes\", 1 : \"Neither\", 0: \"No\"}\n",
    "    \n",
    "    for p in (always_pattern_regex):\n",
    "        m = list(set(re.findall(p, seq)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        if (m != []):\n",
    "            curr.append(\"\".join(m))\n",
    "            label = int(train_df.loc[counter]['annotator_label'])\n",
    "            classes.append(conversion[label])\n",
    "            #classes = list(set(classes))\n",
    "            \n",
    "    a.append(curr)\n",
    "    c.append(classes)\n",
    "    counter += 1\n",
    "\n",
    "train_df[\"always_pattern_match\"] = a\n",
    "train_df[\"always_pattern_classes\"] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(r\"input_with_always_and_without_8_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_optimal = train_df.drop(no_matches.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_optimal[\"further_review\"] = np.where((len(train_df_optimal[\"always_pattern_classes\"]) > 1), 'N', 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "further_review = train_df_optimal[train_df_optimal[\"further_review\"] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "further_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_optimal.to_csv(r\"input_optimized.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_matches.to_csv(r\"test_and_validation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Frequency per Keyword in dataset and comparing to overall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\EDA\\Getting_Data\\keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = regex[\"REGEX\"].to_list()\n",
    "c = regex[\"CASE\"].to_list()\n",
    "p_list = []\n",
    "\n",
    "for i in range(len(k)):\n",
    "    if (c[i] == 0):\n",
    "        p_list.append(re.compile(k[i][5:], re.IGNORECASE))\n",
    "    elif (c[i] == 1):\n",
    "        p_list.append(re.compile(k[i]))\n",
    "print(p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for note in tqdm(train_df[\"original\"]):\n",
    "    curr = []\n",
    "    for p in (p_list):\n",
    "        m = list(set(re.findall(p, note)))\n",
    "        m = list(set(map(str.lower, m)))\n",
    "        if (m != []):\n",
    "            curr.append(\"\".join(m))\n",
    "    #print(curr)\n",
    "    #print(l)\n",
    "    l.append(str(curr))\n",
    "train_df[\"regex_match\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[i]['regex_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  ['dementia', 'cognition',\"\\'cognition\", 'memory', \"memory\\'\", 'cdr', 'mmse', 'moca', 'alzheimer', 'cognitive impairment','cognitiveimpairment', 'mci', 'cerebellar', 'neurocognitive', 'lewy', \"pick's\", 'cortical', 'corticobasal', 'cerebral', 'cerebrovascular', 'amnesia', 'ad', 'lbd']\n",
    "summary_stats  = pd.DataFrame(pd.np.empty((len(train_df), len(cols))) * pd.np.nan) \n",
    "summary_stats.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(train_df))):\n",
    "    keyword_list = list(eval(train_df.loc[i]['regex_match']))\n",
    "    #print(keyword_list, len(keyword_list))\n",
    "    match_tuples = []\n",
    "    for j in range(len(keyword_list)):\n",
    "        x = keyword_list[j]\n",
    "\n",
    "        match_tuples.append((x, 0))\n",
    "\n",
    "    #print(match_tuples)\n",
    "\n",
    "    freq = {\"dementia\":0\n",
    "        ,\"cognition\":0\n",
    "        ,\"\\'cognition\":0\n",
    "        ,\"memory\":0\n",
    "        ,\"memory\\'\":0\n",
    "        ,\"cdr\":0\n",
    "        ,\"mmse\":0\n",
    "        ,\"moca\":0\n",
    "        ,\"alzheimer\":0\n",
    "        ,'cognitive impairment':0\n",
    "        , \"cognitiveimpairment\":0\n",
    "        ,\"mci\":0\n",
    "        ,\"cerebellar\":0\n",
    "        ,\"neurocognitive\":0\n",
    "        ,\"lewy\":0\n",
    "        ,\"pick's\":0\n",
    "        ,\"cortical\":0\n",
    "        ,\"corticobasal\":0\n",
    "        ,\"cerebral\":0\n",
    "        ,\"cerebrovascular\":0\n",
    "        ,\"amnesia\":0\n",
    "        ,'ad':0\n",
    "        ,'lbd':0\n",
    "    }\n",
    "\n",
    "    for k, v in match_tuples:   \n",
    "        if (k != ''):\n",
    "            freq.update({str(k.lower()):int(freq[k.lower()] + 1)})\n",
    "\n",
    "    #print(freq, \"\\n\")\n",
    "\n",
    "    #summary_stats[\"encounters_with_keywords\"][i] = len(train_df[train_df[\"PatientID\"] == unique_ids[i]])\n",
    "\n",
    "    for k, v in freq.items():\n",
    "        summary_stats[str(k.lower())][i] = int(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Optimizing-data\\keywords_distribution_train_set_8_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "TyjWzdvA7Yt-",
    "outputId": "20828434-583a-498c-b555-fe3d38d58fc0"
   },
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", analyzer='word', token_pattern=r'\\b[A-Za-z0-9]+\\b')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "count_valid = count_vectorizer.transform(X_valid)\n",
    "count = count_vectorizer.transform(X)\n",
    "\n",
    "# Create the CountVectorizer DataFrame: count_train\n",
    "count_train = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "count_test = pd.DataFrame(count_test.A, columns=count_vectorizer.get_feature_names())\n",
    "count_valid = pd.DataFrame(count_valid.A, columns=count_vectorizer.get_feature_names())\n",
    "count = pd.DataFrame(count.A, columns=count_vectorizer.get_feature_names())\n",
    "count_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "7xtHzY0F7cBD",
    "outputId": "ceef9692-5ffe-43a5-ec81-72ed65705c0e"
   },
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",analyzer='word', token_pattern=r'\\b[A-Za-z0-9]+\\b')\n",
    "tfidf_train= tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_valid = tfidf_vectorizer.transform(X_valid)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "tfidf = tfidf_vectorizer.transform(X)\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_train = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_valid = pd.DataFrame(tfidf_valid.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_test = pd.DataFrame(tfidf_test.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf = pd.DataFrame(tfidf.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY run once\n",
    "tfidf_train_features_df = pd.concat([tfidf_train, y_train.reset_index(drop=True)], axis=1)\n",
    "tfidf_test_features_df = pd.concat([tfidf_test, y_test.reset_index(drop = True)], axis = 1)\n",
    "tfidf_valid_features_df = pd.concat([tfidf_valid, y_valid.reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_data = tfidf_train_features_df.values\n",
    "X_smote, y_smote = tfidf_train_features_df_data[:, :-1], tfidf_train_features_df_data[:, -1]\n",
    "\n",
    "tfidf_test_features_df_data = tfidf_test_features_df.values\n",
    "X_smote_test, y_smote_test = tfidf_test_features_df_data[:, :-1], tfidf_test_features_df_data[:, -1]\n",
    "\n",
    "tfidf_valid_features_df_data = tfidf_valid_features_df.values\n",
    "X_smote_valid, y_smote_valid = tfidf_valid_features_df_data[:, :-1], tfidf_valid_features_df_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pd.DataFrame(X_train)\n",
    "y_train_new = pd.Series(y_train)\n",
    "\n",
    "X_test_new = pd.DataFrame(X_test)\n",
    "y_test_new = pd.Series(y_test)\n",
    "\n",
    "X_valid_new = pd.DataFrame(X_valid)\n",
    "y_valid_new = pd.Series(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(tfidf_train_features_df.columns) \n",
    "x.append('annotator_label')\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_new = tfidf_train_features_df #pd.concat([tfidf_train_features_df, y_train_new.reset_index(drop = True)], axis = 1)#pd.concat([X_train_new, y_train_new.reset_index(drop=True)], axis=1)\n",
    "tmp = list(tfidf_train_features_df.columns) \n",
    "tfidf_train_features_df_new.columns = tmp\n",
    "\n",
    "tfidf_test_features_df_new = tfidf_test_features_df #pd.concat([tfidf_test_features_df, y_test_new.reset_index(drop = True)], axis = 1)#pd.concat([X_test_new, y_test_new.reset_index(drop=True)], axis=1)\n",
    "tmp2 = list(tfidf_test_features_df.columns) \n",
    "tfidf_test_features_df_new.columns = tmp2\n",
    "\n",
    "tfidf_valid_features_df_new = tfidf_valid_features_df #pd.concat([tfidf_valid_features_df, y_valid_new.reset_index(drop = True)], axis = 1)#pd.concat([X_valid_new, y_valid_new.reset_index(drop=True)], axis=1)\n",
    "tmp3 = list(tfidf_valid_features_df.columns) \n",
    "tfidf_valid_features_df_new.columns = tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {0.0 : 0, 1.0 : 1, 2.0 : 2}\n",
    "\n",
    "tfidf_train_features_df_new.annotator_label = [mappings[item] for item in tfidf_train_features_df_new.annotator_label]\n",
    "tfidf_test_features_df_new.annotator_label = [mappings[item] for item in tfidf_test_features_df_new.annotator_label]\n",
    "tfidf_valid_features_df_new.annotator_label = [mappings[item] for item in tfidf_valid_features_df_new.annotator_label]\n",
    "\n",
    "tfidf_features_master = pd.concat([tfidf_train_features_df_new,tfidf_test_features_df_new,tfidf_valid_features_df_new])\n",
    "\n",
    "tfidf_features_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = pd.concat([tfidf_train_features_df_new,tfidf_valid_features_df_new])\n",
    "len(cross_validation), cross_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_new.drop(columns = 'annotator_label')\n",
    "tfidf_test_features_df_new.drop(columns = 'annotator_label')\n",
    "tfidf_valid_features_df_new.drop(columns = 'annotator_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tfidf = pd.concat([tfidf_train_features_df_new, tfidf_test_features_df_new, tfidf_valid_features_df_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6NodJua7hCH"
   },
   "outputs": [],
   "source": [
    "def filter_features_by_cor(df):\n",
    "    m = len(df.columns)\n",
    "    output = df.iloc[:,m-1] \n",
    "    output_list = output.tolist()\n",
    "    corrcoef_array = []\n",
    "\n",
    "    for i in range(0,m-2):\n",
    "        input_list = df.iloc[:,i].tolist()\n",
    "        cols = [input_list, output_list]\n",
    "        corrcoef = abs(np.corrcoef(cols)) \n",
    "        corrcoef_array = np.append(corrcoef_array,corrcoef[0,1])\n",
    "\n",
    "    feature_names = list(df)\n",
    "    feature_names = feature_names[0:m-2]\n",
    "    \n",
    "    output_df = pd.DataFrame(feature_names, columns=['Features'])\n",
    "    output_df['CorrCoef'] = corrcoef_array\n",
    "    output_df = output_df.sort_values('CorrCoef')\n",
    "    output_df = output_df.reset_index()\n",
    "    output_df = output_df.drop(columns = \"index\")\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Dz-msJeB7pxz",
    "outputId": "cca008b4-d250-4696-e247-7fee23febeb0"
   },
   "outputs": [],
   "source": [
    "tfidf_output_df = filter_features_by_cor(tfidf_train_features_df)\n",
    "tfidf_output_df = tfidf_output_df.sort_values(by=['CorrCoef'],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_output_df.to_csv('Data/tfidf_vector_feature_corr_8_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucYzO4Db76AW"
   },
   "outputs": [],
   "source": [
    "# Setting Correlation threshold\n",
    "top_tfidf_features_df = tfidf_output_df[:500]\n",
    "filtered_tfidf_train = tfidf_train_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "filtered_tfidf_test = tfidf_test_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "filtered_tfidf_valid   = tfidf_valid_features_df_new.filter(items=top_tfidf_features_df['Features'])\n",
    "#filtered_tfidf_tanish = tfidf_tanish.filter(items = top_tfidf_features_df['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tfidf_train.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_train_smote.csv\", index = False)\n",
    "filtered_tfidf_test.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_test_smote.csv\", index = False)\n",
    "filtered_tfidf_valid.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\tfidf_validation_smote.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_train_smote.csv\", index = False)\n",
    "y_test.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_test_smote.csv\", index = False)\n",
    "y_valid.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\y_validation_smote.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Correlation Threshold Value by running Logistic Regression model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_corr(corr, tfidf_output_df):\n",
    "    # Setting Correlation threshold\n",
    "    top_tfidf_features_df = tfidf_output_df[tfidf_output_df['CorrCoef'] > corr]\n",
    "    filtered_tfidf_train = tfidf_train_features_df.filter(items=top_tfidf_features_df['Features'])\n",
    "    filtered_tfidf_test = tfidf_test_features_df.filter(items=top_tfidf_features_df['Features'])\n",
    "    filtered_tfidf_valid = tfidf_valid_features_df.filter(items=top_tfidf_features_df['Features'])\n",
    "    \n",
    "    return top_tfidf_features_df, filtered_tfidf_train, filtered_tfidf_test, filtered_tfidf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisitic_regression(X_train, y_train, X_test, y_test, c, want_report, want_conf_mat, testing = False):\n",
    "    # fitting model\n",
    "    lr = LogisticRegression(penalty='l1', solver='liblinear', C = c, random_state=0)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    if (testing):\n",
    "        filename = 'Storage/lr_8_9.sav'\n",
    "        pickle.dump(lr, open(filename, 'wb'))\n",
    "    \n",
    "    # predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "    y_prob = lr.predict_proba(X_test)\n",
    "    \n",
    "    #print(y_pred, y_prob)\n",
    "    \n",
    "    # collecting results\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    if (want_report == True):\n",
    "        target_names = ['NO', 'NTR', 'YES']\n",
    "        results_lgr = classification_report(y_test, y_pred, target_names = target_names, output_dict=True)\n",
    "        results_lgr = pd.DataFrame(results_lgr).transpose()\n",
    "        \n",
    "        if (want_conf_mat == True):\n",
    "            return lr, acc, auc, c, results_lgr, confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "        return lr, acc, auc, c, results_lgr\n",
    "    \n",
    "    if (want_conf_mat == True):\n",
    "        return lr, acc, auc, c, confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "    return lr, acc, auc, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "    # ensuring straftification across label\n",
    "    yes = cross_validation[cross_validation[\"annotator_label\"] == 2].reset_index(drop = True)\n",
    "    no = cross_validation[cross_validation[\"annotator_label\"] == 0].reset_index(drop = True)\n",
    "    ntr = cross_validation[cross_validation[\"annotator_label\"] == 1].reset_index(drop = True)\n",
    "    #print(len(yes), len(no), len(ntr))\n",
    "    \n",
    "    yes_count = len(yes) // 10\n",
    "    no_count = len(no) // 10\n",
    "    ntr_count = len(ntr) // 10\n",
    "    #print(yes_count, no_count, ntr_count)\n",
    "    split = list()\n",
    "    fold_size = len(cross_validation) // 10\n",
    "\n",
    "    # shuffling data to avoid having to generate random nums through while loop\n",
    "    yes = yes.sample(frac=1).reset_index(drop=True)\n",
    "    no = no.sample(frac=1).reset_index(drop=True)\n",
    "    ntr = ntr.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # creating folds\n",
    "    for i in tqdm(range(n_folds)):\n",
    "        fold = pd.DataFrame(columns = cross_validation.columns)\n",
    "\n",
    "        fold = fold.append(yes[yes_count * i : (yes_count * i) + yes_count])\n",
    "        #print(len(fold), \"YES\", )\n",
    "        fold = fold.append(no[no_count * i : (no_count * i) + no_count])\n",
    "        #print(len(fold), \"NO\", )\n",
    "        fold = fold.append(ntr[ntr_count * i : (ntr_count * i) + ntr_count])\n",
    "        #print(len(fold), \"NTR\", ((ntr_count * i) + ntr_count) - (ntr_count * i))\n",
    "        split.append(fold)\n",
    "        \n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = cross_validation.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = cross_validation_split(cross_validation, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validation.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Data\\cross_validation_tfidf_8_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(dataset, n_folds):\n",
    "    splits = cross_validation_split(dataset, n_folds)\n",
    "    \n",
    "    counter = 0\n",
    "    tfidf_all_df = pd.DataFrame()\n",
    "    df_list  = []\n",
    "    \n",
    "    for fold in splits:\n",
    "        train = splits.copy()\n",
    "        del train[counter]\n",
    "        train = pd.concat(train)\n",
    "        \n",
    "        y_train = train[\"annotator_label\"].reset_index(drop = True)\n",
    "        y_train = y_train.astype(int)\n",
    "        \n",
    "        y_test = fold[\"annotator_label\"].reset_index(drop = True)\n",
    "        y_test = y_test.astype(int)\n",
    "        \n",
    "        train = train.drop(columns = [\"annotator_label\"])\n",
    "        fold = fold.drop(columns = [\"annotator_label\"])\n",
    "        \n",
    "        test = list()\n",
    "        corr_list = list(np.arange(1,30) * 0.01)\n",
    "       \n",
    "        for corr in corr_list:\n",
    "            acc_list = []\n",
    "            auc_list = []\n",
    "            c_list = []\n",
    "             \n",
    "            # filtering by correlation coefficient\n",
    "            top_tfidf_features_df = tfidf_output_df[tfidf_output_df['CorrCoef'] > corr]\n",
    "            filtered_tfidf_train = train.filter(items=top_tfidf_features_df['Features'])\n",
    "            filtered_tfidf_fold = fold.filter(items=top_tfidf_features_df['Features'])\n",
    "            filtered_tfidf_test = tfidf_test_features_df.filter(items=top_tfidf_features_df['Features'])\n",
    "            \n",
    "            #print(filtered_tfidf_train.shape)\n",
    "            #print(filtered_tfidf_fold.shape)\n",
    "            \n",
    "            # tuning for optimal lambda value\n",
    "            for c in [0.01, 0.1, 1, 10, 100]:\n",
    "                lr, acc, auc, c = logisitic_regression(filtered_tfidf_train, y_train, filtered_tfidf_fold, y_test, c, False, False, False)\n",
    "                acc_list.append(acc)\n",
    "                auc_list.append(auc)\n",
    "                c_list.append(c)\n",
    "            \n",
    "            # gathering model stats\n",
    "            acc_df = pd.DataFrame(acc_list, columns=['acc'])\n",
    "            auc_df = pd.DataFrame(auc_list, columns=['auc'])\n",
    "            c_df = pd.DataFrame(c_list, columns=['c_value'])\n",
    "            \n",
    "            assert len(acc_df) == len(auc_df) == len(c_df)\n",
    "            \n",
    "            #acc_df[\"fold_number\"] = auc_df[\"fold_number\"] = c_df[\"fold_number\"] = [counter] * len(auc_df)\n",
    "            \n",
    "            iter_df = pd.concat([c_df, acc_df, auc_df], axis=1)\n",
    "            iter_df['corr_thres'] = [corr] * len(iter_df)\n",
    "            iter_df['fold_number'] = [(counter + 1)] * len(iter_df)\n",
    "            df_list.append(iter_df)\n",
    "            \n",
    "        print(\"Completed Fold #: \", counter + 1)\n",
    "        counter += 1\n",
    "        \n",
    "        print(\"Stats DF has\", len(df_list), \"records\")\n",
    "        \n",
    "        #df_list.to_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Optimizing-data\\sample_stat_df.csv\", index = False)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all_df = evaluate_algorithm(cross_validation, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all_df = pd.concat(tfidf_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all_df = tfidf_all_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all_df.to_csv('tfidf_vect_performance_l1_regularization_10_fold_cross_validation_8_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list = list(np.arange(1,30) * 0.01)\n",
    "average_results_df = []\n",
    "\n",
    "for corr in corr_list:\n",
    "    for c in [0.01, 0.1, 1, 10, 100]:\n",
    "        filtered = tfidf_all_df[(tfidf_all_df[\"corr_thres\"] == corr) & (tfidf_all_df[\"c_value\"] == c)]\n",
    "        avg_auc = filtered[\"auc\"].mean()\n",
    "        avg_acc = filtered[\"acc\"].mean()\n",
    "\n",
    "        filler = np.arange(5, 9)**2\n",
    "        df = pd.DataFrame(filler.reshape(1, 4), columns = [\"c_value\", \"acc\", \"auc\", \"corr_thres\"])\n",
    "        df.loc[df.index] = [c, avg_acc, avg_auc, corr]\n",
    "        #print(df)\n",
    "        \n",
    "        average_results_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df = pd.concat(average_results_df)\n",
    "average_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df[average_results_df['auc'] == max(average_results_df['auc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df[average_results_df['auc'] == min(average_results_df['auc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_all_df[tfidf_all_df['auc'] > 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Logistic Regression model with Optimal Parameters (identified from validation set) on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific parameter setting performance\n",
    "corr = 0.01\n",
    "c = 10\n",
    "\n",
    "y_train = cross_validation[\"annotator_label\"]\n",
    "y_test = tfidf_test_features_df[\"annotator_label\"]\n",
    "\n",
    "cross_validation.drop(columns = [\"annotator_label\"])\n",
    "tfidf_test_features_df.drop(columns = [\"annotator_label\"])\n",
    "\n",
    "# Setting Correlation threshold\n",
    "top_tfidf_features_df = tfidf_output_df[tfidf_output_df['CorrCoef'] > corr]\n",
    "filtered_tfidf_train = cross_validation.filter(items=top_tfidf_features_df['Features'])\n",
    "filtered_tfidf_test = tfidf_test_features_df.filter(items=top_tfidf_features_df['Features'])\n",
    "\n",
    "# Running model\n",
    "lr, acc_optimized, auc_optimized, c_list, report, conf_mat = logisitic_regression(filtered_tfidf_train, y_train, filtered_tfidf_test, y_test, c, True, True)\n",
    "\n",
    "print(\"\\nC: \", c, \"\\n\", report)\n",
    "print(\"\\nAUC: \", auc_optimized)\n",
    "print(\"ACC: \", acc_optimized)\n",
    "print(\"\\nConfusion Matrix: \\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = conf_mat.sum(axis = 0) - np.diag(conf_mat) \n",
    "FN = conf_mat.sum(axis = 1) - np.diag(conf_mat)\n",
    "TP = np.diag(conf_mat)\n",
    "TN = conf_mat.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "print(\"Sensitivity: \", TPR)\n",
    "print(\"Specificity: \", TNR)\n",
    "print(\"NPV: \", NPV)\n",
    "print(\"PPV: \", PPV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on 20K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\BigDataSets\\Regex_match\\Diversity_Sampling\\20K_sample_8_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sequence(seq):\n",
    "    #getting rid of special characters\n",
    "    specials = '/' #etc\n",
    "    seq_no_special_chars = seq.translate(str.maketrans(specials, ' '*len(specials)))\n",
    "    \n",
    "    #having only 1 space between words\n",
    "    n = 1\n",
    "    seq_no_spaces = (' '*n).join(seq_no_special_chars.split())\n",
    "    \n",
    "    return seq_no_spaces.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.loc[i, \"regex_sent\"] = clean_sequence(df.loc[i][\"regex_sent\"][7:len(df.loc[i][\"regex_sent\"]) - 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2][\"regex_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"regex_sent\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\Tanish\\APOE-SLAT\\Modeling\\Storage\\lr_8_9.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(tfidf_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter(lambda x : x == 2, predictions)\n",
    "print(len(list(filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_set = pd.concat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "      NO   NTR YES\n",
    "NO  [[439  9  21]\n",
    "NTR [  5 457   7]\n",
    "YES [ 26  19 424]]\n",
    "\n",
    "NO - 439/469 TP, 9/469 NTR when should be NO, and 21/469 YES when should be NO\n",
    "    Precision: 439/(439+5+26)\n",
    "    Recall: 439/(439+21+9)\n",
    "    fl: (439*2)/(470+469)\n",
    "\n",
    "NTR - 457/469 TP, 5/469 NO when should be NTR, and 7/469 YES when should be NTR\n",
    "YES - 424/469 TP, 19/469 NTR when should be YES, and 26/469 are NO when should be YES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ROC-AUC:\n",
    "    TPR = TP / P = TP / (TP+FN) = number of true positives / number of positives\n",
    "    FPR = FP / N = FP / (FP+TN) = number of false positives / number of negatives\n",
    "    plots FPR aganist TPR \n",
    "    \n",
    "classifier achieves the good performance on the positive class (high AUC) \n",
    "at the cost of a high false negatives rate (or a low number of true negative), resulting in low ACC.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_number = 20\n",
    "lr_feature_names = list(filtered_tfidf_train.columns)\n",
    "lr_feature_coef = list(lr.coef_.tolist()[0])\n",
    "lr_feature_importance_df = pd.DataFrame(list(zip(lr_feature_names,lr_feature_coef)), columns = ['feature','lr_coef'])\n",
    "lr_feature_importance_df = lr_feature_importance_df.sort_values('lr_coef',ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"LR Top Feature Importmant Rank\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.barh(lr_feature_importance_df[:show_number]['feature'], lr_feature_importance_df[:show_number]['lr_coef'], color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(columns = ['patient_id', 'empi', 'label', 'apoe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['patient_id'] = df['PatientID']\n",
    "sample['empi'] = df['EMPI']\n",
    "sample['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample[\"patient_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib1 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\BioBank\\sd587_03212118253982866_6375194824947826181_Bib.csv\")\n",
    "bib2 = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\BioBank\\sd587_03212118253982866_6375194824947826182_Bib.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup = pd.read_csv(r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\NLP\\BioBank\\Partners_biobank_APOE_nodup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib = pd.concat([bib1, bib2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib = bib.reset_index(drop = True)\n",
    "nodup = nodup.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele = str((nodup[nodup[\"subject_id\"] == bib.loc[i][\"Subject_Id\"]][\"APOE\"].values))\n",
    "allele = allele[2:len(allele)-2]\n",
    "allele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib = bib[bib['Subject_Id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apoe = []\n",
    "for i in tqdm(range(len(bib))):\n",
    "    allele = str((nodup[nodup[\"subject_id\"] == bib.loc[i][\"Subject_Id\"]][\"APOE\"].values))\n",
    "    allele = allele[2:len(allele)-2]\n",
    "    apoe.append(allele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib[\"APOE\"] = apoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apoe = []\n",
    "for i in range(len(sample)):\n",
    "    allele = bib[bib[\"EMPI\"] == bib.loc[i][\"EMPI\"]][\"APOE\"].values\n",
    "    apoe.append(allele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"apoe\"] = apoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = {2 : \"Yes\", 1 : \"NTR\", 0: \"NO\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.label = [convert[i] for i in sample.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample[sample[\"label\"] == \"Yes\"]\n",
    "ntr = sample[sample[\"label\"] == \"NTR\"]\n",
    "n = sample[sample[\"label\"] == \"NO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"apoe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n[\"apoe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr[\"apoe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"apoe\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression, SVM with TFIDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
