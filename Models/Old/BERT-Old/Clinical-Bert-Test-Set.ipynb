{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import itertools\n",
    "import sklearn.metrics as sk\n",
    "from functools import reduce\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "# file system manipulation\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to make computations deterministic\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? \", \"Yes\" if cuda_available else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv(r\"Storage/Bert/test_8_11.csv\")\n",
    "final_test.columns = [\"PatientID\", \"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"Storage/Bert/NoHyperParameterTuningResults/trial_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results, test_outputs, test_wrong = best_model.eval_model(\n",
    "    final_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob_list = []\n",
    "test_prob_list = []\n",
    "test_pred_list = []\n",
    "for i in range(len(final_test)):\n",
    "    # prob_list = list(torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)[:,1])\n",
    "    prob_list = torch.softmax(torch.from_numpy(test_outputs[i]), axis=0)\n",
    "    #print(\"Prob List: \", prob_list, type(prob_list))\n",
    "\n",
    "    extracted_prob_list = []\n",
    "    for i in range(len(prob_list)):\n",
    "        extracted_prob_list.append(float(prob_list[i]))\n",
    "\n",
    "    #print(\"Extracted Prob List: \", extracted_prob_list)\n",
    "    # find max one in each submatrix of length 3\n",
    "    max_proba = max(extracted_prob_list)\n",
    "\n",
    "    # identify model prediction based on location of max_proba within extracted_prob_list\n",
    "    if (extracted_prob_list[0] == max_proba):\n",
    "        test_pred_list.append(0)\n",
    "    elif (extracted_prob_list[1] == max_proba):\n",
    "        test_pred_list.append(1)\n",
    "    else:\n",
    "        test_pred_list.append(2)\n",
    "\n",
    "    max_prob_list.append(max_proba)\n",
    "    test_prob_list.append(extracted_prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test[\"pred\"] = test_pred_list\n",
    "final_test[\"proba\"] = max_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_preds = pd.read_csv(r\"Storage/Bert/Revised Bert Preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_made = 0\n",
    "for i in range(len(final_test)):\n",
    "    for j in range(len(revised_preds)):\n",
    "        if (final_test.at[i, \"text\"] == revised_preds.at[j, \"text\"]):\n",
    "            edits_made += 1\n",
    "            final_test.at[i, \"labels\"] = revised_preds.at[j, \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.accuracy_score(test_pred_list, final_test[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.roc_auc_score(final_test[\"labels\"], test_prob_list, multi_class = \"ovr\", average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"Storage/Bert/test_set_prob.pkl\", \"wb\")\n",
    "pickle.dump(str(test_prob_list), f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "target_names = ['Negative', 'Neither', 'Positive']\n",
    "results = metrics.classification_report(final_test[\"labels\"], final_test[\"pred\"], target_names = target_names, output_dict=True)\n",
    "results = pd.DataFrame(results).transpose()\n",
    "conf_mat = metrics.confusion_matrix(final_test[\"labels\"], final_test[\"pred\"])\n",
    "print(\"Micro F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"micro\"))\n",
    "print(\"Macro F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"macro\"))\n",
    "print(\"Weighted F1: \", metrics.f1_score(final_test[\"labels\"], final_test[\"pred\"], average = \"weighted\"))\n",
    "print(\"Confusion Matrix: \\n\", conf_mat)\n",
    "print(\"Classification Report: \\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = conf_mat.sum(axis = 0) - np.diag(conf_mat) \n",
    "FN = conf_mat.sum(axis = 1) - np.diag(conf_mat)\n",
    "TP = np.diag(conf_mat)\n",
    "TN = conf_mat.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "print(\"Sensitivity: \", TPR)\n",
    "print(\"Specificity: \", TNR)\n",
    "print(\"NPV: \", NPV)\n",
    "print(\"PPV: \", PPV)\n",
    "print(\"FPR: \", FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.93121693 +  0.96855346 + 0.98739496) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.98076923 + 0.93283582 + 0.81818182) / 3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0168968bb7ecfa78684047e3c95d55595c46869da584ba4da41f68e310286ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('apoe-dl-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
