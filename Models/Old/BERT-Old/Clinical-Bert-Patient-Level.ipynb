{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import itertools\n",
    "import sklearn.metrics as sk\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna.visualization.matplotlib as oviz\n",
    "\n",
    "# file system manipulation\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to make computations deterministic\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? \", \"Yes\" if cuda_available else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"Storage/Bert/NoHyperParameterTuningResults/trial_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"Storage/Bert/sample_8_12.csv\")\n",
    "data = data[[\"PatientID\", \"regex_sent\"]]\n",
    "data.columns = [\"PatientID\", \"text\"]\n",
    "data.head()\n",
    "sample_predictions = []\n",
    "sample_outputs = []\n",
    "chunk_size = 500\n",
    "total_predicted = 0\n",
    "\n",
    "for i in tqdm(range(int(len(data) / chunk_size))): \n",
    "    if (len(data) - total_predicted < chunk_size):        \n",
    "        subset_predictions, subset_outputs = best_model.predict(\n",
    "            data[total_predicted:][\"text\"].to_list()\n",
    "        )\n",
    "        assert len(subset_predictions) == len(subset_outputs)\n",
    "        sample_predictions.append(subset_predictions)\n",
    "        sample_outputs.append(subset_outputs)\n",
    "    else:\n",
    "        subset_predictions, subset_outputs = best_model.predict(\n",
    "            data[chunk_size * i : chunk_size * (i+1)][\"text\"].to_list()\n",
    "        )\n",
    "        assert len(subset_predictions) == len(subset_outputs)\n",
    "        sample_predictions.append(subset_predictions)\n",
    "        sample_outputs.append(subset_outputs)\n",
    "    total_predicted += len(subset_predictions)\n",
    "\n",
    "    f = open(\"Storage/Bert/sample_predictions.pkl\", \"wb\")\n",
    "    pickle.dump(str(sample_predictions), f)\n",
    "    f.close()\n",
    "\n",
    "    f = open(\"Storage/Bert/sample_outputs.pkl\", \"wb\")\n",
    "    pickle.dump(sample_outputs, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob_list = []\n",
    "sample_prob_list = []\n",
    "\n",
    "for i in range((373 * 500)):\n",
    "    # prob_list = list(torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)[:,1])\n",
    "    prob_list = torch.softmax(torch.from_numpy(sample_outputs[i]), axis=0)\n",
    "    #print(\"Prob List: \", prob_list, type(prob_list))\n",
    "\n",
    "    extracted_prob_list = []\n",
    "    for i in range(len(prob_list)):\n",
    "        extracted_prob_list.append(float(prob_list[i]))\n",
    "\n",
    "    #print(\"Extracted Prob List: \", extracted_prob_list)\n",
    "    # find max one in each submatrix of length 3\n",
    "    max_proba = max(extracted_prob_list)\n",
    "\n",
    "    max_prob_list.append(max_proba)\n",
    "    sample_prob_list.append(extracted_prob_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, raw_outputs = best_model.predict(\n",
    "    data[373 * 500:][\"text\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = np.append(sample_predictions, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range((230)):\n",
    "    # prob_list = list(torch.softmax(torch.from_numpy(model_outputs[i]), axis=0)[:,1])\n",
    "    prob_list = torch.softmax(torch.from_numpy(raw_outputs[i]), axis=0)\n",
    "    #print(\"Prob List: \", prob_list, type(prob_list))\n",
    "\n",
    "    extracted_prob_list = []\n",
    "    for i in range(len(prob_list)):\n",
    "        extracted_prob_list.append(float(prob_list[i]))\n",
    "\n",
    "    #print(\"Extracted Prob List: \", extracted_prob_list)\n",
    "    # find max one in each submatrix of length 3\n",
    "    max_proba = max(extracted_prob_list)\n",
    "\n",
    "    max_prob_list.append(max_proba)\n",
    "    sample_prob_list.append(extracted_prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_predictions), len(max_prob_list), len(sample_prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pred\"] = sample_predictions\n",
    "data[\"proba\"] = max_prob_list\n",
    "data[\"three_class_proba\"] = sample_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"Storage/Bert/sample_preds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apoe = pd.read_csv(r\"Storage/Bert/tanish_predictions_with_structured_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_apoe = data.copy()\n",
    "data_apoe = pd.merge(data_apoe, apoe, how = \"right\", on = \"PatientID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_2 = data_apoe[(data_apoe[\"APOE\"] == \"e2/e2\") | (data_apoe[\"APOE\"] == \"e2/e3\")]\n",
    "e_3 = data_apoe[(data_apoe[\"APOE\"] == \"e3/e3\")]\n",
    "e_4 = data_apoe[(data_apoe[\"APOE\"] == \"e4/e4\") | (data_apoe[\"APOE\"] == \"e2/e4 or e1/e3\") | (data_apoe[\"APOE\"] == \"e3/e4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_ci(df, sequence_threshold):\n",
    "    patient_level_preds = []\n",
    "    p_id_ls = []\n",
    "    ci_count = 0\n",
    "    for p_id in df[\"PatientID\"].unique():\n",
    "        sequences = df[df[\"PatientID\"] == p_id]\n",
    "        if len(sequences[sequences[\"pred\"] == 2]) >= sequence_threshold:\n",
    "            patient_level_preds.append(1)\n",
    "            ci_count += 1\n",
    "        else:\n",
    "            patient_level_preds.append(0)\n",
    "        p_id_ls.append(p_id)\n",
    "    return patient_level_preds, ci_count/len(df[\"PatientID\"].unique()), p_id_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = []\n",
    "y_axis_e2 = []; y_axis_e3 = []; y_axis_e4 = []\n",
    "e2_preds = []; e3_preds = []; e4_preds = []\n",
    "e2_pid = []; e3_pid = []; e4_pid = []\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    e_2_preds, e_2_percent, e_2_p_id = percent_ci(e_2, i+1)\n",
    "    e_3_preds, e_3_percent, e_3_p_id = percent_ci(e_3, i+1)\n",
    "    e_4_preds, e_4_percent, e_4_p_id = percent_ci(e_4, i+1)\n",
    "    \n",
    "    x_axis.append(i+1)\n",
    "    y_axis_e2.append(e_2_percent); y_axis_e3.append(e_3_percent); y_axis_e4.append(e_4_percent)\n",
    "    e2_preds.append(e_2_preds); e3_preds.append(e_3_preds); e4_preds.append(e_4_preds)\n",
    "    e2_pid.append(e_2_p_id); e3_pid.append(e_3_p_id); e4_pid.append(e_4_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, y_axis_e2, label = 'APOE ε2')\n",
    "plt.plot(x_axis, y_axis_e3, label = 'APOE ε3')\n",
    "plt.plot(x_axis, y_axis_e4, label = 'APOE ε4')\n",
    "\n",
    "plt.xlabel(\"Sequence Threshold\")\n",
    "plt.ylabel(\"Percent of Patients Predicted as having CI\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_2_preds, e_2_percent, e_2_p_id = percent_ci(e_2, 2)\n",
    "e_3_preds, e_3_percent, e_3_p_id = percent_ci(e_3, 2)\n",
    "e_4_preds, e_4_percent, e_4_p_id = percent_ci(e_4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_2_percent, e_3_percent, e_4_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e_2_p_id) + len(e_3_p_id) + len(e_4_p_id), len(e_2_preds) + len(e_3_preds) + len(e_4_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_preds = pd.DataFrame()\n",
    "patient_preds[\"PatientID\"] = (e_2_p_id) + (e_3_p_id) + (e_4_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_preds[\"Pred\"] = (e_2_preds) + (e_3_preds) + (e_4_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_preds = pd.merge(apoe, patient_preds, how = \"right\", on = \"PatientID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_preds.drop(columns = [\"patient_CI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = len(patient_preds[(patient_preds[\"Pred\"] == 1) & (patient_preds[\"AD_Med_or_ICD_Code\"] == 1)])\n",
    "FP = len(patient_preds[(patient_preds[\"Pred\"] == 1) & (patient_preds[\"AD_Med_or_ICD_Code\"] == 0)])\n",
    "TN = len(patient_preds[(patient_preds[\"Pred\"] == 0) & (patient_preds[\"AD_Med_or_ICD_Code\"] == 0)])\n",
    "FN = len(patient_preds[(patient_preds[\"Pred\"] == 0) & (patient_preds[\"AD_Med_or_ICD_Code\"] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_preds.to_csv(r\"patient_level_bert_preds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pp = pd.read_csv(r\"Storage/Bert/patient_level_bert_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_2 = pp[(pp[\"APOE\"] == \"e2/e2\") | (pp[\"APOE\"] == \"e2/e3\")]\n",
    "e_3 = pp[(pp[\"APOE\"] == \"e3/e3\")]\n",
    "e_4 = pp[(pp[\"APOE\"] == \"e4/e4\") | (pp[\"APOE\"] == \"e2/e4 or e1/e3\") | (pp[\"APOE\"] == \"e3/e4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_4_diff = e_4[(e_4[\"patient_CI\"] == 1) & (e_4[\"AD_Med_or_ICD_Code\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = pd.read_csv(r\"Storage/Bert/sample_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs[seqs[\"PatientID\"] == e_4_diff.at[[10508, \"PatientID\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0168968bb7ecfa78684047e3c95d55595c46869da584ba4da41f68e310286ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('apoe-dl-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
