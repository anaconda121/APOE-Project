{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "#  Author: Tanish Tyagi\n",
    "#  \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# deep learning libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Patient Level Dataset with ClinicalBERT Sequence Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_preds = pd.read_csv(r\"Storage/Bert/john_hsu_sequence_preds_proba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_preds = patient_level_preds[patient_level_preds[\"syndromic_dx\"].isna() == False]\n",
    "patient_level_preds = patient_level_preds.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_preds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering to Get Patient Level Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(patient_level_preds))):\n",
    "    if (int(patient_level_preds.at[i, \"syndromic_dx\"]) > 0):\n",
    "        patient_level_preds.at[i, \"syndromic_dx\"] = 1\n",
    "    else:\n",
    "        patient_level_preds.at[i, \"syndromic_dx\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_preds[\"syndromic_dx\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_features = pd.DataFrame(columns = [\"PatientID\", \"percent_yes\", \"percent_no\", \"percent_neither\", \"sequence_count\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percent yes, no, neither Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in tqdm(range(len(patient_level_preds[\"PatientID\"].unique()))):\n",
    "    curr = patient_level_preds[patient_level_preds[\"PatientID\"] == str(patient_level_preds[\"PatientID\"].unique()[i])]\n",
    "    seq_count = len(curr)\n",
    "\n",
    "    if (seq_count <= 10):\n",
    "        continue\n",
    "    \n",
    "    p_yes = len(curr[curr[\"class_pred\"] == 2]) / len(curr)\n",
    "    p_no = len(curr[curr[\"class_pred\"] == 0]) / len(curr)\n",
    "    p_ntr = len(curr[curr[\"class_pred\"] == 1]) / len(curr)\n",
    "\n",
    "    no_count = len(curr[curr[\"syndromic_dx\"] == 0])\n",
    "    yes_count = len(curr[curr[\"syndromic_dx\"] == 1])\n",
    "\n",
    "    label = 0\n",
    "    if (yes_count > no_count):\n",
    "        label = 1 \n",
    "    \n",
    "    curr_dict = {\n",
    "        \"PatientID\" : str(patient_level_preds[\"PatientID\"].unique()[i]),\n",
    "        \"percent_yes\" : p_yes,\n",
    "        \"percent_no\" : p_no,\n",
    "        \"percent_neither\" : p_ntr,\n",
    "        \"sequence_count\" : seq_count,\n",
    "        \"label\" : label  \n",
    "    }\n",
    "\n",
    "    data.append(curr_dict)\n",
    "\n",
    "patient_level_features = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = patient_level_features[\"sequence_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Sequence Count Feature to a discrete value by bucketing based off quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(patient_level_features))):\n",
    "    if (patient_level_features.at[i, \"sequence_count\"] <= x[\"25%\"]):\n",
    "        patient_level_features.at[i, \"sequence_count\"] = 0\n",
    "    elif (patient_level_features.at[i, \"sequence_count\"] <= x[\"50%\"]):\n",
    "        patient_level_features.at[i, \"sequence_count\"] = 1\n",
    "    elif (patient_level_features.at[i, \"sequence_count\"] <= x[\"75%\"]):\n",
    "        patient_level_features.at[i, \"sequence_count\"] = 2\n",
    "    else:\n",
    "        patient_level_features.at[i, \"sequence_count\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_level_features.to_csv(r\"Storage/Bert/jh_patient_level_features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Validation, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = patient_level_features[[\"percent_yes\", \"percent_no\", \"percent_neither\", \"sequence_count\"]]\n",
    "y = patient_level_features[\"label\"]\n",
    "\n",
    "y_label = y.to_numpy()\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X,y,test_size=0.15, stratify=y_label)\n",
    "\n",
    "y_test_valid_label = y_test_valid.to_numpy()\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test_valid, y_test_valid,test_size=0.5, stratify=y_test_valid_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"percent_yes\",\"percent_no\", \"sequence_count\"]\n",
    "\n",
    "for i in num_cols:\n",
    "    scale = StandardScaler().fit(X_train[[i]])\n",
    "\n",
    "    X_train[i] = scale.transform(X_train[[i]])\n",
    "    X_valid[i] = scale.transform(X_valid[[i]])\n",
    "    X_test[i] = scale.transform(X_test[[i]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_valid), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross_validation = pd.concat([X_train, X_valid]).to_numpy()\n",
    "y_cross_validation = pd.concat([y_train, y_valid]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "kf.get_n_splits(X_cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisitic_regression(X_train, y_train, X_test, y_test, c, want_conf_mat):\n",
    "    # fitting model\n",
    "    lr = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = c, random_state = 0, class_weight = 'balanced')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "    y_prob = lr.predict_proba(X_test)\n",
    "\n",
    "    # collecting results\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "    \n",
    "    # if (save_model == True):\n",
    "    #     pickle.dump(lr, open(\"Storage/Model/\" + name, 'wb'))\n",
    "\n",
    "    \n",
    "    if (want_conf_mat == True):\n",
    "        return lr, acc, auc, c, confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "    return lr, acc, auc, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "df_list  = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_cross_validation):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #X_train_cv = X_train_cv.drop(columns = [\"PatientID\"])\n",
    "    #X_test_cv = X_test_cv.drop(columns = [\"PatientID\"])\n",
    "\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    c_list = []\n",
    "\n",
    "    # tuning for optimal lambda value\n",
    "    for c in [0.01, 0.1, 1, 10, 100]:\n",
    "        #name = \"Fold-\" + str((counter + 1)) + \"-Corr-\" + str(corr) + \"-C-\" + str(c) + \".sav\"\n",
    "        lr, acc, auc, c = logisitic_regression(X_train_cv, y_train_cv, X_test_cv, y_test_cv, c, False)\n",
    "        acc_list.append(acc)\n",
    "        auc_list.append(auc)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # gathering model stats\n",
    "    acc_df = pd.DataFrame(acc_list, columns=['acc'])\n",
    "    auc_df = pd.DataFrame(auc_list, columns=['auc'])\n",
    "    c_df = pd.DataFrame(c_list, columns=['c_value'])\n",
    "    \n",
    "    assert len(acc_df) == len(auc_df) == len(c_df)\n",
    "        \n",
    "    iter_df = pd.concat([c_df, acc_df, auc_df], axis=1)\n",
    "    iter_df['fold_number'] = [(counter + 1)] * len(iter_df)\n",
    "    df_list.append(iter_df)\n",
    "        \n",
    "    print(\"Completed Fold #: \", counter + 1)\n",
    "    counter += 1\n",
    "    \n",
    "    print(\"Stats DF has\", len(df_list), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df = []\n",
    "\n",
    "for c in [0.01, 0.1, 1, 10, 100]:\n",
    "    filtered = all_df[(all_df[\"c_value\"] == c)]\n",
    "    avg_auc = filtered[\"auc\"].mean()\n",
    "    avg_acc = filtered[\"acc\"].mean()\n",
    "\n",
    "    filler = np.arange(5, 8)**2\n",
    "    df = pd.DataFrame(filler.reshape(1, 3), columns = [\"c_value\", \"acc\", \"auc\"])\n",
    "    df.loc[df.index] = [c, avg_acc, avg_auc]\n",
    "    #print(df)\n",
    "    \n",
    "    average_results_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df = pd.concat(average_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results_df[average_results_df['auc'] == max(average_results_df['auc'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Held Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 100\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = c, class_weight = 'balanced')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_test_preds)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob = lr.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_test_prob[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Negative', 'Positive']\n",
    "results_lgr = classification_report(y_test, y_test_preds, target_names = target_names, output_dict=True)\n",
    "results_lgr = pd.DataFrame(results_lgr).transpose()\n",
    "results_lgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and features plus ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "if save == True:\n",
    "    file_name = \"lr_12_26_patient_level.sav\"\n",
    "    pickle.dump(lr, open(file_name, 'wb'))\n",
    "\n",
    "    X_train_df = pd.concat([X_train, y_train])\n",
    "    X_valid_df = pd.concat([X_valid, y_valid])\n",
    "    X_test_df = pd.concat([X_test, y_test])\n",
    "\n",
    "    X_train_df.to_csv(r\"Storage/Bert/patient_level_train_85.csv\", index = False)\n",
    "    X_valid_df.to_csv(r\"Storage/Bert/patient_level_valid_85.csv\", index = False)\n",
    "    X_test_df.to_csv(r\"Storage/Bert/patient_level_test_85.csv\", index = False)\n",
    "    n_class = 2\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    thresh = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(n_class):    \n",
    "        fpr[i], tpr[i], thresh[i] = metrics.roc_curve(y_test, y_test_prob[:,i], pos_label = i)\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "        \n",
    "    lw = 2\n",
    "\n",
    "    # plotting    \n",
    "    plt.plot(fpr[0], tpr[0], linestyle='--', color='red', label='L1 Logistic Regression (area = %0.2f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='L1 Logistic Regression (area = %0.2f)' % roc_auc[1])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # plt.savefig(\"Storage/Bert/patient_level_roc_85.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "FP = conf_mat.sum(axis = 0) - np.diag(conf_mat) \n",
    "FN = conf_mat.sum(axis = 1) - np.diag(conf_mat)\n",
    "TP = np.diag(conf_mat)\n",
    "TN = conf_mat.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "print(\"Sensitivity: \", TPR)\n",
    "print(\"Specificity: \", TNR)\n",
    "print(\"NPV: \", NPV)\n",
    "print(\"PPV: \", PPV)\n",
    "print(\"FPR: \", FPR)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0168968bb7ecfa78684047e3c95d55595c46869da584ba4da41f68e310286ae"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
